{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Andersen Lab Dry Guide \u00b6 The Dry Guide details the computational infrastructure and tasks used in the Andersen Lab. Looking for more resources? Check out our Andersen Lab code club GitHub repo for more coding tips and tricks! Other links: \u00b6 Andersen Lab website CaeNDR","title":"Overview"},{"location":"#welcome_to_the_andersen_lab_dry_guide","text":"The Dry Guide details the computational infrastructure and tasks used in the Andersen Lab. Looking for more resources? Check out our Andersen Lab code club GitHub repo for more coding tips and tricks!","title":"Welcome to the Andersen Lab Dry Guide"},{"location":"#other_links","text":"Andersen Lab website CaeNDR","title":"Other links:"},{"location":"knowledge_base/bash/","text":"Command line \u00b6 Command line Basic Commands Piping Redirects More Advanced Good Guides grep awk Rearranging columns Filtering based on criteria bcftools Persistent terminals Screen Tmux Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with this introduction to bash . Also check out this cheatsheet Basic Commands \u00b6 You should familiarize yourself with the following commands. alias - create a shortcut for a command cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files wget - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted. cut - select specific columns from a text file head - select the first N lines of a text file tail - select the last N lines of a text file Piping \u00b6 In order to pass the output of one command to the next, you can use a pipe, | , which allows you to create chains of tools to create single-line workflows on the command line. cat example.tsv | uniq The above line will print out a sorted list of unique lines from the file example.tsv . cat prints every line from the file to the standard output, but instead of being printed to the terminal, they are redirected to the input of uniq , which then sorts the lines and keeps one copy per set of identical lines which are then printed to the terminal. Redirects \u00b6 Like piping, we can tell the command line interpreter to redirect the contents of a file to a command as input or the output of a command to be saved in a file. > - Take the output from the left-hand side of the redict and write it to a file specified on the right-hand side (note that this will overwrite an existing file of the same name) >> - Take the output from the left-hand side of the redict and append it to a file specified on the right-hand side (if no file exists, this will work the same as the > ) < - Take the file on the right-hand side of the redirect and use its contents as the input to the first command on the left-hand side of the redirect (this essentially replaces using cat file.txt | but is used at the end of the line) uniq > example_uniq.txt < example.txt The above line will create the same output as the piping example but save it to a new file named example_uniq.txt rather than print it to the terminal. More Advanced \u00b6 You should learn these once you have the basics down. git - version control awk - file manipulation; Filtering; Rearranging columns sed - quick find/replace Good Guides \u00b6 Below are some good guides for various bash utilities. grep \u00b6 using grep with regular expressions another regex grep guide awk \u00b6 awk guide awk by example - hundreds of examples Rearranging columns \u00b6 cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column. Filtering based on criteria \u00b6 Print only lines that start with a comment (#) character cat example.tsv | awk '$0 ~ \"^#\" { print }' bcftools \u00b6 bcftools view bcftools view - view VCF bcftools view -h - view only header of VCF bcftools view -H - view VCF without header bcftools view -s CB4856,XZ1516,ECA701 - subset vcf for only these three samples bcftools view -S sample_file.txt - subset vcf for only samples listed in sample_file.txt bcftools view -r III:1-800000 - subset vcf for a region of interest can also just use -r III to get entire chromosome bcftools view -R regions.txt - subset vcf for a region(s) of interest in the regions.txt file bcftools query bcftools query -l - print out list of samples in vcf Print out contents of vcf in specified format (i.e. tsv): bcftools query -f '%CHROM\\t%POS\\t%REF\\t%ALT[\\t%SAMPLE=%GT]\\n' <vcf> > out.tsv Output of above line of code: bcftools query -i GT==\"alt\" - keep rows that include a tag (like a filter) bcftools query -e GT==\"ref\" - remove rows that include a tag Note bcftools query -i/e are not necessarily opposites. For example, if you have three genotype options (REF, ALT, or NA), including only ALT calls is different than exluding only REF calls... For more, check out the bcftools manual and this cheatsheet Persistent terminals \u00b6 A persistent terminal is one that will continue to run even after you've logged out of a remote system or closed your terminal program. This allows you to run programs and allow them to continue running without needing to keep the terminal window open. This is especially useful for running pipelines. Programs that create persistent terminals also allow you to have multiple terminal windows that you can easily switch between. Once you have shut the persistent terminal window, you can easily open it again to continue interacting with it and maintain the command history for that window. Screen \u00b6 screen is a persistent terminal that is available on both Rockfish and QUEST. On Rockfish, you need to load it as a module with the command module load screen prior to using it. On QUEST, it is available upon login. To start a screen session, use the following command: screen -S SessionName where SessionName is some meaningful name for your persistent terminal. A new empty terminal window should replace your current terminal window. You can use this like any other terminal window. When you are ready to detach (exit the terminal window but keep it running), you use the key combination ctrl-A, d . This will create a detached screen session. To reattach (reopen) the detached session, use the command: screen -r SessionName This will attach to the screen session specified by SessionName . This also means that you can have multiple screen sessions detached and running at the same time. To see all available sessions, use the command screen -ls . To reattach to one of the listed sessions, use the name following the period in the left-hand column. To permanently close a screen session, just type exit in the attached terminal session. Note * On Rockfish, you are randomly assigned to one of three login nodes. Detached sessions only exist on the node that they were created on. So, note which node you are on. To reopen that session you will need to login ot the same node. Screen basics Tmux \u00b6 tmux is a persistent terminal that is available on QUEST and can easily be installed on Rockfish with conda. To start a tmux session, use the following command: tmux new -s SessionName where SessionName is some meaningful name for your persistent terminal. A new empty terminal window should replace your current terminal window. You can use this like any other terminal window. When you are ready to detach (exit the terminal window but keep it running), you use the key combination ctrl-B, d . This will create a detached tmux session. To reattach (reopen) the detached session, use the command: tmux attach -t SessionName This will attach to the tmux session specified by SessionName . This also means that you can have multiple tmux sessions detached and running at the same time. To see all available sessions, use the command tmux ls . To reattach to one of the listed sessions, use the name to the left of the colon in the left-hand column. To permanently close a screen session, just type exit in the attached terminal session. Note * On Rockfish, you are randomly assigned to one of three login nodes. Detached sessions only exist on the node that they were created on. So, note which node you are on. To reopen that session you will need to login ot the same node. Tmux basics","title":"Command Line"},{"location":"knowledge_base/bash/#command_line","text":"Command line Basic Commands Piping Redirects More Advanced Good Guides grep awk Rearranging columns Filtering based on criteria bcftools Persistent terminals Screen Tmux Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with this introduction to bash . Also check out this cheatsheet","title":"Command line"},{"location":"knowledge_base/bash/#basic_commands","text":"You should familiarize yourself with the following commands. alias - create a shortcut for a command cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files wget - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted. cut - select specific columns from a text file head - select the first N lines of a text file tail - select the last N lines of a text file","title":"Basic Commands"},{"location":"knowledge_base/bash/#piping","text":"In order to pass the output of one command to the next, you can use a pipe, | , which allows you to create chains of tools to create single-line workflows on the command line. cat example.tsv | uniq The above line will print out a sorted list of unique lines from the file example.tsv . cat prints every line from the file to the standard output, but instead of being printed to the terminal, they are redirected to the input of uniq , which then sorts the lines and keeps one copy per set of identical lines which are then printed to the terminal.","title":"Piping"},{"location":"knowledge_base/bash/#redirects","text":"Like piping, we can tell the command line interpreter to redirect the contents of a file to a command as input or the output of a command to be saved in a file. > - Take the output from the left-hand side of the redict and write it to a file specified on the right-hand side (note that this will overwrite an existing file of the same name) >> - Take the output from the left-hand side of the redict and append it to a file specified on the right-hand side (if no file exists, this will work the same as the > ) < - Take the file on the right-hand side of the redirect and use its contents as the input to the first command on the left-hand side of the redirect (this essentially replaces using cat file.txt | but is used at the end of the line) uniq > example_uniq.txt < example.txt The above line will create the same output as the piping example but save it to a new file named example_uniq.txt rather than print it to the terminal.","title":"Redirects"},{"location":"knowledge_base/bash/#more_advanced","text":"You should learn these once you have the basics down. git - version control awk - file manipulation; Filtering; Rearranging columns sed - quick find/replace","title":"More Advanced"},{"location":"knowledge_base/bash/#good_guides","text":"Below are some good guides for various bash utilities.","title":"Good Guides"},{"location":"knowledge_base/bash/#grep","text":"using grep with regular expressions another regex grep guide","title":"grep"},{"location":"knowledge_base/bash/#awk","text":"awk guide awk by example - hundreds of examples","title":"awk"},{"location":"knowledge_base/bash/#rearranging_columns","text":"cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column.","title":"Rearranging columns"},{"location":"knowledge_base/bash/#filtering_based_on_criteria","text":"Print only lines that start with a comment (#) character cat example.tsv | awk '$0 ~ \"^#\" { print }'","title":"Filtering based on criteria"},{"location":"knowledge_base/bash/#bcftools","text":"bcftools view bcftools view - view VCF bcftools view -h - view only header of VCF bcftools view -H - view VCF without header bcftools view -s CB4856,XZ1516,ECA701 - subset vcf for only these three samples bcftools view -S sample_file.txt - subset vcf for only samples listed in sample_file.txt bcftools view -r III:1-800000 - subset vcf for a region of interest can also just use -r III to get entire chromosome bcftools view -R regions.txt - subset vcf for a region(s) of interest in the regions.txt file bcftools query bcftools query -l - print out list of samples in vcf Print out contents of vcf in specified format (i.e. tsv): bcftools query -f '%CHROM\\t%POS\\t%REF\\t%ALT[\\t%SAMPLE=%GT]\\n' <vcf> > out.tsv Output of above line of code: bcftools query -i GT==\"alt\" - keep rows that include a tag (like a filter) bcftools query -e GT==\"ref\" - remove rows that include a tag Note bcftools query -i/e are not necessarily opposites. For example, if you have three genotype options (REF, ALT, or NA), including only ALT calls is different than exluding only REF calls... For more, check out the bcftools manual and this cheatsheet","title":"bcftools"},{"location":"knowledge_base/bash/#persistent_terminals","text":"A persistent terminal is one that will continue to run even after you've logged out of a remote system or closed your terminal program. This allows you to run programs and allow them to continue running without needing to keep the terminal window open. This is especially useful for running pipelines. Programs that create persistent terminals also allow you to have multiple terminal windows that you can easily switch between. Once you have shut the persistent terminal window, you can easily open it again to continue interacting with it and maintain the command history for that window.","title":"Persistent terminals"},{"location":"knowledge_base/bash/#screen","text":"screen is a persistent terminal that is available on both Rockfish and QUEST. On Rockfish, you need to load it as a module with the command module load screen prior to using it. On QUEST, it is available upon login. To start a screen session, use the following command: screen -S SessionName where SessionName is some meaningful name for your persistent terminal. A new empty terminal window should replace your current terminal window. You can use this like any other terminal window. When you are ready to detach (exit the terminal window but keep it running), you use the key combination ctrl-A, d . This will create a detached screen session. To reattach (reopen) the detached session, use the command: screen -r SessionName This will attach to the screen session specified by SessionName . This also means that you can have multiple screen sessions detached and running at the same time. To see all available sessions, use the command screen -ls . To reattach to one of the listed sessions, use the name following the period in the left-hand column. To permanently close a screen session, just type exit in the attached terminal session. Note * On Rockfish, you are randomly assigned to one of three login nodes. Detached sessions only exist on the node that they were created on. So, note which node you are on. To reopen that session you will need to login ot the same node. Screen basics","title":"Screen"},{"location":"knowledge_base/bash/#tmux","text":"tmux is a persistent terminal that is available on QUEST and can easily be installed on Rockfish with conda. To start a tmux session, use the following command: tmux new -s SessionName where SessionName is some meaningful name for your persistent terminal. A new empty terminal window should replace your current terminal window. You can use this like any other terminal window. When you are ready to detach (exit the terminal window but keep it running), you use the key combination ctrl-B, d . This will create a detached tmux session. To reattach (reopen) the detached session, use the command: tmux attach -t SessionName This will attach to the tmux session specified by SessionName . This also means that you can have multiple tmux sessions detached and running at the same time. To see all available sessions, use the command tmux ls . To reattach to one of the listed sessions, use the name to the left of the colon in the left-hand column. To permanently close a screen session, just type exit in the attached terminal session. Note * On Rockfish, you are randomly assigned to one of three login nodes. Detached sessions only exist on the node that they were created on. So, note which node you are on. To reopen that session you will need to login ot the same node. Tmux basics","title":"Tmux"},{"location":"knowledge_base/best_practices/","text":"Andersen Lab Coding Best Practices \u00b6 These best practices are just a few of the important coding tips and tricks for reproducible research. If you have more ideas, contact Mike! General \u00b6 You should be doing most (if not all) of your analyses in /Volumes/vast/projects/YourName (this requires having mapped VAST to your computer. See here for instructions) This is (1) to make sure the data is backed up/saved with version history and (2) to allow other lab members to access your code/scripts when necessary Do NOT use spaces, quotation marks, or brackets in names of files or folders. Try not to use these characters in column names either (although sometimes it is necessary for a final table output) Computers often have a hard time reading spaces and code used to ignore spaces can vary from program to program Instead, you can use _ or . or - or capitalization ( fileName.txt ) NEVER replace raw data!!!!! You should save your raw data in the rawest format, write a script to analyze it then if you wish, save the processed data for further use. This is important because it always allows you to go back to the original raw data in case something happens Some suggested project folder structures might look like something below: Include a README.md (or README.txt ) file in each project folder to explain where the data and scripts can be found for certain analyses. Trust me, after a few years you will definitely forget\u2026 And don\u2019t forget to update the README regularly, an old README doesn\u2019t do anyone good! Either use full path names in scripts or be explicit about where the working directory is This is important to allow other people to run your code (or might even be helpful for you if you ever reorganize folders one day) Date your files, especially when you update an existing file. Write dates in YYYYMMDD format As much as possible, ensure that your processed data is \u201ctidy\u201d (see below). This doesn\u2019t work for all complex data types, but it should be a general norm to follow. Each variable must have its own column Each observation must have its own row Each value must have its own cell No color or highlighting No empty cells (fill with NA if necessary) Save data as plain text files ( .csv , .tsv , .txt etc. -- NOT .xls !!!) R \u00b6 ALWAYS use namespaces before functions from packages (i.e. dplyr::filter() instead of filter() ) This includes ggplot2 and especially dplyr !!! Some packages have functions with the same name, so adding a namespace is crucial for reproducibility. Also, this helps other people read your code by knowing which functions came from which packages When piping with Tidyverse ( %>% ), press <Enter> to go to the next line after a pipe This makes your code more readable In fact, general practices state no more than 80-100 characters per line of code EVER to increase readability Rockfish \u00b6 You should be doing most (if not all) of your analyses in /vast/eande106/projects/yourName (not your home directory (i.e. /home/<jheid> )) Important Main exception: Nextflow temporary working directories should NOT be on /vast/eande106 but rather in the scratch space /scratch4/eande106/ (files get automatically deleted here periodically). A correctly setup bash profile file will take care of this. QUEST \u00b6 You may still be doing some of your analyses in /projects/b1059/projects/yourName (not your home directory (i.e. /home/netid )) Important Main exception: Nextflow temporary working directories should NOT be on b1059 (it will fill us up!) but rather in the scratch space b1042 (files get automatically deleted here every 30 days). A correctly designed nextflow.config file will take care of this. Python \u00b6 ALWAYS indent using spaces. Mixing spaces and tabs will case your script to fail Use clear and descriptive variable names The names should help someone reading the code track the logic of the coder Only use alphanumeric characters or underscores Never start a name with a number Comment as much as you can to help others (and future you) understand what each part of the code is for Make logical blocks of code into functions This helps with interpretation Anything that is done more than once should be a function Try to follow PEP8 style rules","title":"Best Practices"},{"location":"knowledge_base/best_practices/#andersen_lab_coding_best_practices","text":"These best practices are just a few of the important coding tips and tricks for reproducible research. If you have more ideas, contact Mike!","title":"Andersen Lab Coding Best Practices"},{"location":"knowledge_base/best_practices/#general","text":"You should be doing most (if not all) of your analyses in /Volumes/vast/projects/YourName (this requires having mapped VAST to your computer. See here for instructions) This is (1) to make sure the data is backed up/saved with version history and (2) to allow other lab members to access your code/scripts when necessary Do NOT use spaces, quotation marks, or brackets in names of files or folders. Try not to use these characters in column names either (although sometimes it is necessary for a final table output) Computers often have a hard time reading spaces and code used to ignore spaces can vary from program to program Instead, you can use _ or . or - or capitalization ( fileName.txt ) NEVER replace raw data!!!!! You should save your raw data in the rawest format, write a script to analyze it then if you wish, save the processed data for further use. This is important because it always allows you to go back to the original raw data in case something happens Some suggested project folder structures might look like something below: Include a README.md (or README.txt ) file in each project folder to explain where the data and scripts can be found for certain analyses. Trust me, after a few years you will definitely forget\u2026 And don\u2019t forget to update the README regularly, an old README doesn\u2019t do anyone good! Either use full path names in scripts or be explicit about where the working directory is This is important to allow other people to run your code (or might even be helpful for you if you ever reorganize folders one day) Date your files, especially when you update an existing file. Write dates in YYYYMMDD format As much as possible, ensure that your processed data is \u201ctidy\u201d (see below). This doesn\u2019t work for all complex data types, but it should be a general norm to follow. Each variable must have its own column Each observation must have its own row Each value must have its own cell No color or highlighting No empty cells (fill with NA if necessary) Save data as plain text files ( .csv , .tsv , .txt etc. -- NOT .xls !!!)","title":"General"},{"location":"knowledge_base/best_practices/#r","text":"ALWAYS use namespaces before functions from packages (i.e. dplyr::filter() instead of filter() ) This includes ggplot2 and especially dplyr !!! Some packages have functions with the same name, so adding a namespace is crucial for reproducibility. Also, this helps other people read your code by knowing which functions came from which packages When piping with Tidyverse ( %>% ), press <Enter> to go to the next line after a pipe This makes your code more readable In fact, general practices state no more than 80-100 characters per line of code EVER to increase readability","title":"R"},{"location":"knowledge_base/best_practices/#rockfish","text":"You should be doing most (if not all) of your analyses in /vast/eande106/projects/yourName (not your home directory (i.e. /home/<jheid> )) Important Main exception: Nextflow temporary working directories should NOT be on /vast/eande106 but rather in the scratch space /scratch4/eande106/ (files get automatically deleted here periodically). A correctly setup bash profile file will take care of this.","title":"Rockfish"},{"location":"knowledge_base/best_practices/#quest","text":"You may still be doing some of your analyses in /projects/b1059/projects/yourName (not your home directory (i.e. /home/netid )) Important Main exception: Nextflow temporary working directories should NOT be on b1059 (it will fill us up!) but rather in the scratch space b1042 (files get automatically deleted here every 30 days). A correctly designed nextflow.config file will take care of this.","title":"QUEST"},{"location":"knowledge_base/best_practices/#python","text":"ALWAYS indent using spaces. Mixing spaces and tabs will case your script to fail Use clear and descriptive variable names The names should help someone reading the code track the logic of the coder Only use alphanumeric characters or underscores Never start a name with a number Comment as much as you can to help others (and future you) understand what each part of the code is for Make logical blocks of code into functions This helps with interpretation Anything that is done more than once should be a function Try to follow PEP8 style rules","title":"Python"},{"location":"knowledge_base/github/","text":"Git and Github \u00b6 Git and Github Introduction Using Git and Github The GitHub Flow 1. Clone/pull 2. Branch 3. Edit 4. Commit 5. Push 6. Pull request 7. Inspect 8. Merge Command Line git commands GitHub Flow Best Practices Resources Introduction \u00b6 Git is a version control software package similar to tracked changes and saving documents as \"final1.pdf\" and \"final2.pdf\" Github is a 3rd party web-based graphical interface that has a copy of the project that you and/or other people can push and pull from to work on the same code simultaneously. Keep in mind, its not like google docs, it doesn\u2019t update automatically, requires you to push changes and pull changes to the new computer. Note You must install git to use and create an account on Github. Check out this intro guide here Using Git and Github \u00b6 The Andersen Lab Github can be found here . As of the writing of this page, we have 236 \"repositories\" (or projects). Notice some projects are code like NemaScan and others are personal projects ( abamectin ) or manuscripts ( mol_eco_manuscript ). Anything can be a repo! There are two main ways to use Git: (1) on the command line (aka Terminal on Macs) or with a GUI (graphical user interface). While for new users it is usually easier to start with a GUI like Github Desktop , it can cause headaches later on. However, only a few basic commands are really necessary to get started using git on the command line, so don't be nervous! Important Git GUI like Github Desktop cannot be used with repositories on QUEST. If you are building a pipeline on QUEST, it is essential to get comfortable with using Git on the command line The GitHub Flow \u00b6 There are several different Git branching strategies, but the most popular for our lab is the \"GitHub Flow\". This 8 step process can help keep our pipelines flowing, functional, and organized. New to the GitHub flow? I highly recommend you try out this amazing tutorial to practice all the steps from beginning to end. Then start putting it in use for your own pipelines! The following 8 steps can be done on the command line or with a GUI. Below I will show the basic git commands for managing a repo on the command line, for more help you can find the slides from Code club at ~/Dropbox/AndersenLab/LabMeetings/CodeClub/20210326_KSE/20210326_slides.key Note When you are maintaining a project repo that only you are updating, it is less important to follow the GitHub Flow with creating short-lived branches. However, if you are developing/maintaining code that other people will use and/or working collaboratively this is an essential skill to master. 1. Clone/pull \u00b6 # Cloning - new repo cd < directory you want repo stored > git clone https://github.com/AndersenLab/code_club.git cd code_club # pulling - already cloned repo you want to get newest version of cd < directory of repo > git pull 2. Branch \u00b6 # create a new branch AND move to it git checkout -b <branch_name> # list all available branches git branch # move to a branch git checkout < name of branch > 3. Edit \u00b6 No code here... make any edits to the repo. 4. Commit \u00b6 # first step - add changed files to staging area git add <changed file> # OR add ALL files to staging area git add . # commit files in staging area git commit -m \"<some message about what changes you made>\" 5. Push \u00b6 # push changes to remote git push # when it is your first time pushing a new branch, it might prompt you to set an upstream branch: git push --set-upstream origin new_branch It's possible that you will get an error when trying to push due to someone else having made changes to the repo since you last pulled from it. To fix this, simply do a git pull . With any luck, there won't be any conflicts between your changes and the changes you just pulled. If that's the case, go ahead and git push . If there are conflicts, you will need to edit the conflicting files, save them, and commit them. Then push. Conflicts are marked by <<<<<<<< and >>>>>>>> marking the remote version of the code and your version. 6. Pull request \u00b6 I generally like to do this step online at github.com because I think it is useful to visually see the changes I made go to the repo site click the green \"compare and pull request\" button check the branches are right at the top: which branch is merging into which branch optional: assing reviewer and/or assignees on the right hand side. This is often useful when coding collaboratively update the title/comment for the pull request to let yourself and others know what changes were made and why 7. Inspect \u00b6 I generally like to do this step online at github.com because I think it is useful to visually see the changes I made. If you scroll down you should be able to see which files were changed and what exact changes were made. If there are merge conflicts, github will walk you through fixing them. 8. Merge \u00b6 When you are satisfied with your merge, click the green \"merge pull request\" button. Also make sure the delete the old branch when you are done as part of keeping the repo clean and clutter-free Note Good practice is to make a new branch to implement a new feature, then delete the branch once it has been merged. To start a new feature, open a NEW branch. Not as important on self-projects, but very important for collaboration Command Line git commands \u00b6 Basic git clone - clone remote repository git pull - pull most recent version from remote git add - add local files to be staged for remote git commit - stage/commit local changes git push - push local commits to remote Intermediate git branch - list all available branches git checkout - move to new branch git status - checks which branch you are on and if you have any unsaved changes git log - shows log of previous commits on current branch git diff - shows details of changes made For more, check out this tutorial, and others. GitHub Flow Best Practices \u00b6 Any code in the main branch should be deployable Create new descriptively-named branches off the main branch for new work such as feature/add-new-plot Commit new work to your local branches and regularly push work to the remote To request feedback or help, or when you think your work is ready to merge into the main branch, open a pull request After your work or feature has been reviewed and approved, it can be merged into the main branch Delete stale branches! Once your work has been merged into the main branch, it should be deployed immediately Note GitHub Flow is not the only branching strategy out there! This was a great article about the three most common strategies with pros and cons for why you might use each one. I challenge you to think aobut which strategy might be best for our lab moving forward and let's start a discussion about it! Resources \u00b6 This blog on the differences between git and github \"Git started\" using Git on the command line here Overview of top Git GUI from 2021 here Great intro video to the GitHub Flow HIGHLY RECOMMENDED introduction tutorial to GitHub Flow Amazing article on different git branch strategies here","title":"Git and Github"},{"location":"knowledge_base/github/#git_and_github","text":"Git and Github Introduction Using Git and Github The GitHub Flow 1. Clone/pull 2. Branch 3. Edit 4. Commit 5. Push 6. Pull request 7. Inspect 8. Merge Command Line git commands GitHub Flow Best Practices Resources","title":"Git and Github"},{"location":"knowledge_base/github/#introduction","text":"Git is a version control software package similar to tracked changes and saving documents as \"final1.pdf\" and \"final2.pdf\" Github is a 3rd party web-based graphical interface that has a copy of the project that you and/or other people can push and pull from to work on the same code simultaneously. Keep in mind, its not like google docs, it doesn\u2019t update automatically, requires you to push changes and pull changes to the new computer. Note You must install git to use and create an account on Github. Check out this intro guide here","title":"Introduction"},{"location":"knowledge_base/github/#using_git_and_github","text":"The Andersen Lab Github can be found here . As of the writing of this page, we have 236 \"repositories\" (or projects). Notice some projects are code like NemaScan and others are personal projects ( abamectin ) or manuscripts ( mol_eco_manuscript ). Anything can be a repo! There are two main ways to use Git: (1) on the command line (aka Terminal on Macs) or with a GUI (graphical user interface). While for new users it is usually easier to start with a GUI like Github Desktop , it can cause headaches later on. However, only a few basic commands are really necessary to get started using git on the command line, so don't be nervous! Important Git GUI like Github Desktop cannot be used with repositories on QUEST. If you are building a pipeline on QUEST, it is essential to get comfortable with using Git on the command line","title":"Using Git and Github"},{"location":"knowledge_base/github/#the_github_flow","text":"There are several different Git branching strategies, but the most popular for our lab is the \"GitHub Flow\". This 8 step process can help keep our pipelines flowing, functional, and organized. New to the GitHub flow? I highly recommend you try out this amazing tutorial to practice all the steps from beginning to end. Then start putting it in use for your own pipelines! The following 8 steps can be done on the command line or with a GUI. Below I will show the basic git commands for managing a repo on the command line, for more help you can find the slides from Code club at ~/Dropbox/AndersenLab/LabMeetings/CodeClub/20210326_KSE/20210326_slides.key Note When you are maintaining a project repo that only you are updating, it is less important to follow the GitHub Flow with creating short-lived branches. However, if you are developing/maintaining code that other people will use and/or working collaboratively this is an essential skill to master.","title":"The GitHub Flow"},{"location":"knowledge_base/github/#1_clonepull","text":"# Cloning - new repo cd < directory you want repo stored > git clone https://github.com/AndersenLab/code_club.git cd code_club # pulling - already cloned repo you want to get newest version of cd < directory of repo > git pull","title":"1. Clone/pull"},{"location":"knowledge_base/github/#2_branch","text":"# create a new branch AND move to it git checkout -b <branch_name> # list all available branches git branch # move to a branch git checkout < name of branch >","title":"2. Branch"},{"location":"knowledge_base/github/#3_edit","text":"No code here... make any edits to the repo.","title":"3. Edit"},{"location":"knowledge_base/github/#4_commit","text":"# first step - add changed files to staging area git add <changed file> # OR add ALL files to staging area git add . # commit files in staging area git commit -m \"<some message about what changes you made>\"","title":"4. Commit"},{"location":"knowledge_base/github/#5_push","text":"# push changes to remote git push # when it is your first time pushing a new branch, it might prompt you to set an upstream branch: git push --set-upstream origin new_branch It's possible that you will get an error when trying to push due to someone else having made changes to the repo since you last pulled from it. To fix this, simply do a git pull . With any luck, there won't be any conflicts between your changes and the changes you just pulled. If that's the case, go ahead and git push . If there are conflicts, you will need to edit the conflicting files, save them, and commit them. Then push. Conflicts are marked by <<<<<<<< and >>>>>>>> marking the remote version of the code and your version.","title":"5. Push"},{"location":"knowledge_base/github/#6_pull_request","text":"I generally like to do this step online at github.com because I think it is useful to visually see the changes I made go to the repo site click the green \"compare and pull request\" button check the branches are right at the top: which branch is merging into which branch optional: assing reviewer and/or assignees on the right hand side. This is often useful when coding collaboratively update the title/comment for the pull request to let yourself and others know what changes were made and why","title":"6. Pull request"},{"location":"knowledge_base/github/#7_inspect","text":"I generally like to do this step online at github.com because I think it is useful to visually see the changes I made. If you scroll down you should be able to see which files were changed and what exact changes were made. If there are merge conflicts, github will walk you through fixing them.","title":"7. Inspect"},{"location":"knowledge_base/github/#8_merge","text":"When you are satisfied with your merge, click the green \"merge pull request\" button. Also make sure the delete the old branch when you are done as part of keeping the repo clean and clutter-free Note Good practice is to make a new branch to implement a new feature, then delete the branch once it has been merged. To start a new feature, open a NEW branch. Not as important on self-projects, but very important for collaboration","title":"8. Merge"},{"location":"knowledge_base/github/#command_line_git_commands","text":"Basic git clone - clone remote repository git pull - pull most recent version from remote git add - add local files to be staged for remote git commit - stage/commit local changes git push - push local commits to remote Intermediate git branch - list all available branches git checkout - move to new branch git status - checks which branch you are on and if you have any unsaved changes git log - shows log of previous commits on current branch git diff - shows details of changes made For more, check out this tutorial, and others.","title":"Command Line git commands"},{"location":"knowledge_base/github/#github_flow_best_practices","text":"Any code in the main branch should be deployable Create new descriptively-named branches off the main branch for new work such as feature/add-new-plot Commit new work to your local branches and regularly push work to the remote To request feedback or help, or when you think your work is ready to merge into the main branch, open a pull request After your work or feature has been reviewed and approved, it can be merged into the main branch Delete stale branches! Once your work has been merged into the main branch, it should be deployed immediately Note GitHub Flow is not the only branching strategy out there! This was a great article about the three most common strategies with pros and cons for why you might use each one. I challenge you to think aobut which strategy might be best for our lab moving forward and let's start a discussion about it!","title":"GitHub Flow Best Practices"},{"location":"knowledge_base/github/#resources","text":"This blog on the differences between git and github \"Git started\" using Git on the command line here Overview of top Git GUI from 2021 here Great intro video to the GitHub Flow HIGHLY RECOMMENDED introduction tutorial to GitHub Flow Amazing article on different git branch strategies here","title":"Resources"},{"location":"knowledge_base/r/","text":"R \u00b6 R General R resources Andersen Lab R Packages linkagemapping COPASutils easysorter easyXpress easyFulcrum Using R on Rockfish Using R on Quest General R resources \u00b6 If you are looking for some help getting started with R or taking your R to the next level, check out these useful resources: Swirl - interactive R learning Tidyverse workshop and resources Andersen Lab R Knowledge base & Cheatsheet R-bloggers - tips and tricks Using Rprojects to organize scripts Using workflowR for reproducible work Using Rmarkdown to generate reports Also check out the lab_code slack channel for help/questions! Andersen Lab R Packages \u00b6 The Andersen lab maintains several R packages useful for high-throughput data analysis. linkagemapping \u00b6 This package includes all data and functions necessary to complete a mapping for the phenotype of your choice using the recombinant inbred lines from Andersen, et al. 2015 (G3) . Included with this package are the cross and map objects for this strain set as well a markers.rds file containing a lookup table for the physical positions of all markers used for mapping. To learn more about linkagemapping including how to install and use the package, check out the andersenlab/linkagemapping repo. Note Also check out the linkagemapping-nf repo for a reproducible Nextflow pipeline for linkage mapping and two-dimensional genome scans (scan2) for one or several traits. COPASutils \u00b6 The R package COPASutils provides a logical workflow for the reading, processing, and visualization of data obtained from the Union Biometrica Complex Object Parametric Analyzer and Sorter (COPAS) or the BioSorter large-particle flow cytometers. Data obtained from these powerful experimental platforms can be unwieldy, leading to difficulties in the ability to process and visualize the data using existing tools. Researchers studying small organisms, such as Caenorhabditis elegans, Anopheles gambiae, and Danio rerio, and using these devices will benefit from this streamlined and extensible R package. COPASutils offers a powerful suite of functions for the rapid processing and analysis of large high-throughput screening data sets. To learn more about COPASutils including how to install and use the package, check out the andersenlab/COPASutils repo and the COPASutils manuscript easysorter \u00b6 This package is effectively version 2 of the COPASutils package. This package is specialized for use with worms and includes additional functionality on top of that provided by COPASutils, including division of recorded objects by larval stage and the ability to regress out control phenotypes from those recorded in experimental conditions To learn more about easysorter including how to install and use the package, check out the andersenlab/easysorter repo. Here are some of the papers using easysorter : The first easysorter paper A Powerful New Quantitative Genetics Platform, Combining Caenorhabditis elegans High-Throughput Fitness Assays with a Large Collection of Recombinant Strains ( Andersen et al. 2015 ) The first \"V3\" easysorter paper The Gene scb-1 Underlies Variation in Caenorhabditis elegans Chemotherapeutic Responses ( Evans and Andersen 2020 ) The first dominance/hemizygosity easysorter paper A Novel Gene Underlies Bleomycin-Response Variation in Caenorhabditis elegans ( Brady et al. 2019 ) Almost every paper published from the lab has used easysorter, for more, check out our lab papers Note The easysorter package requires COPASutils installation as well. easyXpress \u00b6 This package is designed for the reading, processing, and visualization of images obtained from the Molecular Devices ImageExpress Nano Imager, and processed with CellProfiler's WormToolbox. To learn more about easyXpress including how to install and use the package, check out the andersenlab/easyXpress repo and the easyXpress manuscript . easyFulcrum \u00b6 This package is designed for processing and analyzing ecological sampling data generated using the Fulcrum mobile application. To learn more about how to use easyFulcrum, check out the andersenlab/easyFulcrum repo and the easyFulcrum manuscript . Using R on Rockfish \u00b6 Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. Fortunately, there are several ways to get around this issue: Docker container using Singularity - instead of using conda environments (or maybe in addition to), a docker image can be generated with R packages installed. The benefit is that docker images can be used on Rockfish or locally and minimal set up is required. The downside is that on some occasions there can be errors generating the docker image with incompatible package versions. In order to use this option on Rockfish, you need to load the Singularity module ( module load singularity/3.8.7 ) Library Paths - For several recent pipelines on Rockfish, we have gotten around using a docker container for R packages by installing the proper versions of R packages to a shared location ( /data/eande106/software/R_lib_3.6.0 ). With the following code, any lab member can load the R package version from that location when running the pipeline, causing no errors, even if they don't have the package installed in their local R. # to manually install a package to this specific folder # make sure to first load the correct R version, our pipelines are currently using 3.6.3 module load r/3.6.3 # open R R # install package install.packages(\"tidyverse\", lib = \"/data/eande106/software/R_lib_3.6.0\") # if you load the package normally, it won't work (unless you also have it installed in your path) library(tidyverse) # won't work # load the package from the specific folder library(tidyverse, lib.loc = \"/data/eande106/software/R_lib_3.6.0\") # or add the path to your local R library to make for easier loading .libPaths(c(\"/data/eande106/software/R_lib_3.6.0\", .libPaths() )) library(tidyverse) # works # you can add the following lines to a nextflow script to use these R packages # set a parameter for R library path in case it updates in the future params.R_libpath = \"/data/eande106/software/R_lib_3.6.0\" # add the .libPaths to the top of the script dynamically (we don't want it statically in case someone wants to use the pipeline outside of quest) echo \".libPaths(c(\\\\\"${params.R_libpath}\\\\\", .libPaths() ))\" | cat - ${workflow.projectDir}/bin/Script.R > Script.R Rscript --vanilla Script.R Running Rstudio on Rockfish The Open On Demand (OOD) portal allow users with active Rockfish allocations to use RStudio from a web browser. See the ARCH User Guide for an overview of the system. Go to the OOD Portal and select Rstudio from the list of Interactive Apps ( make sure you are connected with VPN if you are off campus ). Log in with your jheid and password just like you would on Rockfish. Note The version of R on the Rstudio browser can be selected from the versions available via module on Rockfish. You can set the working directory with setwd(\"path_to_directory\") and then open and save files and data in Rstudio just like you were using it locally on your computer -- but with data and files on Rockfish!! Using R on Quest \u00b6 Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. Fortunately, there are several ways to get around this issue: Docker container - instead of using conda environments (or maybe in addition to), a docker image can be generated with R packages installed. The benefit is that docker images can be used on Quest or locally and minimal set up is required. The downside is that on some occasions there can be errors generating the docker image with incompatible package versions. Library Paths - For several recent pipelines on Quest, we have gotten around using a docker container for R packages by installing the proper versions of R packages to a shared location ( /projects/b1059/software/R_lib_3.6.0 ). With the following code, any lab member can load the R package version from that location when running the pipeline, causing no errors, even if they don't have the package installed in their local R. # to manually install a package to this specific folder # make sure to first load the correct R version, our pipelines are currently using 3.6.0 module load R/3.6.0 # open R R # install package install.packages(\"tidyverse\", lib = \"/projects/b1059/software/R_lib_3.6.0\") # if you load the package normally, it won't work (unless you also have it installed in your path) library(tidyverse) # won't work # load the package from the specific folder library(tidyverse, lib.loc = \"/projects/b1059/software/R_lib_3.6.0\") # or add the path to your local R library to make for easier loading .libPaths(c(\"/projects/b1059/software/R_lib_3.6.0\", .libPaths() )) library(tidyverse) # works # you can add the following lines to a nextflow script to use these R packages # set a parameter for R library path in case it updates in the future params.R_libpath = \"/projects/b1059/software/R_lib_3.6.0\" # add the .libPaths to the top of the script dynamically (we don't want it statically in case someone wants to use the pipeline outside of quest) echo \".libPaths(c(\\\\\"${params.R_libpath}\\\\\", .libPaths() ))\" | cat - ${workflow.projectDir}/bin/Script.R > Script.R Rscript --vanilla Script.R Running Rstudio on QUEST The Quest Analytics Nodes allow users with active Quest allocations to use RStudio from a web browser. See Research Computing: Quest Analytics Nodes for an overview of the system. Go to the Rstudio browser ( make sure you are connected with VPN if you are off campus ). Log in with your netid and password just like you would on QUEST. Note The version of R on the Rstudio browser is currently 4.1.1, which is likely different from the version of R you run on QUEST. Therefore, you will need to re-install any packages you want to use in the browser. You can set the working directory with setwd(\"path_to_directory\") and then open and save files and data in Rstudio just like you were using it locally on your computer -- but with data and files on Quest!!","title":"R"},{"location":"knowledge_base/r/#r","text":"R General R resources Andersen Lab R Packages linkagemapping COPASutils easysorter easyXpress easyFulcrum Using R on Rockfish Using R on Quest","title":"R"},{"location":"knowledge_base/r/#general_r_resources","text":"If you are looking for some help getting started with R or taking your R to the next level, check out these useful resources: Swirl - interactive R learning Tidyverse workshop and resources Andersen Lab R Knowledge base & Cheatsheet R-bloggers - tips and tricks Using Rprojects to organize scripts Using workflowR for reproducible work Using Rmarkdown to generate reports Also check out the lab_code slack channel for help/questions!","title":"General R resources"},{"location":"knowledge_base/r/#andersen_lab_r_packages","text":"The Andersen lab maintains several R packages useful for high-throughput data analysis.","title":"Andersen Lab R Packages"},{"location":"knowledge_base/r/#linkagemapping","text":"This package includes all data and functions necessary to complete a mapping for the phenotype of your choice using the recombinant inbred lines from Andersen, et al. 2015 (G3) . Included with this package are the cross and map objects for this strain set as well a markers.rds file containing a lookup table for the physical positions of all markers used for mapping. To learn more about linkagemapping including how to install and use the package, check out the andersenlab/linkagemapping repo. Note Also check out the linkagemapping-nf repo for a reproducible Nextflow pipeline for linkage mapping and two-dimensional genome scans (scan2) for one or several traits.","title":"linkagemapping"},{"location":"knowledge_base/r/#copasutils","text":"The R package COPASutils provides a logical workflow for the reading, processing, and visualization of data obtained from the Union Biometrica Complex Object Parametric Analyzer and Sorter (COPAS) or the BioSorter large-particle flow cytometers. Data obtained from these powerful experimental platforms can be unwieldy, leading to difficulties in the ability to process and visualize the data using existing tools. Researchers studying small organisms, such as Caenorhabditis elegans, Anopheles gambiae, and Danio rerio, and using these devices will benefit from this streamlined and extensible R package. COPASutils offers a powerful suite of functions for the rapid processing and analysis of large high-throughput screening data sets. To learn more about COPASutils including how to install and use the package, check out the andersenlab/COPASutils repo and the COPASutils manuscript","title":"COPASutils"},{"location":"knowledge_base/r/#easysorter","text":"This package is effectively version 2 of the COPASutils package. This package is specialized for use with worms and includes additional functionality on top of that provided by COPASutils, including division of recorded objects by larval stage and the ability to regress out control phenotypes from those recorded in experimental conditions To learn more about easysorter including how to install and use the package, check out the andersenlab/easysorter repo. Here are some of the papers using easysorter : The first easysorter paper A Powerful New Quantitative Genetics Platform, Combining Caenorhabditis elegans High-Throughput Fitness Assays with a Large Collection of Recombinant Strains ( Andersen et al. 2015 ) The first \"V3\" easysorter paper The Gene scb-1 Underlies Variation in Caenorhabditis elegans Chemotherapeutic Responses ( Evans and Andersen 2020 ) The first dominance/hemizygosity easysorter paper A Novel Gene Underlies Bleomycin-Response Variation in Caenorhabditis elegans ( Brady et al. 2019 ) Almost every paper published from the lab has used easysorter, for more, check out our lab papers Note The easysorter package requires COPASutils installation as well.","title":"easysorter"},{"location":"knowledge_base/r/#easyxpress","text":"This package is designed for the reading, processing, and visualization of images obtained from the Molecular Devices ImageExpress Nano Imager, and processed with CellProfiler's WormToolbox. To learn more about easyXpress including how to install and use the package, check out the andersenlab/easyXpress repo and the easyXpress manuscript .","title":"easyXpress"},{"location":"knowledge_base/r/#easyfulcrum","text":"This package is designed for processing and analyzing ecological sampling data generated using the Fulcrum mobile application. To learn more about how to use easyFulcrum, check out the andersenlab/easyFulcrum repo and the easyFulcrum manuscript .","title":"easyFulcrum"},{"location":"knowledge_base/r/#using_r_on_rockfish","text":"Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. Fortunately, there are several ways to get around this issue: Docker container using Singularity - instead of using conda environments (or maybe in addition to), a docker image can be generated with R packages installed. The benefit is that docker images can be used on Rockfish or locally and minimal set up is required. The downside is that on some occasions there can be errors generating the docker image with incompatible package versions. In order to use this option on Rockfish, you need to load the Singularity module ( module load singularity/3.8.7 ) Library Paths - For several recent pipelines on Rockfish, we have gotten around using a docker container for R packages by installing the proper versions of R packages to a shared location ( /data/eande106/software/R_lib_3.6.0 ). With the following code, any lab member can load the R package version from that location when running the pipeline, causing no errors, even if they don't have the package installed in their local R. # to manually install a package to this specific folder # make sure to first load the correct R version, our pipelines are currently using 3.6.3 module load r/3.6.3 # open R R # install package install.packages(\"tidyverse\", lib = \"/data/eande106/software/R_lib_3.6.0\") # if you load the package normally, it won't work (unless you also have it installed in your path) library(tidyverse) # won't work # load the package from the specific folder library(tidyverse, lib.loc = \"/data/eande106/software/R_lib_3.6.0\") # or add the path to your local R library to make for easier loading .libPaths(c(\"/data/eande106/software/R_lib_3.6.0\", .libPaths() )) library(tidyverse) # works # you can add the following lines to a nextflow script to use these R packages # set a parameter for R library path in case it updates in the future params.R_libpath = \"/data/eande106/software/R_lib_3.6.0\" # add the .libPaths to the top of the script dynamically (we don't want it statically in case someone wants to use the pipeline outside of quest) echo \".libPaths(c(\\\\\"${params.R_libpath}\\\\\", .libPaths() ))\" | cat - ${workflow.projectDir}/bin/Script.R > Script.R Rscript --vanilla Script.R Running Rstudio on Rockfish The Open On Demand (OOD) portal allow users with active Rockfish allocations to use RStudio from a web browser. See the ARCH User Guide for an overview of the system. Go to the OOD Portal and select Rstudio from the list of Interactive Apps ( make sure you are connected with VPN if you are off campus ). Log in with your jheid and password just like you would on Rockfish. Note The version of R on the Rstudio browser can be selected from the versions available via module on Rockfish. You can set the working directory with setwd(\"path_to_directory\") and then open and save files and data in Rstudio just like you were using it locally on your computer -- but with data and files on Rockfish!!","title":"Using R on Rockfish"},{"location":"knowledge_base/r/#using_r_on_quest","text":"Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. Fortunately, there are several ways to get around this issue: Docker container - instead of using conda environments (or maybe in addition to), a docker image can be generated with R packages installed. The benefit is that docker images can be used on Quest or locally and minimal set up is required. The downside is that on some occasions there can be errors generating the docker image with incompatible package versions. Library Paths - For several recent pipelines on Quest, we have gotten around using a docker container for R packages by installing the proper versions of R packages to a shared location ( /projects/b1059/software/R_lib_3.6.0 ). With the following code, any lab member can load the R package version from that location when running the pipeline, causing no errors, even if they don't have the package installed in their local R. # to manually install a package to this specific folder # make sure to first load the correct R version, our pipelines are currently using 3.6.0 module load R/3.6.0 # open R R # install package install.packages(\"tidyverse\", lib = \"/projects/b1059/software/R_lib_3.6.0\") # if you load the package normally, it won't work (unless you also have it installed in your path) library(tidyverse) # won't work # load the package from the specific folder library(tidyverse, lib.loc = \"/projects/b1059/software/R_lib_3.6.0\") # or add the path to your local R library to make for easier loading .libPaths(c(\"/projects/b1059/software/R_lib_3.6.0\", .libPaths() )) library(tidyverse) # works # you can add the following lines to a nextflow script to use these R packages # set a parameter for R library path in case it updates in the future params.R_libpath = \"/projects/b1059/software/R_lib_3.6.0\" # add the .libPaths to the top of the script dynamically (we don't want it statically in case someone wants to use the pipeline outside of quest) echo \".libPaths(c(\\\\\"${params.R_libpath}\\\\\", .libPaths() ))\" | cat - ${workflow.projectDir}/bin/Script.R > Script.R Rscript --vanilla Script.R Running Rstudio on QUEST The Quest Analytics Nodes allow users with active Quest allocations to use RStudio from a web browser. See Research Computing: Quest Analytics Nodes for an overview of the system. Go to the Rstudio browser ( make sure you are connected with VPN if you are off campus ). Log in with your netid and password just like you would on QUEST. Note The version of R on the Rstudio browser is currently 4.1.1, which is likely different from the version of R you run on QUEST. Therefore, you will need to re-install any packages you want to use in the browser. You can set the working directory with setwd(\"path_to_directory\") and then open and save files and data in Rstudio just like you were using it locally on your computer -- but with data and files on Quest!!","title":"Using R on Quest"},{"location":"knowledge_base/shiny/","text":"R Shiny applications \u00b6 R Shiny applications Andersen Lab Shiny Applications PCR calculator HTA Dilutions Fine-map QTL NILs NIL browser Linkagemapping analysis How to start a new shiny app? Simple example Reactivity Publishing your shiny app to shinyapps.io Shiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions. Andersen Lab Shiny Applications \u00b6 PCR calculator \u00b6 An R shiny web app developed for calculating PCR reagents. Link to application: here Link to github page with code and explanation of functionality: NA HTA Dilutions \u00b6 An R shiny web app developed to calculate drug dilutions for the high-throughput drug-response assays (sorter or imager). Link to application: here Link to github page with code and explanation of functionality: here Fine-map QTL NILs \u00b6 An R shiny web app developed to visualize the results from the high-throughput assays (specifically NIL results for fine-mapping a QTL). Link to application: here Link to github page with code and explanation of functionality: here NIL browser \u00b6 An R shiny web app developed to 1) visualize NIL genotypes and 2) find existing NILs for a project. Link to application: here Link to github page with code and explanation of functionality: NA Linkagemapping analysis \u00b6 An R shiny web app developed to visualize the results from the Andersen Lab linkagemapping experiments in 2014. Link to application: here Link to github page with code and explanation of functionality: here How to start a new shiny app? \u00b6 If you already know R, getting started in shiny just requires learning a few new concepts and some new functions/syntax. Check out this great tutorial to learn more (great video!)! Getting started You can create a shiny application in Rstudio by clicking File > New File > Shiny Web App . Rstudio will install any packages necessary (like shiny ) and then ask if you want your application to be in one file called app.R or two files: ui.R and server.R . Either way is okay. If you have a large, complex application, it might be easier to split up the UI (user interface) and the server (the meat of the application). Just like with Rmarkdown, a new Rshiny application comes preloaded with some basic code for you to get a look at. To run a shiny application from Rstudio, simply press the green \"run application\" button in the upper right of the script panel. A new window should pop up with the application. You can also choose at this point to \"open in browser\" instead by selecting that button in the upper right of the shiny app. You now have the beginning of a shiny app! You can add new visual elements to your application in the ui.R or in the shiny::ui() function in app.R and you can create the objects, plots, and manipulate data in the server . Here are some of the most commonly used objects (for a full list, check out this page ): ** Inputs You can create an input using one of the following functions in the ui , then use that input in the sever with input$input_id * shiny::radioButtons() * shiny::textInput() * shiny::selectInput() * shiny::sliderInput() * shiny::fileInput() * shiny::actionButton() Outputs You must create an output in the server and then display that output in the ui : | Create ouput in server | Show output in UI | | --- | --- | | shiny::renderPlot() | shiny::plotOutput() | | shiny::renderTable() | shiny::tableOutput() | | shiny::renderUI() | shiny::uiOutput() | | shiny::renderText() | shiny::textOutput() | Simple example \u00b6 library(shiny) # Define UI for application that draws a histogram ui <- shiny::fluidPage( # Application title shiny::titlePanel(\"Old Faithful Geyser Data\"), # Sidebar with a slider input for number of bins shiny::sidebarLayout( shiny::sidebarPanel( shiny::sliderInput(\"bins\", \"Number of bins:\", min = 1, max = 50, value = 30) ), # Show a plot of the generated distribution shiny::mainPanel( shiny::plotOutput(\"distPlot\") ) ) ) # Define server logic required to draw a histogram server <- function(input, output) { output$distPlot <- shiny::renderPlot({ # generate bins based on input$bins from ui.R x <- faithful[, 2] bins <- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = 'darkgray', border = 'white') }) } # Run the application shiny::shinyApp(ui = ui, server = server) Reactivity \u00b6 Check out this great overview on reactivity , the cornerstone of shiny applications. It talks about how the inputs are related to outputs and when outputs update in response to inputs. Publishing your shiny app to shinyapps.io \u00b6 After testing your new shiny app in Rstudio, you might be ready to deploy to the web for other people to access! The Andersen Lab has their own shinyapps.io account, so if you are making a lab-related app it is best to use this account (for login details, ask Robyn!) Publishing is simple - press the \"publish\" button in the upper right-hand corner of your running application and follow the prompts to select the right account. Once published, your application will be available at https://andersen-lab.shinyapps.io/{your_app_name}","title":"R Shiny"},{"location":"knowledge_base/shiny/#r_shiny_applications","text":"R Shiny applications Andersen Lab Shiny Applications PCR calculator HTA Dilutions Fine-map QTL NILs NIL browser Linkagemapping analysis How to start a new shiny app? Simple example Reactivity Publishing your shiny app to shinyapps.io Shiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions.","title":"R Shiny applications"},{"location":"knowledge_base/shiny/#andersen_lab_shiny_applications","text":"","title":"Andersen Lab Shiny Applications"},{"location":"knowledge_base/shiny/#pcr_calculator","text":"An R shiny web app developed for calculating PCR reagents. Link to application: here Link to github page with code and explanation of functionality: NA","title":"PCR calculator"},{"location":"knowledge_base/shiny/#hta_dilutions","text":"An R shiny web app developed to calculate drug dilutions for the high-throughput drug-response assays (sorter or imager). Link to application: here Link to github page with code and explanation of functionality: here","title":"HTA Dilutions"},{"location":"knowledge_base/shiny/#fine-map_qtl_nils","text":"An R shiny web app developed to visualize the results from the high-throughput assays (specifically NIL results for fine-mapping a QTL). Link to application: here Link to github page with code and explanation of functionality: here","title":"Fine-map QTL NILs"},{"location":"knowledge_base/shiny/#nil_browser","text":"An R shiny web app developed to 1) visualize NIL genotypes and 2) find existing NILs for a project. Link to application: here Link to github page with code and explanation of functionality: NA","title":"NIL browser"},{"location":"knowledge_base/shiny/#linkagemapping_analysis","text":"An R shiny web app developed to visualize the results from the Andersen Lab linkagemapping experiments in 2014. Link to application: here Link to github page with code and explanation of functionality: here","title":"Linkagemapping analysis"},{"location":"knowledge_base/shiny/#how_to_start_a_new_shiny_app","text":"If you already know R, getting started in shiny just requires learning a few new concepts and some new functions/syntax. Check out this great tutorial to learn more (great video!)! Getting started You can create a shiny application in Rstudio by clicking File > New File > Shiny Web App . Rstudio will install any packages necessary (like shiny ) and then ask if you want your application to be in one file called app.R or two files: ui.R and server.R . Either way is okay. If you have a large, complex application, it might be easier to split up the UI (user interface) and the server (the meat of the application). Just like with Rmarkdown, a new Rshiny application comes preloaded with some basic code for you to get a look at. To run a shiny application from Rstudio, simply press the green \"run application\" button in the upper right of the script panel. A new window should pop up with the application. You can also choose at this point to \"open in browser\" instead by selecting that button in the upper right of the shiny app. You now have the beginning of a shiny app! You can add new visual elements to your application in the ui.R or in the shiny::ui() function in app.R and you can create the objects, plots, and manipulate data in the server . Here are some of the most commonly used objects (for a full list, check out this page ): ** Inputs You can create an input using one of the following functions in the ui , then use that input in the sever with input$input_id * shiny::radioButtons() * shiny::textInput() * shiny::selectInput() * shiny::sliderInput() * shiny::fileInput() * shiny::actionButton() Outputs You must create an output in the server and then display that output in the ui : | Create ouput in server | Show output in UI | | --- | --- | | shiny::renderPlot() | shiny::plotOutput() | | shiny::renderTable() | shiny::tableOutput() | | shiny::renderUI() | shiny::uiOutput() | | shiny::renderText() | shiny::textOutput() |","title":"How to start a new shiny app?"},{"location":"knowledge_base/shiny/#simple_example","text":"library(shiny) # Define UI for application that draws a histogram ui <- shiny::fluidPage( # Application title shiny::titlePanel(\"Old Faithful Geyser Data\"), # Sidebar with a slider input for number of bins shiny::sidebarLayout( shiny::sidebarPanel( shiny::sliderInput(\"bins\", \"Number of bins:\", min = 1, max = 50, value = 30) ), # Show a plot of the generated distribution shiny::mainPanel( shiny::plotOutput(\"distPlot\") ) ) ) # Define server logic required to draw a histogram server <- function(input, output) { output$distPlot <- shiny::renderPlot({ # generate bins based on input$bins from ui.R x <- faithful[, 2] bins <- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = 'darkgray', border = 'white') }) } # Run the application shiny::shinyApp(ui = ui, server = server)","title":"Simple example"},{"location":"knowledge_base/shiny/#reactivity","text":"Check out this great overview on reactivity , the cornerstone of shiny applications. It talks about how the inputs are related to outputs and when outputs update in response to inputs.","title":"Reactivity"},{"location":"knowledge_base/shiny/#publishing_your_shiny_app_to_shinyappsio","text":"After testing your new shiny app in Rstudio, you might be ready to deploy to the web for other people to access! The Andersen Lab has their own shinyapps.io account, so if you are making a lab-related app it is best to use this account (for login details, ask Robyn!) Publishing is simple - press the \"publish\" button in the upper right-hand corner of your running application and follow the prompts to select the right account. Once published, your application will be available at https://andersen-lab.shinyapps.io/{your_app_name}","title":"Publishing your shiny app to shinyapps.io"},{"location":"other/backup/","text":"Backup data \u00b6 Backup files to google cloud \u00b6 1. Download and setup google cloud SDK - only need to do once # download from google curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-325.0.0-linux-x86_64.tar.gz # un-tar tar -xf google-cloud-sdk-325.0.0-linux-x86_64.tar.gz # add to path ./google-cloud-sdk/install.sh # initialize and authenticate gcloud init # follow prompts 2. Add files from QUEST folder to google cloud bucket You can see the structure of the google buckets by going here . Most things are currently in the elegansvariation.org bucket. # copy all files in current directory to specified bucket gsutil cp * gs://<YOUR_BUCKET_NAME> # copy all files in parallel (good for multiple files) gsutil -m cp * gs://<YOUR_BUCKET_NAME> # view list of files in bucket gsutil ls gs://<YOUR_BUCKET_NAME> # example - move all vcf files to the variation folder under 20210121 release gsutil -m cp * gs://elegansvarition.org/releases/20210121/variation Note We store .bam and .vcf files on google because they are used by CeNDR. The .bam files are not separated by release but the .vcf files (and accompanying files for a CeNDR release) are. Local Backup \u00b6 We also store all .fastq files locally in duplicate. If anything ever happens to any downstream data we can always recreate it with the original FASTQ files. This step is extremely important . A list of all data and which hardrive it is backed up on can be found here . The main copies of all data can be found on Pangolin, Armadillo, Raven, Karasu, and Turkey Choose a hardrive that has enough space for the data you need to back up and plug it in to your computer Change directories on your local computer into the hardrive cd /Volumes/{name_of_harddrive}/{path} Sync data from QUEST with Rsync -avh <netid>@quest.northwestern.edu:<path_to_folder_quest> . . Don't forget the '.'!!! When finished, check that the total file size is the same on the hard drive and on QUEST, you can always re-run the Rsync command to verify it is complete. Repeat with the second hard drive Complete the data backup google sheet with your new backup. Adding FASTQ to SRA project \u00b6 Checkout this guide for how to upload data to SRA. This should be done with wild isolate FASTQ files after each new CeNDR release. The SRA submission ID might need to be cited in publications.","title":"Backup"},{"location":"other/backup/#backup_data","text":"","title":"Backup data"},{"location":"other/backup/#backup_files_to_google_cloud","text":"1. Download and setup google cloud SDK - only need to do once # download from google curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-325.0.0-linux-x86_64.tar.gz # un-tar tar -xf google-cloud-sdk-325.0.0-linux-x86_64.tar.gz # add to path ./google-cloud-sdk/install.sh # initialize and authenticate gcloud init # follow prompts 2. Add files from QUEST folder to google cloud bucket You can see the structure of the google buckets by going here . Most things are currently in the elegansvariation.org bucket. # copy all files in current directory to specified bucket gsutil cp * gs://<YOUR_BUCKET_NAME> # copy all files in parallel (good for multiple files) gsutil -m cp * gs://<YOUR_BUCKET_NAME> # view list of files in bucket gsutil ls gs://<YOUR_BUCKET_NAME> # example - move all vcf files to the variation folder under 20210121 release gsutil -m cp * gs://elegansvarition.org/releases/20210121/variation Note We store .bam and .vcf files on google because they are used by CeNDR. The .bam files are not separated by release but the .vcf files (and accompanying files for a CeNDR release) are.","title":"Backup files to google cloud"},{"location":"other/backup/#local_backup","text":"We also store all .fastq files locally in duplicate. If anything ever happens to any downstream data we can always recreate it with the original FASTQ files. This step is extremely important . A list of all data and which hardrive it is backed up on can be found here . The main copies of all data can be found on Pangolin, Armadillo, Raven, Karasu, and Turkey Choose a hardrive that has enough space for the data you need to back up and plug it in to your computer Change directories on your local computer into the hardrive cd /Volumes/{name_of_harddrive}/{path} Sync data from QUEST with Rsync -avh <netid>@quest.northwestern.edu:<path_to_folder_quest> . . Don't forget the '.'!!! When finished, check that the total file size is the same on the hard drive and on QUEST, you can always re-run the Rsync command to verify it is complete. Repeat with the second hard drive Complete the data backup google sheet with your new backup.","title":"Local Backup"},{"location":"other/backup/#adding_fastq_to_sra_project","text":"Checkout this guide for how to upload data to SRA. This should be done with wild isolate FASTQ files after each new CeNDR release. The SRA submission ID might need to be cited in publications.","title":"Adding FASTQ to SRA project"},{"location":"other/caendr/","text":"CeNDR \u00b6 CeNDR CeNDR User privileges Updating Staff/Collaborator/Committee Profiles Updating Publications Updating Site Tools Creating a new release Uploading BAMs to Google Storage Adding isotype images Uploading Release Data to Google Storage Adding the release to the CeNDR website Adding Strain Variant Annotation Data Google Storage Details caendr-photos-bucket caendr-db-bucket caendr-main-terraform-state caendr-nextflow-work-bucket caendr-site-private-bucket caendr-site-public-bucket caendr-site-static-bucket caendr-src-bucket CeNDR User privileges \u00b6 To view, modify, or edit a user account, navigate to the 'Admin -> Users' menu. You can promote existing users to 'Admin' through this form as well. Updating Staff/Collaborator/Committee Profiles \u00b6 To modify the personal profiles of individuals associated with the project on the 'Staff', 'Scientific Advisory Committee', and 'Collaborators' pages: Admin -> Profile Pages From there you can create, modify, or delete user profiles and select on which page the profile should be published. Updating Publications \u00b6 The publications page ( /about/publications ) is generated using a google spreadsheet. The spreadsheet can be accessed here or through the 'Admin' menu. You can request access to edit the spreadsheet by visiting that link. The last row of the spreadsheet contains a function that can fetch publication data from Pubmed using its API. Simply fill in column A with the PMID (Pubmed Identifier), and the publication data will be fetched. Once you have retrieved the latest pubmed data, create a new row and copy/paste the values for any new publications so they are not fetched from the Pubmed API. Alternatively, you can fill in the details for a publication manually. In either case, any details added should be double checked. Changes should be instant, but there may be some dely on the CeNDR website. Updating Site Tools \u00b6 To change the version of the container that CeNDR uses for a tool: Admin -> Tool Versions The container versions are populated from the andersenlab docker hub repository. Updating the selected container version tag will switch the version that CeNDR uses for all future operations. Creating a new release \u00b6 Before a new release is possible, you must have first completed the following tasks: See Pipeline Overview for details . Add new wild isolate sequence data, and process with the trim-fq-nf pipeline. Align reads with alignment-nf . Call variants with wi-gatk . Identify new isotypes using the concordance-nf . Update the C. elegans WI Strain Info spreadsheet with the new isotypes and update the release column to reflect the release date. Perform population genetic analysis with post-gatk-nf . Impute the VCF. Annotate the VCF with the annotation-nf pipeline. Pushing a new release requires a series of steps described below. Uploading BAMs to Google Storage \u00b6 You will need Google Cloud credentials to upload BAMs to Google Storage. Install the Google Cloud SDK and configure with your GCP credentials and the caendr project ID. gcloud init Once configured, navigate to the BAM location on b1059. # CD to bams folder... cd /projects/b1059/data/c_elegans/WI/alignments/ Run this command in screen to ensure that it completes (it's going to take a while) gsutil rsync . gs://caendr-site-private-bucket/bam/c_elegans/ Adding isotype images \u00b6 Isolation photos are initially prepared on dropbox and are located in the folder here: ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans Each file should be named using the isotype name and the strain name strain name in the following format: <strain>.jpg Upload the image files to Google Storage. Thumbnails for the images will be automatically generated by a cloud function that monitors the photos bucket. Images should be uploaded to: gs://caendr-photos-bucket/c_elegans You can drag/drop the photos using the web-based browser or use gsutil : # First cd to the appropriate directory cd ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans gsutil rsync -x \".DS_Store\" . gs://caendr-photos-bucket/c_elegans Uploading Release Data to Google Storage \u00b6 When you run the wi-gatk pipeline it will create a folder with the format WI-YYYYMMDD . These data are output in a format that CeNDR can read as a release. You must upload the WI-YYYYMMDD folder to google storage with a command that looks like this: Note Data from several different pipelines are combined to form a CeNDR data release. Check the bottom of the page of each pipeline to see what data will be incorporated. # first cd into the folder you want to upload gsutil rsync . gs://caendr-site-public-bucket/dataset_release/c_elegans/20210121/ Important Use rsync to copy the files up to google storage. Note that the WI- prefix has been dropped from the YYYYMMDD declaration. There are 2 different expected filename and directory structures for dataset releases, V1 (the legacy format) and V2 (the current format). You will need to select the appropriate release format when adding a new dataset release through the admin panel. These directories may contain additional data not listed here, but these are the files that are referenced by CeNDR. Substitute [RELEASE_VERSION] in each filename with the version number for the dataset release ex: 20210121 V2 Structure: [RELEASE_VERSION]/alignment_report.html [RELEASE_VERSION]/concordance_report.html [RELEASE_VERSION]/divergent_regions_strain.[RELEASE_VERSION].bed [RELEASE_VERSION]/divergent_regions_all.[RELEASE_VERSION].bed [RELEASE_VERSION]/gatk_report.html [RELEASE_VERSION]/haplotype/haplotype.png [RELEASE_VERSION]/haplotype/haplotype.pdf [RELEASE_VERSION]/haplotype/sweep.pdf [RELEASE_VERSION]/haplotype/sweep_summary.tsv [RELEASE_VERSION]/methods.md [RELEASE_VERSION]/release_notes.md [RELEASE_VERSION]/tree/WI.[RELEASE_VERSION].hard-filter.min4.tree [RELEASE_VERSION]/tree/WI.[RELEASE_VERSION].hard-filter.min4.tree.pdf [RELEASE_VERSION]/tree/WI.[RELEASE_VERSION].hard-filter.isotype.min4.tree [RELEASE_VERSION]/tree/WI.[RELEASE_VERSION].hard-filter.isotype.min4.tree.pdf [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].soft-filter.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].soft-filter.vcf.gz.tbi [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].soft-filter.isotype.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].soft-filter.isotype.vcf.gz.tbi [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].hard-filter.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].hard-filter.vcf.gz.tbi [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].hard-filter.isotype.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].hard-filter.isotype.vcf.gz.tbi [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].impute.isotype.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].impute.isotype.vcf.gz.tbi V1 Structure: [RELEASE_VERSION]/methods.md [RELEASE_VERSION]/haplotype/haplotype.png [RELEASE_VERSION]/haplotype/haplotype.thumb.png [RELEASE_VERSION]/popgen/tajima_d.png [RELEASE_VERSION]/popgen/tajima_d.thumb.png [RELEASE_VERSION]/popgen/trees/genome.svg [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].soft-filter.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].hard-filter.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].impute.vcf.gz [RELEASE_VERSION]/multiqc_bcftools_stats.json [RELEASE_VERSION]/popgen/trees/genome.pdf Adding the release to the CeNDR website \u00b6 To publish the release data on the CeNDR website, you must be logged into the CeNDR website as an Admin user. If you do not see the 'Admin' menu in the navbar menu of the site, request administrative privileges for your CeNDR user account. Perform the following steps in order: 1. Updating the site's internal database of Strains (~5 minutes). This data is populated from the C. elegans WI Strain Info Google Sheet linked in the 'Admin' menu. Confirm this data is accurate and formatted correctly before begin the import operation. While the table is being updated, some portions of the site may have errors or display incorrect data. The status of the table update operation is shown on the Database Operations Admin page, but that status only reflects the success of the database operation - you should also verify the correctness of the updated data. The 'strain' table is used to as the data source for the 'Strain Catalog', 'Istotype List', 'Strain Map', 'Strain Issues', etc... Admin -> Database Operations Click the 'New Operation' button Select 'Rebuild strain table from Google Sheet' Add any (optional) notes about why the operation is being run Click 'Start' 2. (Optional) If the wormbase version used in the release is different from the previous release, that gene data must also be loaded into the site's internal database (~15 minutes). Gene data is compiled from several external databases and used to look up chromosome:start-stop intervals using Wormbase Gene IDs, gene names, Homologenes from other species, etc..: Admin -> Database Operations Click the 'New Operation' button Select 'Rebuild wormbase gene table from external sources' Enter the wormbase version number to use (ie: 280 to use version WS280) Click 'Start' 3. Finally, you can publish the release files on the site: Admin -> Dataset Releases Click 'Create Release' Enter the RELEASE_VERSION of the release that was uploaded to Google Storage in previous section (format: YYYYMMDD) Enter the WORMBASE_VERSION that the release uses (ie: 280 to use version WS280) Leave the Report Type as 'V2' for new releases (if you are adding a legacy format release created before 20200101, use V1) Click 'Save' The release should now show as a new tab on the 'Data -> Genomic Data' page Adding Strain Variant Annotation Data \u00b6 Strain Variant Annotation data must first be gzipped and uploaded to Google Storage. The CSV should be named with the pattern: WI.strain-annotation.bcsq.[VERSION_NUMBER].csv Substitute [RELEASE_VERSION] in the filename with the version number for the dataset release in the form YYYYMMDD ex: 20210401 # First cd to the appropriate directory gzip WI.strain-annotation.bcsq.[VERSION_NUMBER].csv gsutil cp WI.strain-annotation.bcsq.[VERSION_NUMBER].csv.gz gs://caendr-db-bucket/strain_variant_annotation/c_elegans/WI.strain-annotation.bcsq.[VERSION_NUMBER].csv.gz You can also upload the gzipped file directly through the Google Cloud Console to this locations: gs://caendr-db-bucket/strain_variant_annotation/c_elegans/ Then update the CeNDR tool through the 'Admin' portal (This operation may take a long time to complete ~24hrs): Admin -> Database Operations Click the 'New Operation' button Select 'Rebuild Strain Annotated Variant table from .csv.gz file' Enter the [VERSION_NUMBER] to use (ie: 20210401) Click 'Start' Google Storage Details \u00b6 caendr-photos-bucket \u00b6 This bucket contains photos of the environment from where a strain was isolated. Photos follow the naming pattern: 'gs:// / / .jpg' Examples: 'gs://caendr-photos-bucket/c_elegans/ECA1217.jpg' 'gs://caendr-photos-bucket/c_elegans/WN2082.jpg' When an image is uploaded to the bucket, a scaled down thumbnail will automatically be generated with the naming pattern: ' .thumb.jpg' Examples: 'gs://caendr-photos-bucket/c_elegans/ECA1217.thumb.jpg' 'gs://caendr-photos-bucket/c_elegans/WN2082.thumb.jpg' caendr-db-bucket \u00b6 This bucket is used for storing local backups of external databases (ie: Wormbase) and backing up CeNDR's internal database. Strain Variant Annotation data must be manually uploaded to the bucket before you can update the 'Strain Variant Annotation' table in the CeNDR database through the 'Admin' portal. The uploaded CSV file must be gzipped. For the example file below, the database operation form would require version '20210401'. caendr-db-bucket strain_variant_annotation c_elegans WI.strain-annotation.bcsq.20210401.csv.gz caendr-main-terraform-state \u00b6 This bucket contains the state information about the cloud infrastructure for CeNDR as well as a zipped backup of the secret.env file caendr-nextflow-work-bucket \u00b6 Work bucket for storing intermediate files in between Nextflow stages caendr-site-private-bucket \u00b6 This bucket contains any files that may (now or in the future) require custom access permissions and should not necessarily be globally public. tools - This directory contains any data or file depencies for an associated tool. tools pairwise_indel_primer sv.20200815.bed.gz sv.20200815.bed.gz.tbi sv.20200815.vcf.gz sv.20200815.vcf.gz.csi sv.20200815.vcf.gz.tbi nemascan input_data all_species c_elegans annotations genotypes isotypes phenotypes c_briggsae ... c_tropicalis ... bam - contains the BAM and BAI files for each sequenced strain of each species. It also contains the 'Download All BAM/BAI Files' script that is automatically generated by CAENDR (bam_bai_signed_download_script.sh) bam c_elegans AB1.bam AB1.bam.bai AB4.bam AB4.bam.bai etc... reports - This directory contains the source data and the output from running the associated tool. The data and results are organized by the container and version (with the exception of the indel primer), and the hash of the tool's input data. reports heritability v0.01a 15251d73f683450317089dae6736dee3 data.tsv result.tsv indel-primer 00d67eecc8a9ea13b7eb5e615b3a6860 input.json result.tsv nemascan-nxf v1.0 00d67eecc8a9ea13b7eb5e615b3a6860 data.tsv results Divergent_and_haplotype Genotype_Matrix Mapping Nextflow Phenotypes Plots Reports v1.0a 3facbf334a99a2cde3b6c4372cbe7da6 data.tsv results Divergent_and_haplotype Genotype_Matrix Mapping Nextflow Phenotypes Plots Reports caendr-site-public-bucket \u00b6 This bucket contains files that have been manually uploaded to the bucket (following an expected naming convention) or created through the 'Admin' portal. The content of several pages on CeNDR depends on these files and their contents. dataset_release - This is the directory where formal releases of CeNDR data are uploaded dataset_release c_elegans 20160408 20170531 20180527 20200815 20210121 release_notes.md methods.md alignment_report.html gatk_report.html concordance_report.html divergent_regions_strain.20210121.bed.gz variation WI.20210121.soft-filter.vcf.gz WI.20210121.soft-filter.vcf.gz.tbi WI.20210121.soft-filter.isotype.vcf.gz WI.20210121.soft-filter.isotype.vcf.gz.tbi WI.20210121.hard-filter.vcf.gz WI.20210121.hard-filter.vcf.gz.tbi WI.20210121.hard-filter.isotype.vcf.gz WI.20210121.hard-filter.isotype.vcf.gz.tbi WI.20210121.impute.isotype.vcf.gz WI.20210121.impute.isotype.vcf.gz.tbi tree WI.20210121.hard-filter.min4.tree WI.20210121.hard-filter.min4.tree.pdf WI.20210121.hard-filter.isotype.min4.tree WI.20210121.hard-filter.isotype.min4.tree.pdf haplotype haplotype.png haplotype.pdf sweep.pdf sweep_summary.tsv Profile - User Profile photos for Staff, Advisory Committee, etc... are uploaded here when managed using the 'Admin' portal profile photos 0f7394fa0d48431884e673b34f4c4dea.jpg caendr-site-static-bucket \u00b6 This bucket contains static site resources like images, videos, example data, etc... These files are stored in the CAENDR git repo, but are not accessible from the CeNDR web server. Terraform will automatically upload them to the caendr-site-static-bucket. To reference any of these assets on a page, you can use the Jinja macro ext_asset() example: <img src=\"{{ ext_asset('img/logo.png') }}\"> caendr-src-bucket \u00b6 This bucket is used by Terraform during the deployment process to store source code before it gets provisioned","title":"CeNDR"},{"location":"other/caendr/#cendr","text":"CeNDR CeNDR User privileges Updating Staff/Collaborator/Committee Profiles Updating Publications Updating Site Tools Creating a new release Uploading BAMs to Google Storage Adding isotype images Uploading Release Data to Google Storage Adding the release to the CeNDR website Adding Strain Variant Annotation Data Google Storage Details caendr-photos-bucket caendr-db-bucket caendr-main-terraform-state caendr-nextflow-work-bucket caendr-site-private-bucket caendr-site-public-bucket caendr-site-static-bucket caendr-src-bucket","title":"CeNDR"},{"location":"other/caendr/#cendr_user_privileges","text":"To view, modify, or edit a user account, navigate to the 'Admin -> Users' menu. You can promote existing users to 'Admin' through this form as well.","title":"CeNDR User privileges"},{"location":"other/caendr/#updating_staffcollaboratorcommittee_profiles","text":"To modify the personal profiles of individuals associated with the project on the 'Staff', 'Scientific Advisory Committee', and 'Collaborators' pages: Admin -> Profile Pages From there you can create, modify, or delete user profiles and select on which page the profile should be published.","title":"Updating Staff/Collaborator/Committee Profiles"},{"location":"other/caendr/#updating_publications","text":"The publications page ( /about/publications ) is generated using a google spreadsheet. The spreadsheet can be accessed here or through the 'Admin' menu. You can request access to edit the spreadsheet by visiting that link. The last row of the spreadsheet contains a function that can fetch publication data from Pubmed using its API. Simply fill in column A with the PMID (Pubmed Identifier), and the publication data will be fetched. Once you have retrieved the latest pubmed data, create a new row and copy/paste the values for any new publications so they are not fetched from the Pubmed API. Alternatively, you can fill in the details for a publication manually. In either case, any details added should be double checked. Changes should be instant, but there may be some dely on the CeNDR website.","title":"Updating Publications"},{"location":"other/caendr/#updating_site_tools","text":"To change the version of the container that CeNDR uses for a tool: Admin -> Tool Versions The container versions are populated from the andersenlab docker hub repository. Updating the selected container version tag will switch the version that CeNDR uses for all future operations.","title":"Updating Site Tools"},{"location":"other/caendr/#creating_a_new_release","text":"Before a new release is possible, you must have first completed the following tasks: See Pipeline Overview for details . Add new wild isolate sequence data, and process with the trim-fq-nf pipeline. Align reads with alignment-nf . Call variants with wi-gatk . Identify new isotypes using the concordance-nf . Update the C. elegans WI Strain Info spreadsheet with the new isotypes and update the release column to reflect the release date. Perform population genetic analysis with post-gatk-nf . Impute the VCF. Annotate the VCF with the annotation-nf pipeline. Pushing a new release requires a series of steps described below.","title":"Creating a new release"},{"location":"other/caendr/#uploading_bams_to_google_storage","text":"You will need Google Cloud credentials to upload BAMs to Google Storage. Install the Google Cloud SDK and configure with your GCP credentials and the caendr project ID. gcloud init Once configured, navigate to the BAM location on b1059. # CD to bams folder... cd /projects/b1059/data/c_elegans/WI/alignments/ Run this command in screen to ensure that it completes (it's going to take a while) gsutil rsync . gs://caendr-site-private-bucket/bam/c_elegans/","title":"Uploading BAMs to Google Storage"},{"location":"other/caendr/#adding_isotype_images","text":"Isolation photos are initially prepared on dropbox and are located in the folder here: ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans Each file should be named using the isotype name and the strain name strain name in the following format: <strain>.jpg Upload the image files to Google Storage. Thumbnails for the images will be automatically generated by a cloud function that monitors the photos bucket. Images should be uploaded to: gs://caendr-photos-bucket/c_elegans You can drag/drop the photos using the web-based browser or use gsutil : # First cd to the appropriate directory cd ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans gsutil rsync -x \".DS_Store\" . gs://caendr-photos-bucket/c_elegans","title":"Adding isotype images"},{"location":"other/caendr/#uploading_release_data_to_google_storage","text":"When you run the wi-gatk pipeline it will create a folder with the format WI-YYYYMMDD . These data are output in a format that CeNDR can read as a release. You must upload the WI-YYYYMMDD folder to google storage with a command that looks like this: Note Data from several different pipelines are combined to form a CeNDR data release. Check the bottom of the page of each pipeline to see what data will be incorporated. # first cd into the folder you want to upload gsutil rsync . gs://caendr-site-public-bucket/dataset_release/c_elegans/20210121/ Important Use rsync to copy the files up to google storage. Note that the WI- prefix has been dropped from the YYYYMMDD declaration. There are 2 different expected filename and directory structures for dataset releases, V1 (the legacy format) and V2 (the current format). You will need to select the appropriate release format when adding a new dataset release through the admin panel. These directories may contain additional data not listed here, but these are the files that are referenced by CeNDR. Substitute [RELEASE_VERSION] in each filename with the version number for the dataset release ex: 20210121 V2 Structure: [RELEASE_VERSION]/alignment_report.html [RELEASE_VERSION]/concordance_report.html [RELEASE_VERSION]/divergent_regions_strain.[RELEASE_VERSION].bed [RELEASE_VERSION]/divergent_regions_all.[RELEASE_VERSION].bed [RELEASE_VERSION]/gatk_report.html [RELEASE_VERSION]/haplotype/haplotype.png [RELEASE_VERSION]/haplotype/haplotype.pdf [RELEASE_VERSION]/haplotype/sweep.pdf [RELEASE_VERSION]/haplotype/sweep_summary.tsv [RELEASE_VERSION]/methods.md [RELEASE_VERSION]/release_notes.md [RELEASE_VERSION]/tree/WI.[RELEASE_VERSION].hard-filter.min4.tree [RELEASE_VERSION]/tree/WI.[RELEASE_VERSION].hard-filter.min4.tree.pdf [RELEASE_VERSION]/tree/WI.[RELEASE_VERSION].hard-filter.isotype.min4.tree [RELEASE_VERSION]/tree/WI.[RELEASE_VERSION].hard-filter.isotype.min4.tree.pdf [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].soft-filter.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].soft-filter.vcf.gz.tbi [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].soft-filter.isotype.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].soft-filter.isotype.vcf.gz.tbi [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].hard-filter.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].hard-filter.vcf.gz.tbi [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].hard-filter.isotype.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].hard-filter.isotype.vcf.gz.tbi [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].impute.isotype.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].impute.isotype.vcf.gz.tbi V1 Structure: [RELEASE_VERSION]/methods.md [RELEASE_VERSION]/haplotype/haplotype.png [RELEASE_VERSION]/haplotype/haplotype.thumb.png [RELEASE_VERSION]/popgen/tajima_d.png [RELEASE_VERSION]/popgen/tajima_d.thumb.png [RELEASE_VERSION]/popgen/trees/genome.svg [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].soft-filter.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].hard-filter.vcf.gz [RELEASE_VERSION]/variation/WI.[RELEASE_VERSION].impute.vcf.gz [RELEASE_VERSION]/multiqc_bcftools_stats.json [RELEASE_VERSION]/popgen/trees/genome.pdf","title":"Uploading Release Data to Google Storage"},{"location":"other/caendr/#adding_the_release_to_the_cendr_website","text":"To publish the release data on the CeNDR website, you must be logged into the CeNDR website as an Admin user. If you do not see the 'Admin' menu in the navbar menu of the site, request administrative privileges for your CeNDR user account. Perform the following steps in order: 1. Updating the site's internal database of Strains (~5 minutes). This data is populated from the C. elegans WI Strain Info Google Sheet linked in the 'Admin' menu. Confirm this data is accurate and formatted correctly before begin the import operation. While the table is being updated, some portions of the site may have errors or display incorrect data. The status of the table update operation is shown on the Database Operations Admin page, but that status only reflects the success of the database operation - you should also verify the correctness of the updated data. The 'strain' table is used to as the data source for the 'Strain Catalog', 'Istotype List', 'Strain Map', 'Strain Issues', etc... Admin -> Database Operations Click the 'New Operation' button Select 'Rebuild strain table from Google Sheet' Add any (optional) notes about why the operation is being run Click 'Start' 2. (Optional) If the wormbase version used in the release is different from the previous release, that gene data must also be loaded into the site's internal database (~15 minutes). Gene data is compiled from several external databases and used to look up chromosome:start-stop intervals using Wormbase Gene IDs, gene names, Homologenes from other species, etc..: Admin -> Database Operations Click the 'New Operation' button Select 'Rebuild wormbase gene table from external sources' Enter the wormbase version number to use (ie: 280 to use version WS280) Click 'Start' 3. Finally, you can publish the release files on the site: Admin -> Dataset Releases Click 'Create Release' Enter the RELEASE_VERSION of the release that was uploaded to Google Storage in previous section (format: YYYYMMDD) Enter the WORMBASE_VERSION that the release uses (ie: 280 to use version WS280) Leave the Report Type as 'V2' for new releases (if you are adding a legacy format release created before 20200101, use V1) Click 'Save' The release should now show as a new tab on the 'Data -> Genomic Data' page","title":"Adding the release to the CeNDR website"},{"location":"other/caendr/#adding_strain_variant_annotation_data","text":"Strain Variant Annotation data must first be gzipped and uploaded to Google Storage. The CSV should be named with the pattern: WI.strain-annotation.bcsq.[VERSION_NUMBER].csv Substitute [RELEASE_VERSION] in the filename with the version number for the dataset release in the form YYYYMMDD ex: 20210401 # First cd to the appropriate directory gzip WI.strain-annotation.bcsq.[VERSION_NUMBER].csv gsutil cp WI.strain-annotation.bcsq.[VERSION_NUMBER].csv.gz gs://caendr-db-bucket/strain_variant_annotation/c_elegans/WI.strain-annotation.bcsq.[VERSION_NUMBER].csv.gz You can also upload the gzipped file directly through the Google Cloud Console to this locations: gs://caendr-db-bucket/strain_variant_annotation/c_elegans/ Then update the CeNDR tool through the 'Admin' portal (This operation may take a long time to complete ~24hrs): Admin -> Database Operations Click the 'New Operation' button Select 'Rebuild Strain Annotated Variant table from .csv.gz file' Enter the [VERSION_NUMBER] to use (ie: 20210401) Click 'Start'","title":"Adding Strain Variant Annotation Data"},{"location":"other/caendr/#google_storage_details","text":"","title":"Google Storage Details"},{"location":"other/caendr/#caendr-photos-bucket","text":"This bucket contains photos of the environment from where a strain was isolated. Photos follow the naming pattern: 'gs:// / / .jpg' Examples: 'gs://caendr-photos-bucket/c_elegans/ECA1217.jpg' 'gs://caendr-photos-bucket/c_elegans/WN2082.jpg' When an image is uploaded to the bucket, a scaled down thumbnail will automatically be generated with the naming pattern: ' .thumb.jpg' Examples: 'gs://caendr-photos-bucket/c_elegans/ECA1217.thumb.jpg' 'gs://caendr-photos-bucket/c_elegans/WN2082.thumb.jpg'","title":"caendr-photos-bucket"},{"location":"other/caendr/#caendr-db-bucket","text":"This bucket is used for storing local backups of external databases (ie: Wormbase) and backing up CeNDR's internal database. Strain Variant Annotation data must be manually uploaded to the bucket before you can update the 'Strain Variant Annotation' table in the CeNDR database through the 'Admin' portal. The uploaded CSV file must be gzipped. For the example file below, the database operation form would require version '20210401'. caendr-db-bucket strain_variant_annotation c_elegans WI.strain-annotation.bcsq.20210401.csv.gz","title":"caendr-db-bucket"},{"location":"other/caendr/#caendr-main-terraform-state","text":"This bucket contains the state information about the cloud infrastructure for CeNDR as well as a zipped backup of the secret.env file","title":"caendr-main-terraform-state"},{"location":"other/caendr/#caendr-nextflow-work-bucket","text":"Work bucket for storing intermediate files in between Nextflow stages","title":"caendr-nextflow-work-bucket"},{"location":"other/caendr/#caendr-site-private-bucket","text":"This bucket contains any files that may (now or in the future) require custom access permissions and should not necessarily be globally public. tools - This directory contains any data or file depencies for an associated tool. tools pairwise_indel_primer sv.20200815.bed.gz sv.20200815.bed.gz.tbi sv.20200815.vcf.gz sv.20200815.vcf.gz.csi sv.20200815.vcf.gz.tbi nemascan input_data all_species c_elegans annotations genotypes isotypes phenotypes c_briggsae ... c_tropicalis ... bam - contains the BAM and BAI files for each sequenced strain of each species. It also contains the 'Download All BAM/BAI Files' script that is automatically generated by CAENDR (bam_bai_signed_download_script.sh) bam c_elegans AB1.bam AB1.bam.bai AB4.bam AB4.bam.bai etc... reports - This directory contains the source data and the output from running the associated tool. The data and results are organized by the container and version (with the exception of the indel primer), and the hash of the tool's input data. reports heritability v0.01a 15251d73f683450317089dae6736dee3 data.tsv result.tsv indel-primer 00d67eecc8a9ea13b7eb5e615b3a6860 input.json result.tsv nemascan-nxf v1.0 00d67eecc8a9ea13b7eb5e615b3a6860 data.tsv results Divergent_and_haplotype Genotype_Matrix Mapping Nextflow Phenotypes Plots Reports v1.0a 3facbf334a99a2cde3b6c4372cbe7da6 data.tsv results Divergent_and_haplotype Genotype_Matrix Mapping Nextflow Phenotypes Plots Reports","title":"caendr-site-private-bucket"},{"location":"other/caendr/#caendr-site-public-bucket","text":"This bucket contains files that have been manually uploaded to the bucket (following an expected naming convention) or created through the 'Admin' portal. The content of several pages on CeNDR depends on these files and their contents. dataset_release - This is the directory where formal releases of CeNDR data are uploaded dataset_release c_elegans 20160408 20170531 20180527 20200815 20210121 release_notes.md methods.md alignment_report.html gatk_report.html concordance_report.html divergent_regions_strain.20210121.bed.gz variation WI.20210121.soft-filter.vcf.gz WI.20210121.soft-filter.vcf.gz.tbi WI.20210121.soft-filter.isotype.vcf.gz WI.20210121.soft-filter.isotype.vcf.gz.tbi WI.20210121.hard-filter.vcf.gz WI.20210121.hard-filter.vcf.gz.tbi WI.20210121.hard-filter.isotype.vcf.gz WI.20210121.hard-filter.isotype.vcf.gz.tbi WI.20210121.impute.isotype.vcf.gz WI.20210121.impute.isotype.vcf.gz.tbi tree WI.20210121.hard-filter.min4.tree WI.20210121.hard-filter.min4.tree.pdf WI.20210121.hard-filter.isotype.min4.tree WI.20210121.hard-filter.isotype.min4.tree.pdf haplotype haplotype.png haplotype.pdf sweep.pdf sweep_summary.tsv Profile - User Profile photos for Staff, Advisory Committee, etc... are uploaded here when managed using the 'Admin' portal profile photos 0f7394fa0d48431884e673b34f4c4dea.jpg","title":"caendr-site-public-bucket"},{"location":"other/caendr/#caendr-site-static-bucket","text":"This bucket contains static site resources like images, videos, example data, etc... These files are stored in the CAENDR git repo, but are not accessible from the CeNDR web server. Terraform will automatically upload them to the caendr-site-static-bucket. To reference any of these assets on a page, you can use the Jinja macro ext_asset() example: <img src=\"{{ ext_asset('img/logo.png') }}\">","title":"caendr-site-static-bucket"},{"location":"other/caendr/#caendr-src-bucket","text":"This bucket is used by Terraform during the deployment process to store source code before it gets provisioned","title":"caendr-src-bucket"},{"location":"other/cloud/","text":"AndersenLab cloud resources \u00b6 AndersenLab cloud resources Google Domains Google Cloud Google Cloud Storage Buckets elegansvariation.org andersenlab.org Other buckets Secret Bucket cegwas (deprecated) Google datastore App engine Error Reporting BigQuery AWS S3 Fargate For full documentation visit mkdocs.org . Google Domains \u00b6 Any domain names the lab uses should be registered with Google Domains. The two ones currently are: andersenlab.org elegansvariation.org Google domains can be used to forward domain-specific email addresses if necessary. For example, example@andersenlab.org could be created and forwarded to an email address. Google Cloud \u00b6 Google cloud is used for a variety of services that the lab uses. Google Cloud Storage \u00b6 Google cloud storage is used to store and distribute files on CeNDR, for cegwas, and for some files on the lab website. Buckets \u00b6 Files are grouped into 'buckets' on google storage. We use the following buckets: elegansvariation.org \u00b6 This bucket contains all the data associated with elegansvariation.org. It is broken down into six primary directories. browser_tracks - for genome-browser tracks that rarely if ever change. db - Storage/access to the SQLite database. photos - sample collection photos. releases - dataset releases. For more detail, see post-gatk-nf . reports - images and data files within reports. static - static assets used by the site. bam - stores all BAM files at the strain level andersenlab.org \u00b6 In some cases the data associated with a publication is too large to put on github. We store those data here, along with a couple other odds and ends. Other buckets \u00b6 Google Cloud creates a bunch of other buckets too. Most of these should be ignored as they are part of the Secret Bucket \u00b6 There is one other secret bucket. Ask Erik about it. cegwas (deprecated) \u00b6 Currently this bucket has a SQLite database used by cegwas. This will be replaced to use the same database used by CeNDR in the db/ folder. Once the new version of cegwas is developed - this section should be removed, and that bucket should be deleted. Google datastore \u00b6 Google datastore used as the database for CeNDR. It stores information on mappings, traits, and more. App engine \u00b6 App engine is the platform CeNDR runs on. Error Reporting \u00b6 Google cloud contains a really nice error reporting interface. Error reports are generated whenever something goes wrong on CeNDR. A github issue can be created for these errors and they can be addressed. BigQuery \u00b6 We have used bigquery in the past for large query jobs. We are not actively using it as of late. AWS \u00b6 S3 \u00b6 In the past we stored BAMs on AWS at elegansvariation.org , however these data have now been migrated to GCP as of the 20210121 CeNDR release. Fargate \u00b6 Amazon Fargate was used to run the mapping pipeline on CeNDR in the past, but this is now moved to GCP as of 2021","title":"Cloud"},{"location":"other/cloud/#andersenlab_cloud_resources","text":"AndersenLab cloud resources Google Domains Google Cloud Google Cloud Storage Buckets elegansvariation.org andersenlab.org Other buckets Secret Bucket cegwas (deprecated) Google datastore App engine Error Reporting BigQuery AWS S3 Fargate For full documentation visit mkdocs.org .","title":"AndersenLab cloud resources"},{"location":"other/cloud/#google_domains","text":"Any domain names the lab uses should be registered with Google Domains. The two ones currently are: andersenlab.org elegansvariation.org Google domains can be used to forward domain-specific email addresses if necessary. For example, example@andersenlab.org could be created and forwarded to an email address.","title":"Google Domains"},{"location":"other/cloud/#google_cloud","text":"Google cloud is used for a variety of services that the lab uses.","title":"Google Cloud"},{"location":"other/cloud/#google_cloud_storage","text":"Google cloud storage is used to store and distribute files on CeNDR, for cegwas, and for some files on the lab website.","title":"Google Cloud Storage"},{"location":"other/cloud/#buckets","text":"Files are grouped into 'buckets' on google storage. We use the following buckets:","title":"Buckets"},{"location":"other/cloud/#elegansvariationorg","text":"This bucket contains all the data associated with elegansvariation.org. It is broken down into six primary directories. browser_tracks - for genome-browser tracks that rarely if ever change. db - Storage/access to the SQLite database. photos - sample collection photos. releases - dataset releases. For more detail, see post-gatk-nf . reports - images and data files within reports. static - static assets used by the site. bam - stores all BAM files at the strain level","title":"elegansvariation.org"},{"location":"other/cloud/#andersenlaborg","text":"In some cases the data associated with a publication is too large to put on github. We store those data here, along with a couple other odds and ends.","title":"andersenlab.org"},{"location":"other/cloud/#other_buckets","text":"Google Cloud creates a bunch of other buckets too. Most of these should be ignored as they are part of the","title":"Other buckets"},{"location":"other/cloud/#secret_bucket","text":"There is one other secret bucket. Ask Erik about it.","title":"Secret Bucket"},{"location":"other/cloud/#cegwas_deprecated","text":"Currently this bucket has a SQLite database used by cegwas. This will be replaced to use the same database used by CeNDR in the db/ folder. Once the new version of cegwas is developed - this section should be removed, and that bucket should be deleted.","title":"cegwas (deprecated)"},{"location":"other/cloud/#google_datastore","text":"Google datastore used as the database for CeNDR. It stores information on mappings, traits, and more.","title":"Google datastore"},{"location":"other/cloud/#app_engine","text":"App engine is the platform CeNDR runs on.","title":"App engine"},{"location":"other/cloud/#error_reporting","text":"Google cloud contains a really nice error reporting interface. Error reports are generated whenever something goes wrong on CeNDR. A github issue can be created for these errors and they can be addressed.","title":"Error Reporting"},{"location":"other/cloud/#bigquery","text":"We have used bigquery in the past for large query jobs. We are not actively using it as of late.","title":"BigQuery"},{"location":"other/cloud/#aws","text":"","title":"AWS"},{"location":"other/cloud/#s3","text":"In the past we stored BAMs on AWS at elegansvariation.org , however these data have now been migrated to GCP as of the 20210121 CeNDR release.","title":"S3"},{"location":"other/cloud/#fargate","text":"Amazon Fargate was used to run the mapping pipeline on CeNDR in the past, but this is now moved to GCP as of 2021","title":"Fargate"},{"location":"other/labsite/","text":"Andersenlab.org \u00b6 Andersenlab.org Getting Started Software-Dependencies Cloning the repo Updating the site andersenlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software Getting Started \u00b6 The Andersen Lab website was built using jekyll and runs using the Github Pages service. Software-Dependencies \u00b6 Several software packages are required for editing/maintaining the Andersen Lab site. They can be installed using Homebrew : brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml . Cloning the repo \u00b6 To get started editing, clone the repo: git clone https://github.com/andersenlab/andersenlab.github.io This repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url. Updating the site \u00b6 In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git. andersenlab.github.io \u00b6 The structure of the Andersen Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with Announcements \u00b6 Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication. General Announcements \u00b6 To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more. Publication Post \u00b6 New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Katie's paper accepted at <em>G3</em>!\" subtitle: \"Correlations of geneotype with climate parameters suggest <em>Caenorhabditis elegans</em> niche adaptations\" layout: post tags: news published: true PMID: 27866149 --- Congratulations to Katie for her paper accepted at G3! Lab members \u00b6 Adding new lab members: \u00b6 (1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page. Set Status to Former \u00b6 Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Mostafa pub_names: - Zamanian M last_name: Zamanian description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2015-2017 photo: Mostafa2014.jpg former: true github: mzamanian email: zamanian@northwestern.edu current_status: Assistant Professor at UW Madison -- <a href='http://www.zamanianlab.org/'>Zamanian Lab Website</a> Remove lab members \u00b6 Remove the persons information from _data/people.yaml ; Optionally delete their photo. Funding \u00b6 Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: nigms: https://www.nigms.nih.gov/Pages/default.aspx acs: http://www.cancer.org/ pew: http://www.pewtrusts.org/en niaid: https://www.niaid.nih.gov/ aws: https://aws.amazon.com/ weinberg: http://www.weinberg.northwestern.edu/ mod: http://www.marchofdimes.org/ cbc: http://www.chicagobiomedicalconsortium.org/ Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here. Protocols \u00b6 Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>C. elegans</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Brood Assay file: SemiQuantitativeBroodAssay.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Pseudomonas aeruginosa</em> Fast-killing assay</a> file: FKAprotocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Staphylococcus aureus</em> killing assay</a> file: Staphaureus_Protocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Bacillus thuringiensis</em> toxin assay on plates</a> file: Bacillus-thuringiensis-toxin-plate-assay.pdf group: <em>C. elegans</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines. Research \u00b6 The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: High-throughput approaches to understand conserved drug responses image: worms_drugs2.jpg order: 1 --- Because of the efforts of a number of highly dedicated scientists and citizen volunteers... To this end, we deep sequenced all of these strains... The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000). Publications \u00b6 Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the Caenorhabditis elegans species You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Caenorhabditis elegans</em> Final result: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Date_Published: 2017 May 09 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the <em>Caenorhabditis elegans</em> species PDF: 2017Laricchia (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2017Laricchia has an associated folder in publications/ where supplemental data and figures are stored: publications/2017Laricchia/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2017Laricchia : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page. Photo Albums \u00b6 Photo albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2017-08-05-Hawaii Trip . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the andersenlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push Software \u00b6 If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Andersen Labsite"},{"location":"other/labsite/#andersenlaborg","text":"Andersenlab.org Getting Started Software-Dependencies Cloning the repo Updating the site andersenlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software","title":"Andersenlab.org"},{"location":"other/labsite/#getting_started","text":"The Andersen Lab website was built using jekyll and runs using the Github Pages service.","title":"Getting Started"},{"location":"other/labsite/#software-dependencies","text":"Several software packages are required for editing/maintaining the Andersen Lab site. They can be installed using Homebrew : brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml .","title":"Software-Dependencies"},{"location":"other/labsite/#cloning_the_repo","text":"To get started editing, clone the repo: git clone https://github.com/andersenlab/andersenlab.github.io This repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url.","title":"Cloning the repo"},{"location":"other/labsite/#updating_the_site","text":"In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git.","title":"Updating the site"},{"location":"other/labsite/#andersenlabgithubio","text":"The structure of the Andersen Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with","title":"andersenlab.github.io"},{"location":"other/labsite/#announcements","text":"Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication.","title":"Announcements"},{"location":"other/labsite/#general_announcements","text":"To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more.","title":"General Announcements"},{"location":"other/labsite/#publication_post","text":"New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Katie's paper accepted at <em>G3</em>!\" subtitle: \"Correlations of geneotype with climate parameters suggest <em>Caenorhabditis elegans</em> niche adaptations\" layout: post tags: news published: true PMID: 27866149 --- Congratulations to Katie for her paper accepted at G3!","title":"Publication Post"},{"location":"other/labsite/#lab_members","text":"","title":"Lab members"},{"location":"other/labsite/#adding_new_lab_members","text":"(1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page.","title":"Adding new lab members:"},{"location":"other/labsite/#set_status_to_former","text":"Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Mostafa pub_names: - Zamanian M last_name: Zamanian description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2015-2017 photo: Mostafa2014.jpg former: true github: mzamanian email: zamanian@northwestern.edu current_status: Assistant Professor at UW Madison -- <a href='http://www.zamanianlab.org/'>Zamanian Lab Website</a>","title":"Set Status to Former"},{"location":"other/labsite/#remove_lab_members","text":"Remove the persons information from _data/people.yaml ; Optionally delete their photo.","title":"Remove lab members"},{"location":"other/labsite/#funding","text":"Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: nigms: https://www.nigms.nih.gov/Pages/default.aspx acs: http://www.cancer.org/ pew: http://www.pewtrusts.org/en niaid: https://www.niaid.nih.gov/ aws: https://aws.amazon.com/ weinberg: http://www.weinberg.northwestern.edu/ mod: http://www.marchofdimes.org/ cbc: http://www.chicagobiomedicalconsortium.org/ Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here.","title":"Funding"},{"location":"other/labsite/#protocols","text":"Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>C. elegans</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Brood Assay file: SemiQuantitativeBroodAssay.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Pseudomonas aeruginosa</em> Fast-killing assay</a> file: FKAprotocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Staphylococcus aureus</em> killing assay</a> file: Staphaureus_Protocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Bacillus thuringiensis</em> toxin assay on plates</a> file: Bacillus-thuringiensis-toxin-plate-assay.pdf group: <em>C. elegans</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines.","title":"Protocols"},{"location":"other/labsite/#research","text":"The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: High-throughput approaches to understand conserved drug responses image: worms_drugs2.jpg order: 1 --- Because of the efforts of a number of highly dedicated scientists and citizen volunteers... To this end, we deep sequenced all of these strains... The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000).","title":"Research"},{"location":"other/labsite/#publications","text":"Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the Caenorhabditis elegans species You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Caenorhabditis elegans</em> Final result: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Date_Published: 2017 May 09 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the <em>Caenorhabditis elegans</em> species PDF: 2017Laricchia (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2017Laricchia has an associated folder in publications/ where supplemental data and figures are stored: publications/2017Laricchia/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2017Laricchia : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page.","title":"Publications"},{"location":"other/labsite/#photo_albums","text":"Photo albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2017-08-05-Hawaii Trip . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the andersenlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push","title":"Photo Albums"},{"location":"other/labsite/#software","text":"If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Software"},{"location":"other/sharing-globus/","text":"Sharing files with Globus \u00b6 Sharing files with Globus Setting up a share Receiving a Globus share Setting up a share \u00b6 Sign in to Globus . The account doesn't have to be Northwestern-affiliated, but you will have to sign in with your Northwestern credentials to access Quest anyways, so might as well. In \"Collection\", search for \"Northwestern Quest\", then authenticate with your Northwestern account (if you didn't already in step 1). If you just authenticated, might need to search \"Northwestern Quest\" in the Collection again. Navigate to the folder you want to share, click \"Share\", and follow the prompts. Click \"Add Permissions - Share With\" and set permissions so that all users can read. Click \"Show link for sharing\", copy the link, and send to collaborators along with the link to this page walking them through downloading the files. Receiving a Globus share \u00b6 Click on the globus link you were sent, it should take you here: After you log in, the screen should look like this: If you have personal Globus endpoint installed already, skip to step 6 Click the search bar, then \"Install Globus Connect Personal\" Follow this page and prompts to install and set up personal endpoint. After installation, start Globus personal endpoint in Applications. Nothing will seem to happen, except a new icon will appear in the upper right corner of the computer (for Macs). Click on the icon and take note of the name of your personal end point. Click on the icon, then \"Preferences\". Only folders on this list can recieve files from Globus, so add new folders if necessary. Go back to the internet browser and click on \"File Manager\" at the top left. Click on the search bar on the right panel and type in the name of your personal end point (step), then select it. Select files you want to download from the left panel (they should look blue after selection), then navigate to the receiving folder on the right panel and select \"Start\" to begin transfer.","title":"Globus file sharing"},{"location":"other/sharing-globus/#sharing_files_with_globus","text":"Sharing files with Globus Setting up a share Receiving a Globus share","title":"Sharing files with Globus"},{"location":"other/sharing-globus/#setting_up_a_share","text":"Sign in to Globus . The account doesn't have to be Northwestern-affiliated, but you will have to sign in with your Northwestern credentials to access Quest anyways, so might as well. In \"Collection\", search for \"Northwestern Quest\", then authenticate with your Northwestern account (if you didn't already in step 1). If you just authenticated, might need to search \"Northwestern Quest\" in the Collection again. Navigate to the folder you want to share, click \"Share\", and follow the prompts. Click \"Add Permissions - Share With\" and set permissions so that all users can read. Click \"Show link for sharing\", copy the link, and send to collaborators along with the link to this page walking them through downloading the files.","title":"Setting up a share"},{"location":"other/sharing-globus/#receiving_a_globus_share","text":"Click on the globus link you were sent, it should take you here: After you log in, the screen should look like this: If you have personal Globus endpoint installed already, skip to step 6 Click the search bar, then \"Install Globus Connect Personal\" Follow this page and prompts to install and set up personal endpoint. After installation, start Globus personal endpoint in Applications. Nothing will seem to happen, except a new icon will appear in the upper right corner of the computer (for Macs). Click on the icon and take note of the name of your personal end point. Click on the icon, then \"Preferences\". Only folders on this list can recieve files from Globus, so add new folders if necessary. Go back to the internet browser and click on \"File Manager\" at the top left. Click on the search bar on the right panel and type in the name of your personal end point (step), then select it. Select files you want to download from the left panel (they should look blue after selection), then navigate to the receiving folder on the right panel and select \"Start\" to begin transfer.","title":"Receiving a Globus share"},{"location":"other/sra/","text":"Uploading WI FASTQ sequence data to SRA \u00b6 For each CENDR release, it is important to also upload the FASTQ files to NCBI's Sequence Read Archive (SRA). If a bioproject already exists, you can create a new submission and link to the previous bioproject . If there is no previous bioproject, you can create a new bioproject and add all relevant data. See below for more instructions. SRA submission \u00b6 Begin submission In the SRA submission portal , click the button for \"New submission\" and follow the prompts. Remember to add the previous bioproject ID if applicable to link this submission to previous submissions. Select \"Model organism or animal\" for biosample type, select \"upload file using excel\" and download the template Create biosample sheet (an example can be found below) An easy starting point here is the sample sheet used for alignment-nf . You will keep the id as the sample name (unique identifier) and strain (note strain also needs to be unique - if there are multiple library preps for the same strain, please append \"-2\" etc. to the strain) To these two columns, you can add bioproject, organism, developmental stage, sex, and tissue (same across all strains) Finally, join this data with the WI species master sheet to get collected by, collection date, and latitude/longitude. - Note: latitude/longitude need to be converted into one shared column in the format \"34.89 S 56.15 W\" (+ refers to North and East) Copy the data into the relevent columns in the template , save, and upload to the submission portal Create SRA metadata sheet Again, select \"upload a file using excel\" and download the template An easy starting point here is, again, the sample sheet used for alignment-nf . You will keep the id as the sample name and the lb as library id. You will also keep fq1 and fq2 for filename and filename2. You will then add the rest of the columns as shown below. Note: the formatting is very specific for this sheet. The title can be found on the bioproject page. The instrument_model can be found by using the \"sequencing_folder\" (not shown, but part of the original sample sheet) and looking up the instrument that folder was run on in the Sequencing Runs google sheet ( here ) Copy and paste the rows from this file into the template to check for correct formatting. Then save the tab as a tsv and upload to the submission portal. Pre-upload FASTQ files using FTP Create a list of files to upload to the FTP server by combining the filename1 and filename2 from the SRA metadata sheet (above). Begin submission by creating an NCBI account (or signing in -- personal account). Then follow the link to the SRA submission portal Follow the instructions under the \"FTP upload\": # establish FTP connection from terminal (on QUEST!) # ftp <address> ftp ftp-private.ncbi.nlm.nih.gov # navigate to your account folder (i.e.) cd uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4 # create new folder for submission (i.e.) mkdir 20210121_submission # exit FTP connection exit # back on quest, run the following line to transfer every file with path listed in \"files_to_upload.tsv\" to that folder # make sure to change your upload folder and files to upload module load parallel parallel --verbose lftp -e \\\"put -O /uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4/20210121_submission {}\\; bye\\\" -u subftp,w4pYB9VQ ftp-private.ncbi.nlm.nih.gov < files_to_upload.tsv Note: it is important that this step is completely finished before you complete your SRA submission Complete submission When finished, in the SRA portal you will be asked to select which folder you want to pull files from Review and submit! If there are any issues they will let you know. List of current bioprojects associated with the Andersen Lab \u00b6 C. elegans WI genome FASTQ - PRJNA549503 (link here )","title":"SRA"},{"location":"other/sra/#uploading_wi_fastq_sequence_data_to_sra","text":"For each CENDR release, it is important to also upload the FASTQ files to NCBI's Sequence Read Archive (SRA). If a bioproject already exists, you can create a new submission and link to the previous bioproject . If there is no previous bioproject, you can create a new bioproject and add all relevant data. See below for more instructions.","title":"Uploading WI FASTQ sequence data to SRA"},{"location":"other/sra/#sra_submission","text":"Begin submission In the SRA submission portal , click the button for \"New submission\" and follow the prompts. Remember to add the previous bioproject ID if applicable to link this submission to previous submissions. Select \"Model organism or animal\" for biosample type, select \"upload file using excel\" and download the template Create biosample sheet (an example can be found below) An easy starting point here is the sample sheet used for alignment-nf . You will keep the id as the sample name (unique identifier) and strain (note strain also needs to be unique - if there are multiple library preps for the same strain, please append \"-2\" etc. to the strain) To these two columns, you can add bioproject, organism, developmental stage, sex, and tissue (same across all strains) Finally, join this data with the WI species master sheet to get collected by, collection date, and latitude/longitude. - Note: latitude/longitude need to be converted into one shared column in the format \"34.89 S 56.15 W\" (+ refers to North and East) Copy the data into the relevent columns in the template , save, and upload to the submission portal Create SRA metadata sheet Again, select \"upload a file using excel\" and download the template An easy starting point here is, again, the sample sheet used for alignment-nf . You will keep the id as the sample name and the lb as library id. You will also keep fq1 and fq2 for filename and filename2. You will then add the rest of the columns as shown below. Note: the formatting is very specific for this sheet. The title can be found on the bioproject page. The instrument_model can be found by using the \"sequencing_folder\" (not shown, but part of the original sample sheet) and looking up the instrument that folder was run on in the Sequencing Runs google sheet ( here ) Copy and paste the rows from this file into the template to check for correct formatting. Then save the tab as a tsv and upload to the submission portal. Pre-upload FASTQ files using FTP Create a list of files to upload to the FTP server by combining the filename1 and filename2 from the SRA metadata sheet (above). Begin submission by creating an NCBI account (or signing in -- personal account). Then follow the link to the SRA submission portal Follow the instructions under the \"FTP upload\": # establish FTP connection from terminal (on QUEST!) # ftp <address> ftp ftp-private.ncbi.nlm.nih.gov # navigate to your account folder (i.e.) cd uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4 # create new folder for submission (i.e.) mkdir 20210121_submission # exit FTP connection exit # back on quest, run the following line to transfer every file with path listed in \"files_to_upload.tsv\" to that folder # make sure to change your upload folder and files to upload module load parallel parallel --verbose lftp -e \\\"put -O /uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4/20210121_submission {}\\; bye\\\" -u subftp,w4pYB9VQ ftp-private.ncbi.nlm.nih.gov < files_to_upload.tsv Note: it is important that this step is completely finished before you complete your SRA submission Complete submission When finished, in the SRA portal you will be asked to select which folder you want to pull files from Review and submit! If there are any issues they will let you know.","title":"SRA submission"},{"location":"other/sra/#list_of_current_bioprojects_associated_with_the_andersen_lab","text":"C. elegans WI genome FASTQ - PRJNA549503 (link here )","title":"List of current bioprojects associated with the Andersen Lab"},{"location":"other/writing-nextflow/","text":"Writing Nextflow pipelines \u00b6 Check out the Nextflow documentation for help getting started! Note Learning to script with Nextflow definitely has a high learning curve. Don't get discouraged! Start with something small and simple. Maybe convert a current script you have that uses a large for loop into a nextflow pipeline to start getting the hang of things! When should my pipeline be a Nextflow script? \u00b6 Not every analysis needs to be a Nextflow pipeline. For smaller analyses (especially those with few inputs) it might be easier to write a bash/shell script. However, there are many advantages to Nextflow: When you are running many parallel tasks Nextflow takes care of all the job submissions and is really good at running the same basic script across 6 chromosomes, 500 strains, 1000 permutations, whatever you need! When your analysis consists of several different steps that are either sequential and/or can be run simultaneously. Because Nextflow is great at parallelization, it knows which steps rely on other steps and can speed up your script by running independent steps in parallel. Also, you can take advantage of the -resume function for scripts that take a long time to run because Nextflow caches results (which means if there is an error you can fix it but you don't have to start over from the begining!) You want to be able to easily run your script on different computing platforms (i.e. QUEST, local machine, GCP...) This can be done by creating different profiles for each platform. Check out the nextflow documentation on profiles. The basics \u00b6 Channels All input and output files in Nextflow are piped through \"channels\". You can create a channel with an input file or parameter and then feed these channels to a \"process\" (or step). Channels can be merged, split, or rearranged. Check out the Nextflow documentation for channels to learn more. Also check out this cheatsheet for useful operators. Miscellaneous channels tips: Channel.from(\"A.txt\") will put A.txt as is into the channel Channel.fromPath(\"A.txt\") will add a full path (usually current directory) and put /path/A.txt into the channel. Channel.fromPath(\"folder/A.txt\") will add a full path (usually current directory) and put /path/folder/A.txt into the channel. Channel.fromPath(\"/path/A.txt\") will put /path/A.txt into the channel. In other words, Channel.fromPath will only add a full path if there isn't already one and ensure there is always a full path in the resulting channel. This goes hand in hand with input: path(\"A.txt\") inside the process, where Nextflow actually creates a symlink named A.txt (note the path from first / to last / is stripped) linking to /path/A.txt in the working directory , so it can be accessed within the working directory by the script cat A.txt without specifying a path. Processes Each chunk of code in a nextflow script is broken up into distinct \"processes\" that you can think of as \"steps\" or even small/large \"functions\". A process can have one line of code or hundreds. It can be done in bash or by running an R or python script. An example process is shown below: # first process in linkagemapping-nf process split_pheno { input: file('infile') output: file('*.tsv') \"\"\" Rscript --vanilla ${workflow.projectDir}/bin/split_pheno.R ${infile} ${params.thresh} \"\"\" } Each process is defined by process <name> {} and has three basic parts: 1) input, 2) output, 3) script. You can dictate the pipeline by generating a workflow that acts like a protocol or recipe for Nextflow: which inputs go to which process and what order to do things in. Note, this is different than in DSL1, which is not actively used anymore . To learn more about processes, check out the nextflow docs . # example of a simple workflow (at the top of a nextflow script) workflow { # create a channel from the input file and give it to the split_pheno process Channel.fromPath(params.in) | split_pheno # take the output from split_pheno and \"flatten it\" and send to the mapping process split_pheno.out.flatten() | mapping } Miscellaneous notes on input paths for process: With input: path(\"A.txt\") one can refer to the file in the script as A.txt . Side note A.txt doesn't have to be the same name as in channel creation, it can be anything, input: path(\"B.txt\") , input: path(\"n\") etc. With input: path(A) one can refer to the file in the script as $A , and the value of $A will be the original file name (without path, see section above). input: path(\"A.txt\") and input: path \"A.txt\" generally both work. Occasionally had errors that required the following (tip from @danielecook ): If not in a tuple, use input: path \"A.txt\" If in a tuple, use input: tuple path(\"A.txt\"), path(\"B.txt\") This goes the same for output . From @pditommaso : path(A) is almost the same as file(A) , however the first interprets a value of type string as the input file path (ie the location in the file system where it's stored), the latter interprets a value of type string and materialise it to a temporary files. It's recommended the use of path since it's less ambiguous and fits better in most use-cases. The working directory One of the main distinctions of Nextflow is that each execution of a process happens in its own temporary working directory. This is important for several reasons: You do not need to name temporary files dynamically (i.e. with strain or trait name) to avoid overwriting files, because you can repeat the same process with a different trait in a different directory. This means you can call a temporary file strain.bam instead of DL238.bam and CB4856.bam . This can make for simpler coding However, if you provide strain name as a value in the input channel, it is easy to name files dynamically with ${strain}.bam which will output DL238.bam or CB4856.bam If there is an error, you can go into the working directory to see all input and output files (sometimes in the form of symlinks) for that specific process. You can also find the script ( .command.sh ) that was run and try to reproduce the error manually. If there was an error, the message is recorded in errlog.txt If there is an error, the Nextflow error output will point you to the working directory for that specific process and might look something like /projects/b1042/AndersenLab/work/katie/4c/4d9c3b333734a5b63d66f0bc0cfcdc You can also find the working directory from the hash shown next to a running/completed process. For example [4c/4d9c3b] corresponds to the working directory above. See the running nextflow page for creating a function to automatically cd into the working directory given that hash. You can also find the working directory in the .nextflow.log file or in the report.html if one is generated. Miscellaneous tips on the working directory: Note that with publishDir \"path\", mode: 'move' , the output file will be moved outside of the working directory and Nextflow will not be able to use it as input for another process, so only use it when there is not a following process that uses the output file. Be mindful that if the \"\"\" (script section) \"\"\" involves changing directory, such as cd or rmarkdown::render( knit_root_dir = \"folder/\" ) , Nextflow will still only search the working directory for output files. Run nextflow clean -f in the excecution folder to clean up the working directories. In Nextflow scripts (.nf files), one can use: ${workflow.projectDir} to refer where the project locates (usually the folder of main.nf). For example: publishDir \"${workflow.projectDir}/output\", mode: 'copy' or Rscript ${workflow.projectDir}/bin/task.R . ${workflow.launchDir} to refer to where the script is called from. $baseDir usually refers to the same folder as ${workflow.projectDir} but it can also be used in the config file, where ${workflow.projectDir} and ${workflow.launchDir} are not accessible. They are much more reliable than $PWD or $pwd . Note The standard name of a nextflow script is main.nf but it doesn't have to be! If you just call nextflow run andersenlab/nemascan it will automatically choose the main.nf script. It is best practice to always write out the script name though Debugging with print - To print a channel, use .view() . It's especially useful to resolve WARN: Input tuple does not match input set cardinality declared by process . (Don't forget to remove .view() after debugging) channel_vcf .combine(channel_index) .combine(channel_chr) .view() To print from the script section inside the processes, add echo true . process test { echo true // this will print the stdout from the script section on Terminal input: path(vcf) \"\"\" head $vcf \"\"\" } Notes on transition to DSL2 If you are new to nextflow or don't know anything about DSL1 or DSL2, you can disregard this section and use DSL2 syntax! - Moving to DSL2 is a one-way street. It's so intuitive with clean and readable code. - In DSL1, each queue channel can only be used once. - In DSL2, a channel can be fed into multiple processes - In DSL2, each process can only be called once. The solution is either .concat() the input channels so they run as parallel processes, or put the process in a module and import multiple times from the module. (One may be able to call a process in different workflows, haven't tested yet). - DSL2 also enforces that all inputs needs to be combined into 1 channel before it goes into a process. See the cheatsheet for useful operators. - Simple steps to convert from original syntax to DSL2 - Deprecated operators . Run reports nextflow main.nf -with-report -with-timeline -with-dag -with-report Nextflow html report contains resource usage for each process, and details (most useful being the status and working directory) for each process -with-timeline How much wait time and run time each process took for the run. Very useful reference for optimizing resource allocation and improving run time. -with-dag Make a flowchart to show the relationship of channels and processes. Software dependencies to use these features. Note the differences on Mac and Linux. Or, set this up in the nextflow.config file for a pipeline to ensure they are generated each time the script is run: import java.time.* Date now = new Date() params { tracedir = \"pipeline_info\" timestamp = now.format(\"yyyyMMdd-HH-mm-ss\") } timeline { enabled = true file = \"${params.tracedir}/${params.timestamp}_timeline.html\" } report { enabled = true file = \"${params.tracedir}/${params.timestamp}_report.html\" } How to require users to sepcify a parameter value There are 2 types of paramters: (a) one with no actual value (b) one with actual values. (a) If a parameter is specified but no value is given, it is implicitly considered true . So one can use this to run debug mode nextflow main.nf --debug if (params.debug) { ... (set parameters for debug mode) } else { ... (set parameters for normal use) } or to print help message nextflow main.nf --help if (params.help) { println \"\"\" ... (help msg here) \"\"\" exit 0 } (b) For parameters that need to contain a value, Nextflow recommends to set a default and let users to overwrite it as needed. However, if you want to require it to be specified by the user: params.reference = null // no quotes. this line is optional, since without initialising the parameter it will default to null. if (params.reference == null) error \"Please specify a reference genome with --reference\" Below works as long as the user always append a value: --reference=something . It will not print the error message with: nextflow main.nf --reference (without specifying a value) because this will set params.reference to true (see point (a) ) and !params.reference will be false . if (!params.reference) error \"Please specify a reference genome with --reference\" Resources \u00b6 Nextflow documentation Nextflow cheatsheet Nextflow gitter Awesome Nextflow pipeline examples - Repository of great nextflow pipelines. Official Nextflow patterns Google group","title":"Writing Nextflow workflows"},{"location":"other/writing-nextflow/#writing_nextflow_pipelines","text":"Check out the Nextflow documentation for help getting started! Note Learning to script with Nextflow definitely has a high learning curve. Don't get discouraged! Start with something small and simple. Maybe convert a current script you have that uses a large for loop into a nextflow pipeline to start getting the hang of things!","title":"Writing Nextflow pipelines"},{"location":"other/writing-nextflow/#when_should_my_pipeline_be_a_nextflow_script","text":"Not every analysis needs to be a Nextflow pipeline. For smaller analyses (especially those with few inputs) it might be easier to write a bash/shell script. However, there are many advantages to Nextflow: When you are running many parallel tasks Nextflow takes care of all the job submissions and is really good at running the same basic script across 6 chromosomes, 500 strains, 1000 permutations, whatever you need! When your analysis consists of several different steps that are either sequential and/or can be run simultaneously. Because Nextflow is great at parallelization, it knows which steps rely on other steps and can speed up your script by running independent steps in parallel. Also, you can take advantage of the -resume function for scripts that take a long time to run because Nextflow caches results (which means if there is an error you can fix it but you don't have to start over from the begining!) You want to be able to easily run your script on different computing platforms (i.e. QUEST, local machine, GCP...) This can be done by creating different profiles for each platform. Check out the nextflow documentation on profiles.","title":"When should my pipeline be a Nextflow script?"},{"location":"other/writing-nextflow/#the_basics","text":"Channels All input and output files in Nextflow are piped through \"channels\". You can create a channel with an input file or parameter and then feed these channels to a \"process\" (or step). Channels can be merged, split, or rearranged. Check out the Nextflow documentation for channels to learn more. Also check out this cheatsheet for useful operators. Miscellaneous channels tips: Channel.from(\"A.txt\") will put A.txt as is into the channel Channel.fromPath(\"A.txt\") will add a full path (usually current directory) and put /path/A.txt into the channel. Channel.fromPath(\"folder/A.txt\") will add a full path (usually current directory) and put /path/folder/A.txt into the channel. Channel.fromPath(\"/path/A.txt\") will put /path/A.txt into the channel. In other words, Channel.fromPath will only add a full path if there isn't already one and ensure there is always a full path in the resulting channel. This goes hand in hand with input: path(\"A.txt\") inside the process, where Nextflow actually creates a symlink named A.txt (note the path from first / to last / is stripped) linking to /path/A.txt in the working directory , so it can be accessed within the working directory by the script cat A.txt without specifying a path. Processes Each chunk of code in a nextflow script is broken up into distinct \"processes\" that you can think of as \"steps\" or even small/large \"functions\". A process can have one line of code or hundreds. It can be done in bash or by running an R or python script. An example process is shown below: # first process in linkagemapping-nf process split_pheno { input: file('infile') output: file('*.tsv') \"\"\" Rscript --vanilla ${workflow.projectDir}/bin/split_pheno.R ${infile} ${params.thresh} \"\"\" } Each process is defined by process <name> {} and has three basic parts: 1) input, 2) output, 3) script. You can dictate the pipeline by generating a workflow that acts like a protocol or recipe for Nextflow: which inputs go to which process and what order to do things in. Note, this is different than in DSL1, which is not actively used anymore . To learn more about processes, check out the nextflow docs . # example of a simple workflow (at the top of a nextflow script) workflow { # create a channel from the input file and give it to the split_pheno process Channel.fromPath(params.in) | split_pheno # take the output from split_pheno and \"flatten it\" and send to the mapping process split_pheno.out.flatten() | mapping } Miscellaneous notes on input paths for process: With input: path(\"A.txt\") one can refer to the file in the script as A.txt . Side note A.txt doesn't have to be the same name as in channel creation, it can be anything, input: path(\"B.txt\") , input: path(\"n\") etc. With input: path(A) one can refer to the file in the script as $A , and the value of $A will be the original file name (without path, see section above). input: path(\"A.txt\") and input: path \"A.txt\" generally both work. Occasionally had errors that required the following (tip from @danielecook ): If not in a tuple, use input: path \"A.txt\" If in a tuple, use input: tuple path(\"A.txt\"), path(\"B.txt\") This goes the same for output . From @pditommaso : path(A) is almost the same as file(A) , however the first interprets a value of type string as the input file path (ie the location in the file system where it's stored), the latter interprets a value of type string and materialise it to a temporary files. It's recommended the use of path since it's less ambiguous and fits better in most use-cases. The working directory One of the main distinctions of Nextflow is that each execution of a process happens in its own temporary working directory. This is important for several reasons: You do not need to name temporary files dynamically (i.e. with strain or trait name) to avoid overwriting files, because you can repeat the same process with a different trait in a different directory. This means you can call a temporary file strain.bam instead of DL238.bam and CB4856.bam . This can make for simpler coding However, if you provide strain name as a value in the input channel, it is easy to name files dynamically with ${strain}.bam which will output DL238.bam or CB4856.bam If there is an error, you can go into the working directory to see all input and output files (sometimes in the form of symlinks) for that specific process. You can also find the script ( .command.sh ) that was run and try to reproduce the error manually. If there was an error, the message is recorded in errlog.txt If there is an error, the Nextflow error output will point you to the working directory for that specific process and might look something like /projects/b1042/AndersenLab/work/katie/4c/4d9c3b333734a5b63d66f0bc0cfcdc You can also find the working directory from the hash shown next to a running/completed process. For example [4c/4d9c3b] corresponds to the working directory above. See the running nextflow page for creating a function to automatically cd into the working directory given that hash. You can also find the working directory in the .nextflow.log file or in the report.html if one is generated. Miscellaneous tips on the working directory: Note that with publishDir \"path\", mode: 'move' , the output file will be moved outside of the working directory and Nextflow will not be able to use it as input for another process, so only use it when there is not a following process that uses the output file. Be mindful that if the \"\"\" (script section) \"\"\" involves changing directory, such as cd or rmarkdown::render( knit_root_dir = \"folder/\" ) , Nextflow will still only search the working directory for output files. Run nextflow clean -f in the excecution folder to clean up the working directories. In Nextflow scripts (.nf files), one can use: ${workflow.projectDir} to refer where the project locates (usually the folder of main.nf). For example: publishDir \"${workflow.projectDir}/output\", mode: 'copy' or Rscript ${workflow.projectDir}/bin/task.R . ${workflow.launchDir} to refer to where the script is called from. $baseDir usually refers to the same folder as ${workflow.projectDir} but it can also be used in the config file, where ${workflow.projectDir} and ${workflow.launchDir} are not accessible. They are much more reliable than $PWD or $pwd . Note The standard name of a nextflow script is main.nf but it doesn't have to be! If you just call nextflow run andersenlab/nemascan it will automatically choose the main.nf script. It is best practice to always write out the script name though Debugging with print - To print a channel, use .view() . It's especially useful to resolve WARN: Input tuple does not match input set cardinality declared by process . (Don't forget to remove .view() after debugging) channel_vcf .combine(channel_index) .combine(channel_chr) .view() To print from the script section inside the processes, add echo true . process test { echo true // this will print the stdout from the script section on Terminal input: path(vcf) \"\"\" head $vcf \"\"\" } Notes on transition to DSL2 If you are new to nextflow or don't know anything about DSL1 or DSL2, you can disregard this section and use DSL2 syntax! - Moving to DSL2 is a one-way street. It's so intuitive with clean and readable code. - In DSL1, each queue channel can only be used once. - In DSL2, a channel can be fed into multiple processes - In DSL2, each process can only be called once. The solution is either .concat() the input channels so they run as parallel processes, or put the process in a module and import multiple times from the module. (One may be able to call a process in different workflows, haven't tested yet). - DSL2 also enforces that all inputs needs to be combined into 1 channel before it goes into a process. See the cheatsheet for useful operators. - Simple steps to convert from original syntax to DSL2 - Deprecated operators . Run reports nextflow main.nf -with-report -with-timeline -with-dag -with-report Nextflow html report contains resource usage for each process, and details (most useful being the status and working directory) for each process -with-timeline How much wait time and run time each process took for the run. Very useful reference for optimizing resource allocation and improving run time. -with-dag Make a flowchart to show the relationship of channels and processes. Software dependencies to use these features. Note the differences on Mac and Linux. Or, set this up in the nextflow.config file for a pipeline to ensure they are generated each time the script is run: import java.time.* Date now = new Date() params { tracedir = \"pipeline_info\" timestamp = now.format(\"yyyyMMdd-HH-mm-ss\") } timeline { enabled = true file = \"${params.tracedir}/${params.timestamp}_timeline.html\" } report { enabled = true file = \"${params.tracedir}/${params.timestamp}_report.html\" } How to require users to sepcify a parameter value There are 2 types of paramters: (a) one with no actual value (b) one with actual values. (a) If a parameter is specified but no value is given, it is implicitly considered true . So one can use this to run debug mode nextflow main.nf --debug if (params.debug) { ... (set parameters for debug mode) } else { ... (set parameters for normal use) } or to print help message nextflow main.nf --help if (params.help) { println \"\"\" ... (help msg here) \"\"\" exit 0 } (b) For parameters that need to contain a value, Nextflow recommends to set a default and let users to overwrite it as needed. However, if you want to require it to be specified by the user: params.reference = null // no quotes. this line is optional, since without initialising the parameter it will default to null. if (params.reference == null) error \"Please specify a reference genome with --reference\" Below works as long as the user always append a value: --reference=something . It will not print the error message with: nextflow main.nf --reference (without specifying a value) because this will set params.reference to true (see point (a) ) and !params.reference will be false . if (!params.reference) error \"Please specify a reference genome with --reference\"","title":"The basics"},{"location":"other/writing-nextflow/#resources","text":"Nextflow documentation Nextflow cheatsheet Nextflow gitter Awesome Nextflow pipeline examples - Repository of great nextflow pipelines. Official Nextflow patterns Google group","title":"Resources"},{"location":"pipelines/adding-seq-data/","text":"Adding NIL sequence data to lab site \u00b6 Adding NIL sequence data to lab site Basic Commands To Know Prior Your Sequencing Data Step By Step Instructions Viewing your results This section is for adding genomic sequencing data (.tsv files) onto the existing dataset provided and displayed on the NILs browser page on Andersenlab.org . Basic Commands To Know Prior \u00b6 You should freshen up on the following terminal commands. cd - change directories rm - delete files cp - make a copy git Your Sequencing Data \u00b6 You will use the gt_hmm_fill.tsv file output from the nil-ril-nf pipeline for this step. Important In order for your sequencing data to be properly added, it is important to make sure that there are no empty/additional lines located at the bottom of your .tsv file. What you do not want What you do want Once your file has no empty lines at the bottom, save the file and move onto the next instructions below. Step By Step Instructions \u00b6 Clone the andersenlab.org repo git clone https://github.com/AndersenLab/andersenlab.github.io.git [Optional] Create a new branch. This is a good idea if you are newer to github and want to make sure you don't break the lab website by doing something weird. Add your .tsv file into the pages folder of your Andersenlab github directory. Open terminal and use the cd command to change directories into the pages folder in your Andersenlab github directory. If you did everything correctly, when you type ls into your terminal, it should look something like this. Then run the following commands in your terminal (while still in your pages directory): # create a new file called 'copy.tsv' with your data cp yourFileName.tsv copy.tsv # run python script which will add your data to full dataset python addDataTogt_hmm.tsv.py After running the above commands, your sequencing data has now been added to the existing NILs dataset on Andersenlab.org. You can now remove your .tsv from the pages directory by using the rm command in your terminal. rm yourFileName.tsv Finally, commit your changes and push your code to update the Andersenlab github. [Optional] If you created a new branch, you need to merge this new branch back into the master branch for changes to take effect. Viewing your results \u00b6 Once all changes are pushed, you should be able to view your NIL genotypes in the NIL browser shiny app .","title":"Adding NIL sequence data to lab site"},{"location":"pipelines/adding-seq-data/#adding_nil_sequence_data_to_lab_site","text":"Adding NIL sequence data to lab site Basic Commands To Know Prior Your Sequencing Data Step By Step Instructions Viewing your results This section is for adding genomic sequencing data (.tsv files) onto the existing dataset provided and displayed on the NILs browser page on Andersenlab.org .","title":"Adding NIL sequence data to lab site"},{"location":"pipelines/adding-seq-data/#basic_commands_to_know_prior","text":"You should freshen up on the following terminal commands. cd - change directories rm - delete files cp - make a copy git","title":"Basic Commands To Know Prior"},{"location":"pipelines/adding-seq-data/#your_sequencing_data","text":"You will use the gt_hmm_fill.tsv file output from the nil-ril-nf pipeline for this step. Important In order for your sequencing data to be properly added, it is important to make sure that there are no empty/additional lines located at the bottom of your .tsv file. What you do not want What you do want Once your file has no empty lines at the bottom, save the file and move onto the next instructions below.","title":"Your Sequencing Data"},{"location":"pipelines/adding-seq-data/#step_by_step_instructions","text":"Clone the andersenlab.org repo git clone https://github.com/AndersenLab/andersenlab.github.io.git [Optional] Create a new branch. This is a good idea if you are newer to github and want to make sure you don't break the lab website by doing something weird. Add your .tsv file into the pages folder of your Andersenlab github directory. Open terminal and use the cd command to change directories into the pages folder in your Andersenlab github directory. If you did everything correctly, when you type ls into your terminal, it should look something like this. Then run the following commands in your terminal (while still in your pages directory): # create a new file called 'copy.tsv' with your data cp yourFileName.tsv copy.tsv # run python script which will add your data to full dataset python addDataTogt_hmm.tsv.py After running the above commands, your sequencing data has now been added to the existing NILs dataset on Andersenlab.org. You can now remove your .tsv from the pages directory by using the rm command in your terminal. rm yourFileName.tsv Finally, commit your changes and push your code to update the Andersenlab github. [Optional] If you created a new branch, you need to merge this new branch back into the master branch for changes to take effect.","title":"Step By Step Instructions"},{"location":"pipelines/adding-seq-data/#viewing_your_results","text":"Once all changes are pushed, you should be able to view your NIL genotypes in the NIL browser shiny app .","title":"Viewing your results"},{"location":"pipelines/docker/","text":"Create docker image \u00b6 Create docker image Dockerfile Build docker image Tag image with a version Push docker image to dockerhub Running Nextflow with docker Caching singularity images on Rockfish Caching singularity images on QUEST Docker can help us to maintain our computational environments. Each of our Nextflow pipeline has a dedicated docker image in our lab. And all the docker files should be avalible at dockerfile or in specific pipelines. Dockerfile \u00b6 To simplify the image building, we can use conda to install most of the tools. We can collect the tools available on conda cloud into a conda.yml file, which might looks like this. name: concordance-nf channels: - defaults - bioconda - r - biobuilds - conda-forge dependencies: - bwa=0.7.17 - sambamba=0.7.0 - samtools=1.9 - picard=2.20.6 - bcftools=1.9 - csvtk=0.18.2 - r=3.6.0 - r-ggplot2=3.1.1 - r-readr=1.3.1 - r-tidyverse=1.2.1 Then, build the Dockerfile as below. FROM continuumio/miniconda MAINTAINER Katie Evans <kathrynevans2015@u.northwestern.edu> COPY conda.yml . RUN \\ conda env update -n root -f conda.yml \\ && conda clean -a # install other tools not avalible on conda cloud -- tidyverse sometimes need to be installed here separately... RUN apt-get update && apt-get install -y procps RUN R -e \"install.packages('roperators',dependencies=TRUE, repos='http://cran.us.r-project.org')\" Note Put the conda.ymal and Dockerfile under the same folder. Build docker image \u00b6 To build the docker image, you need docker desktop installed on your local machine. Also you should sign up the dockerhub to enable pushing docker image to cloud. Go to the folder which have conda.ymal and Dockerfile , run docker build -t <dockerhub account>/<name of the image> . # don't ingore the dot here Important If you are on a newer Mac (M1 processor or newer), you will need to specify the correct platform to build the container for with the argument --platform linux/amd64 . You can use docker image ls to check the image list you have in your local machine. Importantly, you have to check if the tools were installed sucessfully in your docker image. To test the docker image, run docker run -ti <dockerhub account>/<name of the image> sh The above command will let you into the docker image, where you can check the tools by their normal commands. Make sure all the tools you need have been installed appropriately. Tag image with a version \u00b6 There are sometimes issues with Nextflow thinking it has the latest docker image when it really doesn't. To avoid this issue, it is useful to tag each updated docker image with a version tag. Remember to update the docker call in nextflow to use the new version! docker image tag <dockerhub account>/<name of the image>:latest <dockerhub account>/<name of the image>:<version tag> Push docker image to dockerhub \u00b6 After the image check, you are ready to push the image to dockerhub which allows you to download the image when ever you need to use. docker push <dockerhub account>/<name of the image>:<version tag> Running Nextflow with docker \u00b6 If running Nextflow locally, a docker container can be used with the following line (check out the documentation ): nextflow run <your script> -with-docker [docker image] Alternatively, a docker container can be specified within the nextflow.config script to avoid adding an extra parameter: process.container = 'nextflow/examples:latest' docker.enabled = true // if on quest or rockfish: // singularity.enabled = true Important When running Nextflow with a docker container on QUEST or Rockfish, it is necessary to replace the docker command with singularity (although you still must build a docker container). You must also load singularity using module load singularity before starting a run. Caching singularity images on Rockfish \u00b6 To make the most out of using a shared cache directory for singularity on VAST, make sure to add this line to your ~/.bash_profile before you run a pipeline for the first time (Note: this is not needed to USE a previously cached image, but only when you ADD a new one). export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Caching singularity images on QUEST \u00b6 To make the most out of using a shared cache directory for singularity on b1059, make sure to add this line to your ~/.bash_profile before you run a pipeline for the first time (Note: this is not needed to USE a previously cached image, but only when you ADD a new one). export SINGULARITY_CACHEDIR='/projects/b1059/singularity/'","title":"Docker"},{"location":"pipelines/docker/#create_docker_image","text":"Create docker image Dockerfile Build docker image Tag image with a version Push docker image to dockerhub Running Nextflow with docker Caching singularity images on Rockfish Caching singularity images on QUEST Docker can help us to maintain our computational environments. Each of our Nextflow pipeline has a dedicated docker image in our lab. And all the docker files should be avalible at dockerfile or in specific pipelines.","title":"Create docker image"},{"location":"pipelines/docker/#dockerfile","text":"To simplify the image building, we can use conda to install most of the tools. We can collect the tools available on conda cloud into a conda.yml file, which might looks like this. name: concordance-nf channels: - defaults - bioconda - r - biobuilds - conda-forge dependencies: - bwa=0.7.17 - sambamba=0.7.0 - samtools=1.9 - picard=2.20.6 - bcftools=1.9 - csvtk=0.18.2 - r=3.6.0 - r-ggplot2=3.1.1 - r-readr=1.3.1 - r-tidyverse=1.2.1 Then, build the Dockerfile as below. FROM continuumio/miniconda MAINTAINER Katie Evans <kathrynevans2015@u.northwestern.edu> COPY conda.yml . RUN \\ conda env update -n root -f conda.yml \\ && conda clean -a # install other tools not avalible on conda cloud -- tidyverse sometimes need to be installed here separately... RUN apt-get update && apt-get install -y procps RUN R -e \"install.packages('roperators',dependencies=TRUE, repos='http://cran.us.r-project.org')\" Note Put the conda.ymal and Dockerfile under the same folder.","title":"Dockerfile"},{"location":"pipelines/docker/#build_docker_image","text":"To build the docker image, you need docker desktop installed on your local machine. Also you should sign up the dockerhub to enable pushing docker image to cloud. Go to the folder which have conda.ymal and Dockerfile , run docker build -t <dockerhub account>/<name of the image> . # don't ingore the dot here Important If you are on a newer Mac (M1 processor or newer), you will need to specify the correct platform to build the container for with the argument --platform linux/amd64 . You can use docker image ls to check the image list you have in your local machine. Importantly, you have to check if the tools were installed sucessfully in your docker image. To test the docker image, run docker run -ti <dockerhub account>/<name of the image> sh The above command will let you into the docker image, where you can check the tools by their normal commands. Make sure all the tools you need have been installed appropriately.","title":"Build docker image"},{"location":"pipelines/docker/#tag_image_with_a_version","text":"There are sometimes issues with Nextflow thinking it has the latest docker image when it really doesn't. To avoid this issue, it is useful to tag each updated docker image with a version tag. Remember to update the docker call in nextflow to use the new version! docker image tag <dockerhub account>/<name of the image>:latest <dockerhub account>/<name of the image>:<version tag>","title":"Tag image with a version"},{"location":"pipelines/docker/#push_docker_image_to_dockerhub","text":"After the image check, you are ready to push the image to dockerhub which allows you to download the image when ever you need to use. docker push <dockerhub account>/<name of the image>:<version tag>","title":"Push docker image to dockerhub"},{"location":"pipelines/docker/#running_nextflow_with_docker","text":"If running Nextflow locally, a docker container can be used with the following line (check out the documentation ): nextflow run <your script> -with-docker [docker image] Alternatively, a docker container can be specified within the nextflow.config script to avoid adding an extra parameter: process.container = 'nextflow/examples:latest' docker.enabled = true // if on quest or rockfish: // singularity.enabled = true Important When running Nextflow with a docker container on QUEST or Rockfish, it is necessary to replace the docker command with singularity (although you still must build a docker container). You must also load singularity using module load singularity before starting a run.","title":"Running Nextflow with docker"},{"location":"pipelines/docker/#caching_singularity_images_on_rockfish","text":"To make the most out of using a shared cache directory for singularity on VAST, make sure to add this line to your ~/.bash_profile before you run a pipeline for the first time (Note: this is not needed to USE a previously cached image, but only when you ADD a new one). export SINGULARITY_CACHEDIR='/vast/eande106/singularity/'","title":"Caching singularity images on Rockfish"},{"location":"pipelines/docker/#caching_singularity_images_on_quest","text":"To make the most out of using a shared cache directory for singularity on b1059, make sure to add this line to your ~/.bash_profile before you run a pipeline for the first time (Note: this is not needed to USE a previously cached image, but only when you ADD a new one). export SINGULARITY_CACHEDIR='/projects/b1059/singularity/'","title":"Caching singularity images on QUEST"},{"location":"pipelines/nf-GCPconfig/","text":"Running Nextflow pipeline on GCP \u00b6 Running Nextflow pipeline on GCP Enable API Create service account Generate credential for the service account Nextflow version and mode Configure Nextflow for GCP Google genomic API allows auto scale for computational resources by creating and closing VMs automatically. We have a dedicated google project caendr which using Google genomic APT for all the nextflow pipelines in our lab. To access it, you should provide your gmail accout to Erik and ask Erik give you a project owner role for caendr . I already preset the project to enable running Nextflow pipelines using Google genomic API . See below for more details. Enable API \u00b6 Go the the main page of google cloud platform . In the Product & Services menu, click APIs & Services , and then click Enable APIs and Services . The following APIs should be enabled to run nextflow pipeline on GCP. Genomics API Cloud Life Sciences API Compute Engine API Google Container Registry API Create service account \u00b6 Go to the IAM & admin , find the Service accounts . Click CREATE SERVICE ACCOUNT to create a new service accounts. Note this service accounts must have a project owner role to run Nextflow pipelines. The service account I created here is called nextflowRUN . You don't need to do the above processes when you use GCP. But you have to do all the following processes to make sure you have the right permissions to caendr . Generate credential for the service account \u00b6 After you get the access to caendr , go to the API & Services . Click the Create credentials button, select Service account key . And choose nextflowRUN to generate a JSON file, which is a privite key file for using nextflowRUN . Download the file and save it in a safe place. Finally, define the GOOGLE_APPLICATION_CREDENTIALS variable in .bash_profile with the directory of the JSON file. Which should looks like the example. export GOOGLE_APPLICATION_CREDENTIALS=$HOME/google_creds/caendr-2cae6210c8d1.json Nextflow version and mode \u00b6 Only the version of 19.07.0 or higher of Nextflow are compatible with GCP. And also, the Nextflow should have a google mode. You can define the version and mode in .bash_profile . export NXF_VER=19.07.0 export NXF_MODE=google Then, run the following code to update or install Nextflow. curl https://get.nextflow.io | bash Configure Nextflow for GCP \u00b6 To run Nextflow pipelines on GCP, you need to build docker images for them. Check the docker file repo of our lab for more information. The google genomic API has its own executor called google-pipelines , you need to define the executor variable with google-pipelines in the nextflow.config file. Here is the example for concordance-nf . docker { enabled = true } process { executor = 'google-pipelines' withLabel: bam_coverage { container = 'faithman/bam_toolbox:latest' } container = 'faithman/concordance:latest' machineType = 'n1-standard-4' } google { project = 'caendr' zone = 'us-central1-a' } cloud { preemptible = true } executor { queueSize = 500 } Important The file system of google buckets is not like S3 that can read/write directly by most softwares. You have to use gsutil tool to interact with google buckets to read/write files in most situations. Nextflow has built-in functions to interact with google buckets, but you still can not read/write files directly in your script. All the files have to be read and write via channels in Nextflow!","title":"GCP"},{"location":"pipelines/nf-GCPconfig/#running_nextflow_pipeline_on_gcp","text":"Running Nextflow pipeline on GCP Enable API Create service account Generate credential for the service account Nextflow version and mode Configure Nextflow for GCP Google genomic API allows auto scale for computational resources by creating and closing VMs automatically. We have a dedicated google project caendr which using Google genomic APT for all the nextflow pipelines in our lab. To access it, you should provide your gmail accout to Erik and ask Erik give you a project owner role for caendr . I already preset the project to enable running Nextflow pipelines using Google genomic API . See below for more details.","title":"Running Nextflow pipeline on GCP"},{"location":"pipelines/nf-GCPconfig/#enable_api","text":"Go the the main page of google cloud platform . In the Product & Services menu, click APIs & Services , and then click Enable APIs and Services . The following APIs should be enabled to run nextflow pipeline on GCP. Genomics API Cloud Life Sciences API Compute Engine API Google Container Registry API","title":"Enable API"},{"location":"pipelines/nf-GCPconfig/#create_service_account","text":"Go to the IAM & admin , find the Service accounts . Click CREATE SERVICE ACCOUNT to create a new service accounts. Note this service accounts must have a project owner role to run Nextflow pipelines. The service account I created here is called nextflowRUN . You don't need to do the above processes when you use GCP. But you have to do all the following processes to make sure you have the right permissions to caendr .","title":"Create service account"},{"location":"pipelines/nf-GCPconfig/#generate_credential_for_the_service_account","text":"After you get the access to caendr , go to the API & Services . Click the Create credentials button, select Service account key . And choose nextflowRUN to generate a JSON file, which is a privite key file for using nextflowRUN . Download the file and save it in a safe place. Finally, define the GOOGLE_APPLICATION_CREDENTIALS variable in .bash_profile with the directory of the JSON file. Which should looks like the example. export GOOGLE_APPLICATION_CREDENTIALS=$HOME/google_creds/caendr-2cae6210c8d1.json","title":"Generate credential for the service account"},{"location":"pipelines/nf-GCPconfig/#nextflow_version_and_mode","text":"Only the version of 19.07.0 or higher of Nextflow are compatible with GCP. And also, the Nextflow should have a google mode. You can define the version and mode in .bash_profile . export NXF_VER=19.07.0 export NXF_MODE=google Then, run the following code to update or install Nextflow. curl https://get.nextflow.io | bash","title":"Nextflow version and mode"},{"location":"pipelines/nf-GCPconfig/#configure_nextflow_for_gcp","text":"To run Nextflow pipelines on GCP, you need to build docker images for them. Check the docker file repo of our lab for more information. The google genomic API has its own executor called google-pipelines , you need to define the executor variable with google-pipelines in the nextflow.config file. Here is the example for concordance-nf . docker { enabled = true } process { executor = 'google-pipelines' withLabel: bam_coverage { container = 'faithman/bam_toolbox:latest' } container = 'faithman/concordance:latest' machineType = 'n1-standard-4' } google { project = 'caendr' zone = 'us-central1-a' } cloud { preemptible = true } executor { queueSize = 500 } Important The file system of google buckets is not like S3 that can read/write directly by most softwares. You have to use gsutil tool to interact with google buckets to read/write files in most situations. Nextflow has built-in functions to interact with google buckets, but you still can not read/write files directly in your script. All the files have to be read and write via channels in Nextflow!","title":"Configure Nextflow for GCP"},{"location":"pipelines/pipeline-alignment/","text":"alignment-nf \u00b6 alignment-nf Pipeline overview Software requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --sample_sheet --debug (optional) --species (optional) --fq_prefix (optional) --kmers (optional) --reference (optional) --ncbi (optional) --blob (optional) --output (optional) Output Data storage Cleanup Archive construct_sample_sheet.sh Adding new sequencing datasets The alignment-nf pipeline performs alignment for wild isolate sequence data at the strain level , and outputs BAMs and related information. Those BAMs can be used for downstream analysis including variant calling, concordance analysis , wi-gatk-nf (variant calling) and other analyses. This page details how to run the pipeline and how to add new wild isolate sequencing data. Note Historically, sequence processing was performed at the isotype level. We are still interested in filtering strains used in analysis at the isotype level, but alignment and variant calling are now performed at the strain level rather than at the isotype level. Pipeline overview \u00b6 \u2597\u2596 \u259d\u259c \u259d \u2597 \u2597\u2596 \u2596\u2597\u2584\u2584\u2596 \u2590\u258c \u2590 \u2597\u2584 \u2584\u2584 \u2597\u2597\u2596 \u2597\u2584\u2584 \u2584\u2596 \u2597\u2597\u2596 \u2597\u259f\u2584 \u2590\u259a \u258c\u2590 \u258c\u2590 \u2590 \u2590 \u2590\u2598\u259c \u2590\u2598\u2590 \u2590\u2590\u2590 \u2590\u2598\u2590 \u2590\u2598\u2590 \u2590 \u2590\u2590\u2596\u258c\u2590\u2584\u2584\u2596 \u2599\u259f \u2590 \u2590 \u2590 \u2590 \u2590 \u2590 \u2590\u2590\u2590 \u2590\u2580\u2580 \u2590 \u2590 \u2590 \u2580\u2598 \u2590 \u258c\u258c\u2590 \u2590 \u258c \u259d\u2584 \u2597\u259f\u2584 \u259d\u2599\u259c \u2590 \u2590 \u2590\u2590\u2590 \u259d\u2599\u259e \u2590 \u2590 \u259d\u2584 \u2590 \u2590\u258c\u2590 \u2596\u2590 \u259d\u2598 parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --sample_sheet See test_data/sample_sheet for example null --species Species to map: 'ce', 'cb' or 'ct' null --fq_prefix Path to fastq if not in sample_sheet /vast/eande106/data/{species}/WI/fastq/dna/ --kmers Whether to count kmers false --reference genome.fasta.gz to use in place of default defaults for c.e, c.b, and c.t --output Output folder name. alignment-{date} HELP: http://andersenlab.org/dry-guide/pipelines/pipeline-alignment/ Software requirements \u00b6 Nextflow v23+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env Relevant Docker Images \u00b6 Note: Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker imgaes to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/alignment-nf -r 20220216-Release andersenlab/alignment ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/align.Dockerfile or .github/workflows/build_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/blobtools ( link ): Docker image is created manually, code can be found in the dockerfile repo. andersenlab/multiqc ( link ): Docker image is created within the trim-fq-nf pipeline using GitHub actions. Whenever a change is made to env/multiqc.Dockerfile or .github/workflows/build_multiqc_docker.yml GitHub actions will create a new docker image and push if successful Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name> Note mosdepth is used to calculate coverage. mosdepth is available on Linux machines, but not on Mac OSX. That is why the conda environment for the coverage process is specified as conda { System.properties['os.name'] != \"Mac OS X\" ? 'bioconda::mosdepth=0.2.6' : \"\" } . This snippet allows mosdepth to run off the executable present in the bin folder locally on Mac OSX, or use the conda-based installation when on Linux. Usage \u00b6 Testing on Rockfish \u00b6 This command uses a test dataset nextflow run -latest andersenlab/alignment-nf --debug Running on Rockfish \u00b6 You should run this in a screen or tmux session. Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. nextflow run -latest andersenlab/alignment-nf --sample_sheet <path_to_sample_sheet> --species c_elegans Parameters \u00b6 -profile \u00b6 There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default --sample_sheet \u00b6 The sample sheet for alignment is the output from the trim-fq-nf pipeline. The sample sheet must be tsv formatted , is the full path to the sample sheet (even if it is in your current directory), and has the following columns: strain - the name of the strain. Multiple sequencing runs of the same strain are merged together. id - A unique ID for each sequencing run. This must be unique for every single pair of FASTQs. lb - A library ID. This should uniquely identify a DNA sequencing library. fq1 - The path to FASTQ1 fq2 - The path to FASTQ2 Note Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . The library column is a useful tool for identifying errors by variant callers. For example, if the same library is sequenced twice, and a variant is only observed in one sequencing run then that variant may be excluded as a technical / PCR artifact depending on the variant caller being used. Important The alignment pipeline will merge multiple sequencing runs of the same strain into a single bam. However, summary output is provided at both the strain and id level. In this way, if there is a poor sequencing run it can be identified and removed from a collection of sequencing runs belonging to a strain. For this reason, it is important that each id be unique and not just the strain name Note The sample sheet is a critical tool. It allows us to associated metadata with each sequencing run (e.g. isotype, reference strain, id, library). It also allows us to quickly verify that all results have been output. It is much easier than working with a list of files! --debug (optional) \u00b6 You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data folder) using your specified configuration profile (e.g. rockfish / quest / local). For example: nextflow run -latest andersenlab/alignment-nf --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv --species (optional) \u00b6 Defaults to \"c_elegans\", change to \"c_briggsae\" or \"c_tropicalis\" to select correct reference file. If species == \"c_elegans\", a check will be run for the npr-1 allele. Note: this process used to happen later in concordance-nf , however it was moved up to alignment-nf to avoid having to rerun the long wi-gatk process if an incorrect strain is included. --fq_prefix (optional) \u00b6 Within a sample sheet you may specify the locations of FASTQs using an absolute directory or a relative directory. If you want to use a relative directory, you should use the --fq_prefix to set the path that should be prefixed to each FASTQ. Note Previously, this option was --fqs_file_prefix --kmers (optional) \u00b6 default = false Toggles kmer-analysis --reference (optional) \u00b6 A fasta reference indexed with BWA. WS245 is packaged with the pipeline for convenience when testing or running locally. On Rockfish, the default references are here: c_elegans: /vast/eande106/data/c_elegans/genomes/PRJNA13758/WS283/c_elegans.PRJNA13758.WS283.genome.fa.gz c_briggsae: /vast/eande106/data/c_briggsae/genomes/QX1410_nanopore/Feb2020/c_briggsae.QX1410_nanopore.Feb2020.genome.fa.gz c_tropicalis: /vast/eande106/data/c_tropicalis/genomes/NIC58_nanopore/June2021/c_tropicalis.NIC58_nanopore.June2021.genome.fa.gz Note A different --project and --wsbuild can be used with the --species parameter to generate the path to other reference genomes such as: nextflow run -latest andersenlab/alignment-nf --species c_elegans --project PRJNA13758 --wsbuild WS280 --ncbi (optional) \u00b6 Default - /vast/eande106/data/other/ncbi_blast_db/ Path to the NCBI blast database used for blobtool analysis. Should not need to change. --blob (optional) \u00b6 Defaults to true. Change to false if you don't need to run blobtool analysis on low coverage strains. This step can take a while, so if you don't need it you might want to exclude it. --output (optional) \u00b6 Default - alignment-YYYYMMDD A directory in which to output results. If you have set --debug , the default output directory will be alignment-YYYYMMDD-debug . Output \u00b6 \u251c\u2500\u2500 _aggregate \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 multiqc \u2502 \u251c\u2500\u2500 strain_data/ \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-dist-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-per-contig_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-plot-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_picard_deduplication_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Normalised_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools_alignment_plot_1.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc.log \u2502 \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u2502 \u251c\u2500\u2500 multiqc_general_stats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_picard_dups.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_qualimap_bamqc_genome_results.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_flagstat.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_sources.txt \u2502 \u251c\u2500\u2500 strain_multiqc_report.html \u2502 \u251c\u2500\u2500 id_data/ \u2502 \u2502 \u2514\u2500\u2500... (same as strain_data/) \u2502 \u2514\u2500\u2500 id_multiqc_report.html \u251c\u2500\u2500 bam \u2502 \u251c\u2500\u2500 [strain].bam \u2502 \u2514\u2500\u2500 [strain].bam.bai \u251c\u2500\u2500 blobtools \u2502 \u251c\u2500\u2500 {strain}.*.blobplot.bam0.png \u2502 \u251c\u2500\u2500 {strain}.*.blobplot.read_cov.bam0.png \u2502 \u2514\u2500\u2500 {strain}.*.blobplot.stats.txt \u251c\u2500\u2500 software_versions.txt \u251c\u2500\u2500 sample_sheet.tsv \u251c\u2500\u2500 strain_summary.tsv \u251c\u2500\u2500 stats_strain_all.tsv \u251c\u2500\u2500 stats_strains_with_low_values.tsv \u251c\u2500\u2500 sample_sheet_for_seq_sheet.tsv \u251c\u2500\u2500 sample_sheet_for_seq_sheet_ALL.tsv \u251c\u2500\u2500 low_map_cov_for_seq_sheet.Rmd \u251c\u2500\u2500 low_map_cov_for_seq_sheet.html \u2514\u2500\u2500 summary.txt Most files should be obvious. A few are detailed below. software_versions.txt - Outputs the software versions used for every process (step) of the pipeline. summary.txt - Outputs a summary of the parameters used. sample_sheet.tsv - The sample sheet (input file) that was used to produce the alignment directory. strain_summary.tsv - A summary of all strains and bams in the alignment directory. aggregate - Stores data that has been aggregated across all strains or sequencing IDs. coverage - Contains coverage data at the strain or id level, presented in a variety of ways. low_map_cov_for_seq_sheet.(Rmd/html) - Report showing low coverage or problematic strains to remove. stats_strain_all.tsv - contains stats for all strains, with all replicates combined stats_strains_with_low_values.tsv - contains stats for strains with either (1) low number of reads, (2) low mapping rate, and/or (3) low coverage sample_sheet_for_seq_sheet.tsv - sample sheet to be added to google sheet, filtered to remove low coverage strains sample_sheet_for_seq_sheet_ALL.tsv - sample sheet to be added to google sheet, contains all strains (use this one) blobplot/ - contains plots for low coverage strains to see if they show contamination issues and if they should be resequenced. npr1_allele_strain.tsv - if species == c_elegans, this file will be output to show problematic strains that contain the N2 npr-1 allele and should be manually checked. Important If a new strain is flagged in the npr1_allele_strain.tsv file , tell Erik, Robyn, and the wild isolate team ASAP so they can address the issue. This strain will likely be removed from further analysis. Data storage \u00b6 Cleanup \u00b6 Once the alignment-nf pipeline has completed successfully and you have removed low coverage strains (see pipeline overview ), all BAM files can be moved to /vast/eande106/data/{species}/WI/alignments/ prior to variant calling. Note Low coverage or otherwise problematic BAM files can be moved to /vast/eande106/data/{species}/WI/alignments/_bam_not_for_cendr/ . Make sure to update the _README.md file in this folder with the reason each BAM was moved here. This will help remind people which files might be used again in the future. Archive \u00b6 The following sections have been integrated into other code that no longer needs to be run manually, but I am keeping the documentation here in case we need to go back to it. It is important to always check that the sample sheet is generated appropriately. If there are errors in teh sample sheet, one can be constructed manually using the following code: construct_sample_sheet.sh \u00b6 The scripts/construct_sample_sheet.sh script generates the WI_sample_sheet.tsv file. Warning The WI_sample_sheet.tsv file should never be generated and/or edited by hand. It should only be generated using the scripts/construct_sample_sheet.tsv script. The construct_sample_sheet.sh script does a few things. (1) Parses FASTQ Filenames Unfortunately, no two sequencing centers are alike and they use different formats for naming sequencing files. For example: ECA768_RET-S11_S79_L001_2P.fq.gz [strain]_[lib_lib#]_[sample_#]_[lane]_[read].fq.gz XZ1734_S573_L007_2P.fq.gz [strain]_[sample_#]_[lane]_[read].fq.gz In some cases they even changed formats over time! The script parses the FASTQ filenames from different sequencing centers, extracting the strain name, and a unique ID. Note that the library and unique sequencing run ID ( id ) are named somewhat arbitrarily. The most imporant aspect of these columns is that any DNA library that has been sequenced multiple times possess the same library , and that every pair of FASTQs possess a unique sequencing ID. Consider the following (fake) example: strain isotype reference_strain id library AB1 AB1 TRUE BGI2-RET2-AB1 RET2 AB1 AB1 TRUE BGI2-RET3-AB1 RET3 AB4 CB4858 FALSE BGI1-RET2-AB4 RET2 AB4 CB4858 FALSE BGI2-RET2-AB4 RET2 AB1 was sequenced twice, however two different DNA libraries were produced for each sequencing run ( RET2 and RET3 ). AB4 was also sequenced twice, but both sequencing runs were of the same DNA library (called RET2 ). Note that the id column is always unique across all sequencing runs. If you look at the WI_sample_sheet.tsv in more detail you will observe that the id and library columns are not consistantly named. This is not ideal, but it works. The inconsistancy does not affect analysis, and exists because the filenames are not consistant, but unique library and sequencing run IDs must be derived from them. (2) Clean up strain names The second thing the construct_sample_sheet.sh script does is that it replaces shorthand strain names or innapropriately named strains with the 3-letter system. For example, N2Baer is renamed to ECA254 . (3) Integrate metadata The C. elegans WI Strain Info google spreadsheet is a master spreadlist containing every strain, reference_strain, and isotype for C. elegans wild isolates. The script downloads this dataset and uses it to integrate the isotype and reference strain into the sample sheet. Adding new sequencing datasets \u00b6 Sequencing data should be added to QUEST and processed through the trimming pipeline before being added to WI_sample_sheet.tsv . Before proceeding, be sure to read pipeline-trimming To add new sequencing datasets you will need to devise a strategy for extracting the strain name, a unique ID, and sequencing library from the FASTQ filenames. This may be the same as a past dataset, in which case you can append the sequencing run folder name to the list with that format. Alternatively, you may need to create a custom set of bash commands for generating the rows corresponding to each FASTQ pair. Here is an example from the construct_sample_sheet.sh script. #===================================# # BGI-20161012-ECA23 # #===================================# out=`mktemp` seq_folder=BGI-20161012-ECA23 >&2 echo ${seq_folder} prefix=${fastq_dir}/WI/dna/processed/$seq_folder for i in `ls -1 $prefix/*1P.fq.gz`; do bname=`basename ${i}`; barcode=`zcat ${i} | grep '@' | cut -f 10 -d ':' | sed 's/_//g' | head -n 100 | uniq -c | sort -k 1,1n | cut -c 9-100 | tail -n 1` echo -e \"${bname}\\t${i}\\t${barcode}\" >> ${out} done; cat ${out} |\\ awk -v prefix=${prefix} -v seq_folder=${seq_folder} '{ fq1 = $1; fq2 = $1; LB = $3; gsub(\"N\", \"\", LB); gsub(\"1P.fq.gz\", \"2P.fq.gz\", fq2); ID = $1; gsub(\"_1P.fq.gz\", \"\", ID); split(ID, a, \"[-_]\") SM=a[2]; print SM \"\\t\" ID \"\\t\" LB \"\\t\" prefix \"/\" fq1 \"\\t\" prefix \"/\" fq2 \"\\t\" seq_folder; }' >> ${fq_sheet} Notes on this snippet: SM = strain , LB = library , and ID = id in the final output file. The sequencing run is listed in the comment box at the top. Barcodes are extracted from each FASTQ in the first forloop. These are used to define the library . The id is defined using the basename of the file. A final column corresponding to the seq_folder is always added.","title":"alignment-nf"},{"location":"pipelines/pipeline-alignment/#alignment-nf","text":"alignment-nf Pipeline overview Software requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --sample_sheet --debug (optional) --species (optional) --fq_prefix (optional) --kmers (optional) --reference (optional) --ncbi (optional) --blob (optional) --output (optional) Output Data storage Cleanup Archive construct_sample_sheet.sh Adding new sequencing datasets The alignment-nf pipeline performs alignment for wild isolate sequence data at the strain level , and outputs BAMs and related information. Those BAMs can be used for downstream analysis including variant calling, concordance analysis , wi-gatk-nf (variant calling) and other analyses. This page details how to run the pipeline and how to add new wild isolate sequencing data. Note Historically, sequence processing was performed at the isotype level. We are still interested in filtering strains used in analysis at the isotype level, but alignment and variant calling are now performed at the strain level rather than at the isotype level.","title":"alignment-nf"},{"location":"pipelines/pipeline-alignment/#pipeline_overview","text":"\u2597\u2596 \u259d\u259c \u259d \u2597 \u2597\u2596 \u2596\u2597\u2584\u2584\u2596 \u2590\u258c \u2590 \u2597\u2584 \u2584\u2584 \u2597\u2597\u2596 \u2597\u2584\u2584 \u2584\u2596 \u2597\u2597\u2596 \u2597\u259f\u2584 \u2590\u259a \u258c\u2590 \u258c\u2590 \u2590 \u2590 \u2590\u2598\u259c \u2590\u2598\u2590 \u2590\u2590\u2590 \u2590\u2598\u2590 \u2590\u2598\u2590 \u2590 \u2590\u2590\u2596\u258c\u2590\u2584\u2584\u2596 \u2599\u259f \u2590 \u2590 \u2590 \u2590 \u2590 \u2590 \u2590\u2590\u2590 \u2590\u2580\u2580 \u2590 \u2590 \u2590 \u2580\u2598 \u2590 \u258c\u258c\u2590 \u2590 \u258c \u259d\u2584 \u2597\u259f\u2584 \u259d\u2599\u259c \u2590 \u2590 \u2590\u2590\u2590 \u259d\u2599\u259e \u2590 \u2590 \u259d\u2584 \u2590 \u2590\u258c\u2590 \u2596\u2590 \u259d\u2598 parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --sample_sheet See test_data/sample_sheet for example null --species Species to map: 'ce', 'cb' or 'ct' null --fq_prefix Path to fastq if not in sample_sheet /vast/eande106/data/{species}/WI/fastq/dna/ --kmers Whether to count kmers false --reference genome.fasta.gz to use in place of default defaults for c.e, c.b, and c.t --output Output folder name. alignment-{date} HELP: http://andersenlab.org/dry-guide/pipelines/pipeline-alignment/","title":"Pipeline overview"},{"location":"pipelines/pipeline-alignment/#software_requirements","text":"Nextflow v23+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env","title":"Software requirements"},{"location":"pipelines/pipeline-alignment/#relevant_docker_images","text":"Note: Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker imgaes to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/alignment-nf -r 20220216-Release andersenlab/alignment ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/align.Dockerfile or .github/workflows/build_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/blobtools ( link ): Docker image is created manually, code can be found in the dockerfile repo. andersenlab/multiqc ( link ): Docker image is created within the trim-fq-nf pipeline using GitHub actions. Whenever a change is made to env/multiqc.Dockerfile or .github/workflows/build_multiqc_docker.yml GitHub actions will create a new docker image and push if successful Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name> Note mosdepth is used to calculate coverage. mosdepth is available on Linux machines, but not on Mac OSX. That is why the conda environment for the coverage process is specified as conda { System.properties['os.name'] != \"Mac OS X\" ? 'bioconda::mosdepth=0.2.6' : \"\" } . This snippet allows mosdepth to run off the executable present in the bin folder locally on Mac OSX, or use the conda-based installation when on Linux.","title":"Relevant Docker Images"},{"location":"pipelines/pipeline-alignment/#usage","text":"","title":"Usage"},{"location":"pipelines/pipeline-alignment/#testing_on_rockfish","text":"This command uses a test dataset nextflow run -latest andersenlab/alignment-nf --debug","title":"Testing on Rockfish"},{"location":"pipelines/pipeline-alignment/#running_on_rockfish","text":"You should run this in a screen or tmux session. Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. nextflow run -latest andersenlab/alignment-nf --sample_sheet <path_to_sample_sheet> --species c_elegans","title":"Running on Rockfish"},{"location":"pipelines/pipeline-alignment/#parameters","text":"","title":"Parameters"},{"location":"pipelines/pipeline-alignment/#-profile","text":"There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default","title":"-profile"},{"location":"pipelines/pipeline-alignment/#--sample_sheet","text":"The sample sheet for alignment is the output from the trim-fq-nf pipeline. The sample sheet must be tsv formatted , is the full path to the sample sheet (even if it is in your current directory), and has the following columns: strain - the name of the strain. Multiple sequencing runs of the same strain are merged together. id - A unique ID for each sequencing run. This must be unique for every single pair of FASTQs. lb - A library ID. This should uniquely identify a DNA sequencing library. fq1 - The path to FASTQ1 fq2 - The path to FASTQ2 Note Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . The library column is a useful tool for identifying errors by variant callers. For example, if the same library is sequenced twice, and a variant is only observed in one sequencing run then that variant may be excluded as a technical / PCR artifact depending on the variant caller being used. Important The alignment pipeline will merge multiple sequencing runs of the same strain into a single bam. However, summary output is provided at both the strain and id level. In this way, if there is a poor sequencing run it can be identified and removed from a collection of sequencing runs belonging to a strain. For this reason, it is important that each id be unique and not just the strain name Note The sample sheet is a critical tool. It allows us to associated metadata with each sequencing run (e.g. isotype, reference strain, id, library). It also allows us to quickly verify that all results have been output. It is much easier than working with a list of files!","title":"--sample_sheet"},{"location":"pipelines/pipeline-alignment/#--debug_optional","text":"You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data folder) using your specified configuration profile (e.g. rockfish / quest / local). For example: nextflow run -latest andersenlab/alignment-nf --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv","title":"--debug (optional)"},{"location":"pipelines/pipeline-alignment/#--species_optional","text":"Defaults to \"c_elegans\", change to \"c_briggsae\" or \"c_tropicalis\" to select correct reference file. If species == \"c_elegans\", a check will be run for the npr-1 allele. Note: this process used to happen later in concordance-nf , however it was moved up to alignment-nf to avoid having to rerun the long wi-gatk process if an incorrect strain is included.","title":"--species (optional)"},{"location":"pipelines/pipeline-alignment/#--fq_prefix_optional","text":"Within a sample sheet you may specify the locations of FASTQs using an absolute directory or a relative directory. If you want to use a relative directory, you should use the --fq_prefix to set the path that should be prefixed to each FASTQ. Note Previously, this option was --fqs_file_prefix","title":"--fq_prefix (optional)"},{"location":"pipelines/pipeline-alignment/#--kmers_optional","text":"default = false Toggles kmer-analysis","title":"--kmers (optional)"},{"location":"pipelines/pipeline-alignment/#--reference_optional","text":"A fasta reference indexed with BWA. WS245 is packaged with the pipeline for convenience when testing or running locally. On Rockfish, the default references are here: c_elegans: /vast/eande106/data/c_elegans/genomes/PRJNA13758/WS283/c_elegans.PRJNA13758.WS283.genome.fa.gz c_briggsae: /vast/eande106/data/c_briggsae/genomes/QX1410_nanopore/Feb2020/c_briggsae.QX1410_nanopore.Feb2020.genome.fa.gz c_tropicalis: /vast/eande106/data/c_tropicalis/genomes/NIC58_nanopore/June2021/c_tropicalis.NIC58_nanopore.June2021.genome.fa.gz Note A different --project and --wsbuild can be used with the --species parameter to generate the path to other reference genomes such as: nextflow run -latest andersenlab/alignment-nf --species c_elegans --project PRJNA13758 --wsbuild WS280","title":"--reference (optional)"},{"location":"pipelines/pipeline-alignment/#--ncbi_optional","text":"Default - /vast/eande106/data/other/ncbi_blast_db/ Path to the NCBI blast database used for blobtool analysis. Should not need to change.","title":"--ncbi (optional)"},{"location":"pipelines/pipeline-alignment/#--blob_optional","text":"Defaults to true. Change to false if you don't need to run blobtool analysis on low coverage strains. This step can take a while, so if you don't need it you might want to exclude it.","title":"--blob (optional)"},{"location":"pipelines/pipeline-alignment/#--output_optional","text":"Default - alignment-YYYYMMDD A directory in which to output results. If you have set --debug , the default output directory will be alignment-YYYYMMDD-debug .","title":"--output (optional)"},{"location":"pipelines/pipeline-alignment/#output","text":"\u251c\u2500\u2500 _aggregate \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 multiqc \u2502 \u251c\u2500\u2500 strain_data/ \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-dist-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-per-contig_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-plot-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_picard_deduplication_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Normalised_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools_alignment_plot_1.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc.log \u2502 \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u2502 \u251c\u2500\u2500 multiqc_general_stats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_picard_dups.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_qualimap_bamqc_genome_results.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_flagstat.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_sources.txt \u2502 \u251c\u2500\u2500 strain_multiqc_report.html \u2502 \u251c\u2500\u2500 id_data/ \u2502 \u2502 \u2514\u2500\u2500... (same as strain_data/) \u2502 \u2514\u2500\u2500 id_multiqc_report.html \u251c\u2500\u2500 bam \u2502 \u251c\u2500\u2500 [strain].bam \u2502 \u2514\u2500\u2500 [strain].bam.bai \u251c\u2500\u2500 blobtools \u2502 \u251c\u2500\u2500 {strain}.*.blobplot.bam0.png \u2502 \u251c\u2500\u2500 {strain}.*.blobplot.read_cov.bam0.png \u2502 \u2514\u2500\u2500 {strain}.*.blobplot.stats.txt \u251c\u2500\u2500 software_versions.txt \u251c\u2500\u2500 sample_sheet.tsv \u251c\u2500\u2500 strain_summary.tsv \u251c\u2500\u2500 stats_strain_all.tsv \u251c\u2500\u2500 stats_strains_with_low_values.tsv \u251c\u2500\u2500 sample_sheet_for_seq_sheet.tsv \u251c\u2500\u2500 sample_sheet_for_seq_sheet_ALL.tsv \u251c\u2500\u2500 low_map_cov_for_seq_sheet.Rmd \u251c\u2500\u2500 low_map_cov_for_seq_sheet.html \u2514\u2500\u2500 summary.txt Most files should be obvious. A few are detailed below. software_versions.txt - Outputs the software versions used for every process (step) of the pipeline. summary.txt - Outputs a summary of the parameters used. sample_sheet.tsv - The sample sheet (input file) that was used to produce the alignment directory. strain_summary.tsv - A summary of all strains and bams in the alignment directory. aggregate - Stores data that has been aggregated across all strains or sequencing IDs. coverage - Contains coverage data at the strain or id level, presented in a variety of ways. low_map_cov_for_seq_sheet.(Rmd/html) - Report showing low coverage or problematic strains to remove. stats_strain_all.tsv - contains stats for all strains, with all replicates combined stats_strains_with_low_values.tsv - contains stats for strains with either (1) low number of reads, (2) low mapping rate, and/or (3) low coverage sample_sheet_for_seq_sheet.tsv - sample sheet to be added to google sheet, filtered to remove low coverage strains sample_sheet_for_seq_sheet_ALL.tsv - sample sheet to be added to google sheet, contains all strains (use this one) blobplot/ - contains plots for low coverage strains to see if they show contamination issues and if they should be resequenced. npr1_allele_strain.tsv - if species == c_elegans, this file will be output to show problematic strains that contain the N2 npr-1 allele and should be manually checked. Important If a new strain is flagged in the npr1_allele_strain.tsv file , tell Erik, Robyn, and the wild isolate team ASAP so they can address the issue. This strain will likely be removed from further analysis.","title":"Output"},{"location":"pipelines/pipeline-alignment/#data_storage","text":"","title":"Data storage"},{"location":"pipelines/pipeline-alignment/#cleanup","text":"Once the alignment-nf pipeline has completed successfully and you have removed low coverage strains (see pipeline overview ), all BAM files can be moved to /vast/eande106/data/{species}/WI/alignments/ prior to variant calling. Note Low coverage or otherwise problematic BAM files can be moved to /vast/eande106/data/{species}/WI/alignments/_bam_not_for_cendr/ . Make sure to update the _README.md file in this folder with the reason each BAM was moved here. This will help remind people which files might be used again in the future.","title":"Cleanup"},{"location":"pipelines/pipeline-alignment/#archive","text":"The following sections have been integrated into other code that no longer needs to be run manually, but I am keeping the documentation here in case we need to go back to it. It is important to always check that the sample sheet is generated appropriately. If there are errors in teh sample sheet, one can be constructed manually using the following code:","title":"Archive"},{"location":"pipelines/pipeline-alignment/#construct_sample_sheetsh","text":"The scripts/construct_sample_sheet.sh script generates the WI_sample_sheet.tsv file. Warning The WI_sample_sheet.tsv file should never be generated and/or edited by hand. It should only be generated using the scripts/construct_sample_sheet.tsv script. The construct_sample_sheet.sh script does a few things. (1) Parses FASTQ Filenames Unfortunately, no two sequencing centers are alike and they use different formats for naming sequencing files. For example: ECA768_RET-S11_S79_L001_2P.fq.gz [strain]_[lib_lib#]_[sample_#]_[lane]_[read].fq.gz XZ1734_S573_L007_2P.fq.gz [strain]_[sample_#]_[lane]_[read].fq.gz In some cases they even changed formats over time! The script parses the FASTQ filenames from different sequencing centers, extracting the strain name, and a unique ID. Note that the library and unique sequencing run ID ( id ) are named somewhat arbitrarily. The most imporant aspect of these columns is that any DNA library that has been sequenced multiple times possess the same library , and that every pair of FASTQs possess a unique sequencing ID. Consider the following (fake) example: strain isotype reference_strain id library AB1 AB1 TRUE BGI2-RET2-AB1 RET2 AB1 AB1 TRUE BGI2-RET3-AB1 RET3 AB4 CB4858 FALSE BGI1-RET2-AB4 RET2 AB4 CB4858 FALSE BGI2-RET2-AB4 RET2 AB1 was sequenced twice, however two different DNA libraries were produced for each sequencing run ( RET2 and RET3 ). AB4 was also sequenced twice, but both sequencing runs were of the same DNA library (called RET2 ). Note that the id column is always unique across all sequencing runs. If you look at the WI_sample_sheet.tsv in more detail you will observe that the id and library columns are not consistantly named. This is not ideal, but it works. The inconsistancy does not affect analysis, and exists because the filenames are not consistant, but unique library and sequencing run IDs must be derived from them. (2) Clean up strain names The second thing the construct_sample_sheet.sh script does is that it replaces shorthand strain names or innapropriately named strains with the 3-letter system. For example, N2Baer is renamed to ECA254 . (3) Integrate metadata The C. elegans WI Strain Info google spreadsheet is a master spreadlist containing every strain, reference_strain, and isotype for C. elegans wild isolates. The script downloads this dataset and uses it to integrate the isotype and reference strain into the sample sheet.","title":"construct_sample_sheet.sh"},{"location":"pipelines/pipeline-alignment/#adding_new_sequencing_datasets","text":"Sequencing data should be added to QUEST and processed through the trimming pipeline before being added to WI_sample_sheet.tsv . Before proceeding, be sure to read pipeline-trimming To add new sequencing datasets you will need to devise a strategy for extracting the strain name, a unique ID, and sequencing library from the FASTQ filenames. This may be the same as a past dataset, in which case you can append the sequencing run folder name to the list with that format. Alternatively, you may need to create a custom set of bash commands for generating the rows corresponding to each FASTQ pair. Here is an example from the construct_sample_sheet.sh script. #===================================# # BGI-20161012-ECA23 # #===================================# out=`mktemp` seq_folder=BGI-20161012-ECA23 >&2 echo ${seq_folder} prefix=${fastq_dir}/WI/dna/processed/$seq_folder for i in `ls -1 $prefix/*1P.fq.gz`; do bname=`basename ${i}`; barcode=`zcat ${i} | grep '@' | cut -f 10 -d ':' | sed 's/_//g' | head -n 100 | uniq -c | sort -k 1,1n | cut -c 9-100 | tail -n 1` echo -e \"${bname}\\t${i}\\t${barcode}\" >> ${out} done; cat ${out} |\\ awk -v prefix=${prefix} -v seq_folder=${seq_folder} '{ fq1 = $1; fq2 = $1; LB = $3; gsub(\"N\", \"\", LB); gsub(\"1P.fq.gz\", \"2P.fq.gz\", fq2); ID = $1; gsub(\"_1P.fq.gz\", \"\", ID); split(ID, a, \"[-_]\") SM=a[2]; print SM \"\\t\" ID \"\\t\" LB \"\\t\" prefix \"/\" fq1 \"\\t\" prefix \"/\" fq2 \"\\t\" seq_folder; }' >> ${fq_sheet} Notes on this snippet: SM = strain , LB = library , and ID = id in the final output file. The sequencing run is listed in the comment box at the top. Barcodes are extracted from each FASTQ in the first forloop. These are used to define the library . The id is defined using the basename of the file. A final column corresponding to the seq_folder is always added.","title":"Adding new sequencing datasets"},{"location":"pipelines/pipeline-annotation-nf/","text":"annotation-nf \u00b6 annotation-nf Pipeline overview Software Requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --debug --vcf --species --divergent_regions --reference, --project, --ws_build (optional) --ncsq_param (optional) CSQ compatible GFFs Output Data storage Cleanup Updating CaeNDR and NemaScan The annotation-nf pipeline performs variant annotation for the VCF at the isotype level using both SnpEff and BCFtools/csq (BCSQ) . Note Before running, make sure to check out the genomes-nf pipeline to ensure that the reference genome and annotation databases are set up properly. Pipeline overview \u00b6 ------------- ANNOTATION-NF ------------- nextflow andersenlab/annotation-nf --debug nextflow andersenlab/annotation-nf --vcf=hard-filtered.vcf --species=c_elegans --divergent_regions=divergent_regions_strain.bed parameters description Set/Default ========== =========== ======================== --debug Set to 'true' to test ${params.debug} --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' ${params.species} --vcf hard filtered vcf to calculate variant density ${params.vcf} --divergent_regions (Optional) Divergent region bed file ${params.divergent_regions} --reference Reference used based on species and project ${params.reference} --output (Optional) output folder name ${params.output} username ${\"whoami\".execute().in.text} HELP: http://andersenlab.org/dry-guide/pipelines/pipeline-annotation-nf Software Requirements \u00b6 The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env Relevant Docker Images \u00b6 Note: Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker imgaes to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/annotation-nf -r 20220216-Release andersenlab/annotation ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/annotation.Dockerfile or .github/workflows/build_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/r_packages ( link ): Docker image is created manually, code can be found in the dockerfile repo. Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name> Usage \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. Testing on Rockfish \u00b6 This command uses a test dataset nextflow run -latest andersenlab/annotation-nf --debug Running on Rockfish \u00b6 You should run this in a screen or tmux session. nextflow run -latest andersenlab/annotation-nf --vcf <path_to_vcf> --species <species> --divergent_regions <path_to_file> Parameters \u00b6 -profile \u00b6 There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default --debug \u00b6 You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run -latest andersenlab/annotation-nf --debug --vcf \u00b6 Path to the hard-filter, isotype VCF (output from post-gatk-nf ) --species \u00b6 Choose from c_elegans, c_briggsae, or c_tropicalis. Species will specifiy a default reference genome. You can select a different one if you prefer (see below) --divergent_regions \u00b6 This is the divergent_regions_strain.bed file output from the post-gatk-nf pipeline. This file is used to add a column to the flat file if the variant is within a divergent region. Currently, C. elegans is the only species with divergent regions, if running for another species, do not provide a divergent_regions file and the pipeline will ignore it. --reference, --project, --ws_build (optional) \u00b6 By default, the reference genome is set by the species parameter. If you don't want to use the default, you could change the project and/or ws_build. As long as the genome is in the proper location on quest (for more, see the genomes-nf pipeline), this will work. Alternatively, you could provide the path to a reference of your choice. Defaults: - C. elegans - /vast/eande106/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz - C. briggsae - /vast/eande106/data/c_briggsae/genomes/QX1410_nanopore/Feb2020/c_briggsae.QX1410_nanopore.Feb2020.genome.fa.gz - C. tropicalis - /vast/eande106/data/c_tropicalis/genomes/NIC58_nanopore/June2021/c_tropicalis.NIC58_nanopore.June2021.genome.fa.gz --ncsq_param (optional) \u00b6 This parameter is necessary for correct annotation using BCSQ for variants with many different annotations (like found in divergent regions). In 20210121 we found that the default value of 224 was sufficient, but as more strains are added this number might need to increase. If there is an issue, you should see a warning error from BCFtools and they should suggest what to change this parameter to. CSQ compatible GFFs \u00b6 There are a few specific change that need to be made to the GFF to use CSQ. These additions are made with the following script library(tidyverse) #gff_file <- \"/vast/eande106/projects/Ryan/protein_structure/ben_1_convergence/annotate_cb/gffs/c_briggsae/test.gff\" fix_mRNA <- function(ID, Parent){ transcript_id <- strsplit(ID, \"=\")[[1]][2] gene_id <- strsplit(Parent, \"=\")[[1]][2] new_at <- glue::glue(\"ID={transcript_id};Parent={gene_id};biotype=protein_coding\") return(new_at) } add_mrna_biotype <- function(gff_file, transcript_type = \"mRNA\"){ gff_cols <- c(\"chrm_id\", \"source\", \"type\", \"start\", \"end\", \"score\", \"strand\", \"phase\", \"attributes\") gff <- data.table::fread(gff_file, header = FALSE, col.names = gff_cols) #Separate the attribute column mrna_features <- gff %>% dplyr::filter(type == transcript_type) %>% separate(attributes, sep=\";\", into = c(\"ID\", \"Parent\")) %>% dplyr::mutate(attributes = map2_chr(ID, Parent, fix_mRNA)) %>% select(-ID, -Parent) #return(mrna_features) other_features <- gff %>% dplyr::filter(type != transcript_type) reformatted <- bind_rows(mrna_features, other_features) #name and save output file today <- format(Sys.time(), '%Y%m%d') file_id <- basename(gff_file) write_tsv(reformatted, glue::glue(\"{file_id}_reformatted_{today}.gff\"), col_names = FALSE) } ct_gff = \"/vast/eande106/projects/Ryan/protein_structure/ben_1_convergence/annotate_cb/gffs/c_tropicalis/NIC58.final_annotation.fixed.CSQ.gff\" cb_gff = \"/vast/eande106/projects/Ryan/protein_structure/ben_1_convergence/annotate_cb/gffs/c_briggsae/Curation-VF-230214.PC.clean.renamed.csq.gff3\" #Transcript type allows the \"type\" column in the gff to be dynamic add_mrna_biotype(ct_gff, transcript_type = \"transcript\") add_mrna_biotype(cb_gff, transcript_type = \"mRNA\") As of 05/02/23 these files are in the respective genomes folder on Rockfish Output \u00b6 \u251c\u2500\u2500 strain_vcf \u2502 \u251c\u2500\u2500 {strain}.{date}.vcf.gz \u2502 \u2514\u2500\u2500 {strain}.{date}.vcf.gz.tbi \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz.tbi \u251c\u2500\u2500 snpeff.stats.csv \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.stats.txt \u2514\u2500\u2500 WI.{date}.strain-annotation.bcsq.tsv Data storage \u00b6 Cleanup \u00b6 Once the pipeline has complete successfully and you are satisfied with the results, the final data can be moved to their final storage place on Rockfish accordingly: Both the strain_vcf and the variation folders can be moved to /vast/eande106/data/{species}/WI/variation/{date}/vcf If applicable, all snpeff .bed files (HIGH, LOW, MODERATE, etc.) can be moved to /vast/eande106/data/{species}/WI/variation/{date}/tracks/ ( As of 20210901 this is no longer being produced for CaeNDR ) Updating CaeNDR and NemaScan \u00b6 Check out the CaeNDR page and the WI protocol for more information about updating a new data release for CaeNDR.","title":"annotation-nf"},{"location":"pipelines/pipeline-annotation-nf/#annotation-nf","text":"annotation-nf Pipeline overview Software Requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --debug --vcf --species --divergent_regions --reference, --project, --ws_build (optional) --ncsq_param (optional) CSQ compatible GFFs Output Data storage Cleanup Updating CaeNDR and NemaScan The annotation-nf pipeline performs variant annotation for the VCF at the isotype level using both SnpEff and BCFtools/csq (BCSQ) . Note Before running, make sure to check out the genomes-nf pipeline to ensure that the reference genome and annotation databases are set up properly.","title":"annotation-nf"},{"location":"pipelines/pipeline-annotation-nf/#pipeline_overview","text":"------------- ANNOTATION-NF ------------- nextflow andersenlab/annotation-nf --debug nextflow andersenlab/annotation-nf --vcf=hard-filtered.vcf --species=c_elegans --divergent_regions=divergent_regions_strain.bed parameters description Set/Default ========== =========== ======================== --debug Set to 'true' to test ${params.debug} --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' ${params.species} --vcf hard filtered vcf to calculate variant density ${params.vcf} --divergent_regions (Optional) Divergent region bed file ${params.divergent_regions} --reference Reference used based on species and project ${params.reference} --output (Optional) output folder name ${params.output} username ${\"whoami\".execute().in.text} HELP: http://andersenlab.org/dry-guide/pipelines/pipeline-annotation-nf","title":"Pipeline overview"},{"location":"pipelines/pipeline-annotation-nf/#software_requirements","text":"The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env","title":"Software Requirements"},{"location":"pipelines/pipeline-annotation-nf/#relevant_docker_images","text":"Note: Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker imgaes to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/annotation-nf -r 20220216-Release andersenlab/annotation ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/annotation.Dockerfile or .github/workflows/build_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/r_packages ( link ): Docker image is created manually, code can be found in the dockerfile repo. Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name>","title":"Relevant Docker Images"},{"location":"pipelines/pipeline-annotation-nf/#usage","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page.","title":"Usage"},{"location":"pipelines/pipeline-annotation-nf/#testing_on_rockfish","text":"This command uses a test dataset nextflow run -latest andersenlab/annotation-nf --debug","title":"Testing on Rockfish"},{"location":"pipelines/pipeline-annotation-nf/#running_on_rockfish","text":"You should run this in a screen or tmux session. nextflow run -latest andersenlab/annotation-nf --vcf <path_to_vcf> --species <species> --divergent_regions <path_to_file>","title":"Running on Rockfish"},{"location":"pipelines/pipeline-annotation-nf/#parameters","text":"","title":"Parameters"},{"location":"pipelines/pipeline-annotation-nf/#-profile","text":"There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default","title":"-profile"},{"location":"pipelines/pipeline-annotation-nf/#--debug","text":"You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run -latest andersenlab/annotation-nf --debug","title":"--debug"},{"location":"pipelines/pipeline-annotation-nf/#--vcf","text":"Path to the hard-filter, isotype VCF (output from post-gatk-nf )","title":"--vcf"},{"location":"pipelines/pipeline-annotation-nf/#--species","text":"Choose from c_elegans, c_briggsae, or c_tropicalis. Species will specifiy a default reference genome. You can select a different one if you prefer (see below)","title":"--species"},{"location":"pipelines/pipeline-annotation-nf/#--divergent_regions","text":"This is the divergent_regions_strain.bed file output from the post-gatk-nf pipeline. This file is used to add a column to the flat file if the variant is within a divergent region. Currently, C. elegans is the only species with divergent regions, if running for another species, do not provide a divergent_regions file and the pipeline will ignore it.","title":"--divergent_regions"},{"location":"pipelines/pipeline-annotation-nf/#--reference_--project_--ws_build_optional","text":"By default, the reference genome is set by the species parameter. If you don't want to use the default, you could change the project and/or ws_build. As long as the genome is in the proper location on quest (for more, see the genomes-nf pipeline), this will work. Alternatively, you could provide the path to a reference of your choice. Defaults: - C. elegans - /vast/eande106/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz - C. briggsae - /vast/eande106/data/c_briggsae/genomes/QX1410_nanopore/Feb2020/c_briggsae.QX1410_nanopore.Feb2020.genome.fa.gz - C. tropicalis - /vast/eande106/data/c_tropicalis/genomes/NIC58_nanopore/June2021/c_tropicalis.NIC58_nanopore.June2021.genome.fa.gz","title":"--reference, --project, --ws_build (optional)"},{"location":"pipelines/pipeline-annotation-nf/#--ncsq_param_optional","text":"This parameter is necessary for correct annotation using BCSQ for variants with many different annotations (like found in divergent regions). In 20210121 we found that the default value of 224 was sufficient, but as more strains are added this number might need to increase. If there is an issue, you should see a warning error from BCFtools and they should suggest what to change this parameter to.","title":"--ncsq_param (optional)"},{"location":"pipelines/pipeline-annotation-nf/#csq_compatible_gffs","text":"There are a few specific change that need to be made to the GFF to use CSQ. These additions are made with the following script library(tidyverse) #gff_file <- \"/vast/eande106/projects/Ryan/protein_structure/ben_1_convergence/annotate_cb/gffs/c_briggsae/test.gff\" fix_mRNA <- function(ID, Parent){ transcript_id <- strsplit(ID, \"=\")[[1]][2] gene_id <- strsplit(Parent, \"=\")[[1]][2] new_at <- glue::glue(\"ID={transcript_id};Parent={gene_id};biotype=protein_coding\") return(new_at) } add_mrna_biotype <- function(gff_file, transcript_type = \"mRNA\"){ gff_cols <- c(\"chrm_id\", \"source\", \"type\", \"start\", \"end\", \"score\", \"strand\", \"phase\", \"attributes\") gff <- data.table::fread(gff_file, header = FALSE, col.names = gff_cols) #Separate the attribute column mrna_features <- gff %>% dplyr::filter(type == transcript_type) %>% separate(attributes, sep=\";\", into = c(\"ID\", \"Parent\")) %>% dplyr::mutate(attributes = map2_chr(ID, Parent, fix_mRNA)) %>% select(-ID, -Parent) #return(mrna_features) other_features <- gff %>% dplyr::filter(type != transcript_type) reformatted <- bind_rows(mrna_features, other_features) #name and save output file today <- format(Sys.time(), '%Y%m%d') file_id <- basename(gff_file) write_tsv(reformatted, glue::glue(\"{file_id}_reformatted_{today}.gff\"), col_names = FALSE) } ct_gff = \"/vast/eande106/projects/Ryan/protein_structure/ben_1_convergence/annotate_cb/gffs/c_tropicalis/NIC58.final_annotation.fixed.CSQ.gff\" cb_gff = \"/vast/eande106/projects/Ryan/protein_structure/ben_1_convergence/annotate_cb/gffs/c_briggsae/Curation-VF-230214.PC.clean.renamed.csq.gff3\" #Transcript type allows the \"type\" column in the gff to be dynamic add_mrna_biotype(ct_gff, transcript_type = \"transcript\") add_mrna_biotype(cb_gff, transcript_type = \"mRNA\") As of 05/02/23 these files are in the respective genomes folder on Rockfish","title":"CSQ compatible GFFs"},{"location":"pipelines/pipeline-annotation-nf/#output","text":"\u251c\u2500\u2500 strain_vcf \u2502 \u251c\u2500\u2500 {strain}.{date}.vcf.gz \u2502 \u2514\u2500\u2500 {strain}.{date}.vcf.gz.tbi \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz.tbi \u251c\u2500\u2500 snpeff.stats.csv \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.stats.txt \u2514\u2500\u2500 WI.{date}.strain-annotation.bcsq.tsv","title":"Output"},{"location":"pipelines/pipeline-annotation-nf/#data_storage","text":"","title":"Data storage"},{"location":"pipelines/pipeline-annotation-nf/#cleanup","text":"Once the pipeline has complete successfully and you are satisfied with the results, the final data can be moved to their final storage place on Rockfish accordingly: Both the strain_vcf and the variation folders can be moved to /vast/eande106/data/{species}/WI/variation/{date}/vcf If applicable, all snpeff .bed files (HIGH, LOW, MODERATE, etc.) can be moved to /vast/eande106/data/{species}/WI/variation/{date}/tracks/ ( As of 20210901 this is no longer being produced for CaeNDR )","title":"Cleanup"},{"location":"pipelines/pipeline-annotation-nf/#updating_caendr_and_nemascan","text":"Check out the CaeNDR page and the WI protocol for more information about updating a new data release for CaeNDR.","title":"Updating CaeNDR and NemaScan"},{"location":"pipelines/pipeline-cellprofiler/","text":"Andersen Lab Image Analysis Pipeline \u00b6 Implemented using CellProfiler \u00b6 Andersen Lab Image Analysis Pipeline Implemented using CellProfiler A Pipeline overview Software requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters --project --pipeline --groups --outdir CellProfiler Repository Data Directory Structure (input_data/ With Example Files) Input Directory Structure dauer input toxin input Output Directory Structure dauer output toxin output The cellprofiler-nf pipeline performs worm position and stage identification using CellProfiler and process the output. A Pipeline overview \u00b6 C E L L P R O F I L E R - N F P I P E L I N E parameters description Set/Default ========== =========== ======================== --project The path to your project directory Required --pipeline The CP pipeline to use: toxin, dauer Required --groups Comma separated metadata groupings \"plate,well\" --outdir Output directory to place files \"{projectdir}/Analysis-{date}\" --help This usage statement Software requirements \u00b6 Nextflow v23+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env Relevant Docker Images \u00b6 Note: Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker imgaes to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/cellprofiler-nf -r 20220216-Release cellprofiler/cellprofiler ( link ): Docker image is maintained by the Broad Institute of MIT and Harvard andersenlab/r_packages ( link ): Docker image is created manually, code can be found in the dockerfile repo. Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name> Usage \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. Testing on Rockfish \u00b6 This command uses a test dataset nextflow run -latest andersenlab/cellprofiler-nf --debug Running on Rockfish \u00b6 You should run this in a screen or tmux session. nextflow run -latest andersenlab/cellprofiler --pipeline dauer --project /vast/eande106/project/<user>/<project_dir> Parameters \u00b6 --project \u00b6 The path to your project directory which must contain a directory named \"raw_images\" containing your images to analyze --pipeline \u00b6 The CP pipeline to use: toxin or dauer --groups \u00b6 Comma separated metadata groupings (default: \"plate,well\") --outdir \u00b6 Directory to store results in. (default: /Analysis-{date}) CellProfiler Repository Data Directory Structure ( input_data/ With Example Files) \u00b6 input_data \u251c\u2500\u2500 batch_files | \u2514\u2500\u2500 20191119_example_batch_20201018.h5 \u251c\u2500\u2500 metadata | \u2514\u2500\u2500 20191119_example_metadata_20201018.csv \u251c\u2500\u2500 pipelines | \u251c\u2500\u2500 20191119_example.cpproj | \u2514\u2500\u2500 sample_pipelines \u251c\u2500\u2500 projects | \u2514\u2500\u2500 20191119_example | \u251c\u2500\u2500 raw_images | \u2514\u2500\u2500 output_data | \u2514\u2500\u2500 20191119_example_data_1603047856 | \u251c\u2500\u2500 CellProfiler-Analysis_20191119_example_data_1603047856run1 | \u251c\u2500\u2500 Logs | \u251c\u2500\u2500 ProcessedImages | \u251c\u2500\u2500 OverlappingWorms_Data | \u2514\u2500\u2500 NonOverlappingWorms_Data \u251c\u2500\u2500 scripts | \u251c\u2500\u2500 generate_metadata.R | \u251c\u2500\u2500 run_cellprofiler.sh | \u251c\u2500\u2500 cellprofiler_parallel.sh | \u251c\u2500\u2500 check_run_cellprofiler.sh | \u2514\u2500\u2500 aggregate_cellprofiler_results.R \u251c\u2500\u2500 well_masks | \u2514\u2500\u2500 wellmask_98.png \u2514\u2500\u2500 worm_models \u251c\u2500\u2500 Adult_N2_HB101_100w.xml \u251c\u2500\u2500 L1_N2_HB101_100w.xml \u251c\u2500\u2500 L2L3_N2_HB101_100w.xml \u251c\u2500\u2500 L4_N2_HB101_100w.xml \u251c\u2500\u2500 WM_FBZ_control.xml \u251c\u2500\u2500 WM_FBZ_dose.xml \u2514\u2500\u2500 high_dose_worm_model.xml Input Directory Structure \u00b6 dauer input \u00b6 <project folder name>/ \u2514\u2500\u2500 raw_images \u251c\u2500\u2500 20191119-project-p01-m2x_A01_w1.tif \u251c\u2500\u2500 20191119-project-p01-m2x_A02_w1.tif \u2514\u2500\u2500 ... toxin input \u00b6 <project folder name>/ \u2514\u2500\u2500 raw_images \u251c\u2500\u2500 20191119-project-p01-m2x_A01.tif \u251c\u2500\u2500 20191119-project-p01-m2x_A02.tif \u2514\u2500\u2500 ... Output Directory Structure \u00b6 dauer output \u00b6 <project folder name>/ \u251c\u2500\u2500 raw_images \u2514\u2500\u2500 Analysis-{current date} \u251c\u2500\u2500 pipeline \u251c\u2500\u2500 metadata \u251c\u2500\u2500 groups \u251c\u2500\u2500 processed_data | \u2514\u2500\u2500 20220501_dauerDebug_Analysis-{current date}.RData \u2514\u2500\u2500 processed_images \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w1_overlay.png \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w1_dauerMod_straightened_RFP.png \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w1_nondauerMod_straightened_RFP.png \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w2_dauerMod_NonOverlappingWorms_RFP_mask.png \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w2_nondauerMod_NonOverlappingWorms_RFP_mask.png \u2514\u2500\u2500 ... toxin output \u00b6 <project folder name>/ \u251c\u2500\u2500 raw_images \u2514\u2500\u2500 Analysis-{current date} \u251c\u2500\u2500 pipeline \u251c\u2500\u2500 metadata \u251c\u2500\u2500 groups \u251c\u2500\u2500 processed_data | \u2514\u2500\u2500 20220501_toxinDebug_Analysis-{current date}.RData \u2514\u2500\u2500 processed_images \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w1_overlay.png \u2514\u2500\u2500 ... These data can be analyzed using the R/easyXpress package","title":"Image Analysis"},{"location":"pipelines/pipeline-cellprofiler/#andersen_lab_image_analysis_pipeline","text":"","title":"Andersen Lab Image Analysis Pipeline"},{"location":"pipelines/pipeline-cellprofiler/#implemented_using_cellprofiler","text":"Andersen Lab Image Analysis Pipeline Implemented using CellProfiler A Pipeline overview Software requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters --project --pipeline --groups --outdir CellProfiler Repository Data Directory Structure (input_data/ With Example Files) Input Directory Structure dauer input toxin input Output Directory Structure dauer output toxin output The cellprofiler-nf pipeline performs worm position and stage identification using CellProfiler and process the output.","title":"Implemented using CellProfiler"},{"location":"pipelines/pipeline-cellprofiler/#a_pipeline_overview","text":"C E L L P R O F I L E R - N F P I P E L I N E parameters description Set/Default ========== =========== ======================== --project The path to your project directory Required --pipeline The CP pipeline to use: toxin, dauer Required --groups Comma separated metadata groupings \"plate,well\" --outdir Output directory to place files \"{projectdir}/Analysis-{date}\" --help This usage statement","title":"A Pipeline overview"},{"location":"pipelines/pipeline-cellprofiler/#software_requirements","text":"Nextflow v23+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env","title":"Software requirements"},{"location":"pipelines/pipeline-cellprofiler/#relevant_docker_images","text":"Note: Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker imgaes to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/cellprofiler-nf -r 20220216-Release cellprofiler/cellprofiler ( link ): Docker image is maintained by the Broad Institute of MIT and Harvard andersenlab/r_packages ( link ): Docker image is created manually, code can be found in the dockerfile repo. Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name>","title":"Relevant Docker Images"},{"location":"pipelines/pipeline-cellprofiler/#usage","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page.","title":"Usage"},{"location":"pipelines/pipeline-cellprofiler/#testing_on_rockfish","text":"This command uses a test dataset nextflow run -latest andersenlab/cellprofiler-nf --debug","title":"Testing on Rockfish"},{"location":"pipelines/pipeline-cellprofiler/#running_on_rockfish","text":"You should run this in a screen or tmux session. nextflow run -latest andersenlab/cellprofiler --pipeline dauer --project /vast/eande106/project/<user>/<project_dir>","title":"Running on Rockfish"},{"location":"pipelines/pipeline-cellprofiler/#parameters","text":"","title":"Parameters"},{"location":"pipelines/pipeline-cellprofiler/#--project","text":"The path to your project directory which must contain a directory named \"raw_images\" containing your images to analyze","title":"--project"},{"location":"pipelines/pipeline-cellprofiler/#--pipeline","text":"The CP pipeline to use: toxin or dauer","title":"--pipeline"},{"location":"pipelines/pipeline-cellprofiler/#--groups","text":"Comma separated metadata groupings (default: \"plate,well\")","title":"--groups"},{"location":"pipelines/pipeline-cellprofiler/#--outdir","text":"Directory to store results in. (default: /Analysis-{date})","title":"--outdir"},{"location":"pipelines/pipeline-cellprofiler/#cellprofiler_repository_data_directory_structure_input_data_with_example_files","text":"input_data \u251c\u2500\u2500 batch_files | \u2514\u2500\u2500 20191119_example_batch_20201018.h5 \u251c\u2500\u2500 metadata | \u2514\u2500\u2500 20191119_example_metadata_20201018.csv \u251c\u2500\u2500 pipelines | \u251c\u2500\u2500 20191119_example.cpproj | \u2514\u2500\u2500 sample_pipelines \u251c\u2500\u2500 projects | \u2514\u2500\u2500 20191119_example | \u251c\u2500\u2500 raw_images | \u2514\u2500\u2500 output_data | \u2514\u2500\u2500 20191119_example_data_1603047856 | \u251c\u2500\u2500 CellProfiler-Analysis_20191119_example_data_1603047856run1 | \u251c\u2500\u2500 Logs | \u251c\u2500\u2500 ProcessedImages | \u251c\u2500\u2500 OverlappingWorms_Data | \u2514\u2500\u2500 NonOverlappingWorms_Data \u251c\u2500\u2500 scripts | \u251c\u2500\u2500 generate_metadata.R | \u251c\u2500\u2500 run_cellprofiler.sh | \u251c\u2500\u2500 cellprofiler_parallel.sh | \u251c\u2500\u2500 check_run_cellprofiler.sh | \u2514\u2500\u2500 aggregate_cellprofiler_results.R \u251c\u2500\u2500 well_masks | \u2514\u2500\u2500 wellmask_98.png \u2514\u2500\u2500 worm_models \u251c\u2500\u2500 Adult_N2_HB101_100w.xml \u251c\u2500\u2500 L1_N2_HB101_100w.xml \u251c\u2500\u2500 L2L3_N2_HB101_100w.xml \u251c\u2500\u2500 L4_N2_HB101_100w.xml \u251c\u2500\u2500 WM_FBZ_control.xml \u251c\u2500\u2500 WM_FBZ_dose.xml \u2514\u2500\u2500 high_dose_worm_model.xml","title":"CellProfiler Repository Data Directory Structure (input_data/ With Example Files)"},{"location":"pipelines/pipeline-cellprofiler/#input_directory_structure","text":"","title":"Input Directory Structure"},{"location":"pipelines/pipeline-cellprofiler/#dauer_input","text":"<project folder name>/ \u2514\u2500\u2500 raw_images \u251c\u2500\u2500 20191119-project-p01-m2x_A01_w1.tif \u251c\u2500\u2500 20191119-project-p01-m2x_A02_w1.tif \u2514\u2500\u2500 ...","title":"dauer input"},{"location":"pipelines/pipeline-cellprofiler/#toxin_input","text":"<project folder name>/ \u2514\u2500\u2500 raw_images \u251c\u2500\u2500 20191119-project-p01-m2x_A01.tif \u251c\u2500\u2500 20191119-project-p01-m2x_A02.tif \u2514\u2500\u2500 ...","title":"toxin input"},{"location":"pipelines/pipeline-cellprofiler/#output_directory_structure","text":"","title":"Output Directory Structure"},{"location":"pipelines/pipeline-cellprofiler/#dauer_output","text":"<project folder name>/ \u251c\u2500\u2500 raw_images \u2514\u2500\u2500 Analysis-{current date} \u251c\u2500\u2500 pipeline \u251c\u2500\u2500 metadata \u251c\u2500\u2500 groups \u251c\u2500\u2500 processed_data | \u2514\u2500\u2500 20220501_dauerDebug_Analysis-{current date}.RData \u2514\u2500\u2500 processed_images \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w1_overlay.png \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w1_dauerMod_straightened_RFP.png \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w1_nondauerMod_straightened_RFP.png \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w2_dauerMod_NonOverlappingWorms_RFP_mask.png \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w2_nondauerMod_NonOverlappingWorms_RFP_mask.png \u2514\u2500\u2500 ...","title":"dauer output"},{"location":"pipelines/pipeline-cellprofiler/#toxin_output","text":"<project folder name>/ \u251c\u2500\u2500 raw_images \u2514\u2500\u2500 Analysis-{current date} \u251c\u2500\u2500 pipeline \u251c\u2500\u2500 metadata \u251c\u2500\u2500 groups \u251c\u2500\u2500 processed_data | \u2514\u2500\u2500 20220501_toxinDebug_Analysis-{current date}.RData \u2514\u2500\u2500 processed_images \u251c\u2500\u2500 20220501_dauerDebug-p002-m2X_A01_w1_overlay.png \u2514\u2500\u2500 ... These data can be analyzed using the R/easyXpress package","title":"toxin output"},{"location":"pipelines/pipeline-concordance/","text":"concordance-nf \u00b6 concordance-nf Pipeline overview Software Requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --bam_coverage --vcf --species (optional) --concordance_cutoff (optional) --cores (optional) --out (optional) Output The concordance-nf pipeline is used to detect sample swaps and determine which wild isolate strains should be grouped together as an isotype. To determine which strains belong to the same isotype we use two criteria. First we look at the strains that group together with a concordance threshold of 99.95%. Generally this will group most isotypes without issue. However, it is possible that you will run into cases where the grouping is not clean. For example, strain A groups with B, B groups with C, but C does not group with A. In these cases you must examine the data closely to identify why strains are incompletely grouping. Our second criteria we use to group isotypes may address these types of groupings. The second criteria that we use to group isotypes regards looking for regional differences among strains. If two strains are similar but possess a region of their genome (binned at 1 Mb) that differs by more than 2% then we will separate them out into their own isotypes. The process of grouping isotypes is very hand-on. This pipeline will help process the data but you must carefully review the output and investigate closely. Pipeline overview \u00b6 \u250c\u2500\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u2500\u2510\u250c\u252c\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510 \u250c\u2510\u250c\u250c\u2500\u2510 \u2502 \u2502 \u2502\u2502\u2502\u2502\u2502 \u2502 \u2502\u251c\u252c\u2518 \u2502\u2502\u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u251c\u2524\u2500\u2500\u2500\u2502\u2502\u2502\u251c\u2524 \u2514\u2500\u2518\u2514\u2500\u2518\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518\u2534\u2514\u2500\u2500\u2534\u2518\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518 \u2518\u2514\u2518\u2514 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Regular job cores 4 --out Directory to output results concordance-{date} --vcf Hard filtered vcf null --bam_coverage Table with \"strain\" and \"coverage\" as header null --info_sheet Strain sheet containing exisiting isotype assignment null --species 'c_elegans' will check for npr1. All other values will skip this null --concordance_cutoff Cutoff of concordance value to count two strains as same isotype 0.9995 Software Requirements \u00b6 The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env Relevant Docker Images \u00b6 Note: Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker imgaes to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/concordance-nf -r 20220216-Release andersenlab/concordance ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/concordance.Dockerfile or .github/workflows/build_docker.yml GitHub actions will create a new docker image and push if successful Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name> Usage \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. Testing on Rockfish \u00b6 This command uses a test dataset nextflow run -latest andersenlab/concordance-nf --debug Running on Rockfish \u00b6 You should run this in a screen or tmux session. nextflow run -latest andersenlab/concordance-nf --vcf=a.vcf.gz --bam_coverage=mqc_mosdepth-coverage-per-contig_1.txt Parameters \u00b6 -profile \u00b6 There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default --bam_coverage \u00b6 The sample sheet to use. This is generally the same sample sheet used for wi-gatk . The sample sheet should look like this: Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced. --vcf \u00b6 The hard-filtered VCF output from wi-gatk . --species (optional) \u00b6 Common options include 'c_elegans', 'c_briggsae', and 'c_tropicalis'. --concordance_cutoff (optional) \u00b6 Cutoff to use to determine isotype groups. Default is 0.9995. --cores (optional) \u00b6 The number of cores to use during alignments and variant calling. --out (optional) \u00b6 A directory in which to output results. By default it will be concordance-YYYYMMDD where YYYYMMDD is todays date. Output \u00b6 \u251c\u2500\u2500 concordance \u251c\u2500\u2500 gtcheck.tsv \u251c\u2500\u2500 isotype_count.txt \u251c\u2500\u2500 isotype_groups.tsv \u251c\u2500\u2500 problem_strains.tsv \u251c\u2500\u2500 WI_metadata.tsv \u251c\u2500\u2500 concordance.pdf/png \u251c\u2500\u2500 xconcordance.pdf/png \u2514\u2500\u2500 pairwise \u2514\u2500\u2500 within_group \u2514\u2500\u2500 {isotype_group}.{isotype}.{strain1}_{strain2}.png concordance.png/pdf - An image showing the distribution of pairwise concordances across all strains. The cutoff is at 99.9% above which pairs are considered to be in the same isotype unless issues arise. xconcordance.png/pdf - A close up view of the concordances showing more detail. isotype_groups.tsv - This is the one of the most important output files . It illustrates the isotypes identified for each strain and identifies potential issues. A file with the following structure: group - A number used to group strains (in each row) into an isotype automatically. This number should be unique with the isotype column (e.g. 1--> AB1, 112 --> CB4858, BRC20067 --> 175). The number can change between analyses. strain - the strain isotype - the currently assigned isotype for a strain taken from the WI Strain Info spreadsheet. When new strains are added this is blank. latitude longitude coverage - Depth of coverage for strain. unique_isotypes_per_group - Number of unique isotypes when grouping by the group column. This should be 1. If it is more than 1, it indicates that multiple isotypes were assigned to a grouping and that a previously assigned isotype is now being called into question. unique_groups_per_isotype Number of unique groups assigned to an isotype. This should be 1. If it is higher than 1, it indicates that a strain is concordant with strains in two different isotypes (including blanks). If it is equal to 2 and contains blanks in the isotype column it likely means that an isotype should be assigned to that strain. strain_in_multiple_isotypes - Indicates that a strain is falling into multiple isotypes (a problem!). location_issue - Indicates a location issue. This occurs when strains fall into an isotype but are located far away from one another. Some are known issues and can be ignored. strain_conflict - TRUE if any issue is present that should be investigated. Note This file might change as you manually adjust the concordance cutoff for each run gtcheck.tsv - the other most important file . File produced using bcftools gtcheck ; Raw genotype differences between strains. This file is used in manual inspection of the isotype groups isotype_count.txt - Gives a count of the number of isotypes identified. concordance/pairwise/ (directory) Contains images showing locations where regional discordance occurs among strains classified as being the isotype. You must look through all these images to ensure there are no strains being grouped that have regions with significant differences (> 2%). The image below illustrates an example of this. ED3049 and ED3046 are highly similar (> 99.9%). However, they differ in a region on the right arm of chromosome II. We believe this was enough reason to consider them separate isotypes.","title":"concordance-nf"},{"location":"pipelines/pipeline-concordance/#concordance-nf","text":"concordance-nf Pipeline overview Software Requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --bam_coverage --vcf --species (optional) --concordance_cutoff (optional) --cores (optional) --out (optional) Output The concordance-nf pipeline is used to detect sample swaps and determine which wild isolate strains should be grouped together as an isotype. To determine which strains belong to the same isotype we use two criteria. First we look at the strains that group together with a concordance threshold of 99.95%. Generally this will group most isotypes without issue. However, it is possible that you will run into cases where the grouping is not clean. For example, strain A groups with B, B groups with C, but C does not group with A. In these cases you must examine the data closely to identify why strains are incompletely grouping. Our second criteria we use to group isotypes may address these types of groupings. The second criteria that we use to group isotypes regards looking for regional differences among strains. If two strains are similar but possess a region of their genome (binned at 1 Mb) that differs by more than 2% then we will separate them out into their own isotypes. The process of grouping isotypes is very hand-on. This pipeline will help process the data but you must carefully review the output and investigate closely.","title":"concordance-nf"},{"location":"pipelines/pipeline-concordance/#pipeline_overview","text":"\u250c\u2500\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u2500\u2510\u250c\u252c\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510 \u250c\u2510\u250c\u250c\u2500\u2510 \u2502 \u2502 \u2502\u2502\u2502\u2502\u2502 \u2502 \u2502\u251c\u252c\u2518 \u2502\u2502\u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u251c\u2524\u2500\u2500\u2500\u2502\u2502\u2502\u251c\u2524 \u2514\u2500\u2518\u2514\u2500\u2518\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518\u2534\u2514\u2500\u2500\u2534\u2518\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518 \u2518\u2514\u2518\u2514 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Regular job cores 4 --out Directory to output results concordance-{date} --vcf Hard filtered vcf null --bam_coverage Table with \"strain\" and \"coverage\" as header null --info_sheet Strain sheet containing exisiting isotype assignment null --species 'c_elegans' will check for npr1. All other values will skip this null --concordance_cutoff Cutoff of concordance value to count two strains as same isotype 0.9995","title":"Pipeline overview"},{"location":"pipelines/pipeline-concordance/#software_requirements","text":"The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env","title":"Software Requirements"},{"location":"pipelines/pipeline-concordance/#relevant_docker_images","text":"Note: Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker imgaes to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/concordance-nf -r 20220216-Release andersenlab/concordance ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/concordance.Dockerfile or .github/workflows/build_docker.yml GitHub actions will create a new docker image and push if successful Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name>","title":"Relevant Docker Images"},{"location":"pipelines/pipeline-concordance/#usage","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page.","title":"Usage"},{"location":"pipelines/pipeline-concordance/#testing_on_rockfish","text":"This command uses a test dataset nextflow run -latest andersenlab/concordance-nf --debug","title":"Testing on Rockfish"},{"location":"pipelines/pipeline-concordance/#running_on_rockfish","text":"You should run this in a screen or tmux session. nextflow run -latest andersenlab/concordance-nf --vcf=a.vcf.gz --bam_coverage=mqc_mosdepth-coverage-per-contig_1.txt","title":"Running on Rockfish"},{"location":"pipelines/pipeline-concordance/#parameters","text":"","title":"Parameters"},{"location":"pipelines/pipeline-concordance/#-profile","text":"There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default","title":"-profile"},{"location":"pipelines/pipeline-concordance/#--bam_coverage","text":"The sample sheet to use. This is generally the same sample sheet used for wi-gatk . The sample sheet should look like this: Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced.","title":"--bam_coverage"},{"location":"pipelines/pipeline-concordance/#--vcf","text":"The hard-filtered VCF output from wi-gatk .","title":"--vcf"},{"location":"pipelines/pipeline-concordance/#--species_optional","text":"Common options include 'c_elegans', 'c_briggsae', and 'c_tropicalis'.","title":"--species (optional)"},{"location":"pipelines/pipeline-concordance/#--concordance_cutoff_optional","text":"Cutoff to use to determine isotype groups. Default is 0.9995.","title":"--concordance_cutoff (optional)"},{"location":"pipelines/pipeline-concordance/#--cores_optional","text":"The number of cores to use during alignments and variant calling.","title":"--cores (optional)"},{"location":"pipelines/pipeline-concordance/#--out_optional","text":"A directory in which to output results. By default it will be concordance-YYYYMMDD where YYYYMMDD is todays date.","title":"--out (optional)"},{"location":"pipelines/pipeline-concordance/#output","text":"\u251c\u2500\u2500 concordance \u251c\u2500\u2500 gtcheck.tsv \u251c\u2500\u2500 isotype_count.txt \u251c\u2500\u2500 isotype_groups.tsv \u251c\u2500\u2500 problem_strains.tsv \u251c\u2500\u2500 WI_metadata.tsv \u251c\u2500\u2500 concordance.pdf/png \u251c\u2500\u2500 xconcordance.pdf/png \u2514\u2500\u2500 pairwise \u2514\u2500\u2500 within_group \u2514\u2500\u2500 {isotype_group}.{isotype}.{strain1}_{strain2}.png concordance.png/pdf - An image showing the distribution of pairwise concordances across all strains. The cutoff is at 99.9% above which pairs are considered to be in the same isotype unless issues arise. xconcordance.png/pdf - A close up view of the concordances showing more detail. isotype_groups.tsv - This is the one of the most important output files . It illustrates the isotypes identified for each strain and identifies potential issues. A file with the following structure: group - A number used to group strains (in each row) into an isotype automatically. This number should be unique with the isotype column (e.g. 1--> AB1, 112 --> CB4858, BRC20067 --> 175). The number can change between analyses. strain - the strain isotype - the currently assigned isotype for a strain taken from the WI Strain Info spreadsheet. When new strains are added this is blank. latitude longitude coverage - Depth of coverage for strain. unique_isotypes_per_group - Number of unique isotypes when grouping by the group column. This should be 1. If it is more than 1, it indicates that multiple isotypes were assigned to a grouping and that a previously assigned isotype is now being called into question. unique_groups_per_isotype Number of unique groups assigned to an isotype. This should be 1. If it is higher than 1, it indicates that a strain is concordant with strains in two different isotypes (including blanks). If it is equal to 2 and contains blanks in the isotype column it likely means that an isotype should be assigned to that strain. strain_in_multiple_isotypes - Indicates that a strain is falling into multiple isotypes (a problem!). location_issue - Indicates a location issue. This occurs when strains fall into an isotype but are located far away from one another. Some are known issues and can be ignored. strain_conflict - TRUE if any issue is present that should be investigated. Note This file might change as you manually adjust the concordance cutoff for each run gtcheck.tsv - the other most important file . File produced using bcftools gtcheck ; Raw genotype differences between strains. This file is used in manual inspection of the isotype groups isotype_count.txt - Gives a count of the number of isotypes identified. concordance/pairwise/ (directory) Contains images showing locations where regional discordance occurs among strains classified as being the isotype. You must look through all these images to ensure there are no strains being grouped that have regions with significant differences (> 2%). The image below illustrates an example of this. ED3049 and ED3046 are highly similar (> 99.9%). However, they differ in a region on the right arm of chromosome II. We believe this was enough reason to consider them separate isotypes.","title":"Output"},{"location":"pipelines/pipeline-genomes-nf/","text":"genomes-nf \u00b6 genomes-nf Pipeline overview Software requirements Usage Parameters -profile (optional) Default usage: downloading genome files from Wormbase --wb_version (optional) --projects (optional) --output (optional) Alternative usage: using manually selected genomes --genome --gff Output Notes This repo contains a nextflow pipeline that downloads, indexes, and builds annotation databases for reference genomes from wormbase. The following outputs are created: A BWA Index SNPeff annotation database CSQ annotation database Samtools faidx index A GATK Sequence dictionary file Important When adding a new WormBase version reference genome, especially for c_elegans it is essential that you use this pipeline instead of downloading and adding the files to QUEST manually. These files and this file structure are essential to many other pipelines in the lab. Pipeline overview \u00b6 >AAGACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAA AGACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC GAAATA CGCG GA ATATA AAAA TCG TCGAT GC GGGC CGATCGA TAGAT GA TATATCGC TTAAC ACTAGAGGGG CTATCGAC CGAA CT GACTA CT GCG AT AGCATCACG TGGGGGCCTATC CGAC AA TCAGCTACGAAAT AGCCC TCTATCATAA TATAT T TCT TC AGCGA GA A A T TC ATAAAT TCGGGATCTAGC A CGC AT ATATATATGC GCGAT TCTAC AG GCGGGGGA AT TA AA AAGAC CG TC AT GC AGCTGGGGGC ACG GA TA AT GA CTATATATATCGC AATGC ACTAGAG GG CTATCGAC ACG A CT GACTA CT AGCGG AT AGCATCACGATGGG GCCTATC ACG C AA TCAGCTACGAAAT ACTCC TCTATCA AA AAATATAT TCTC TC AGCGA GA AAACT TC TTCATAAATCTCGG ATCTAGC ATCG AT TATATATATATGC TTAATA FCG GA ATATA AAA TCG TCGAT GC GG ACGATCGA TAGAT GA CTATATATATCGC AACACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAAT CTACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC parameters description Set/Default ========== =========== ======================== --wb_version wormbase version to build WS276 --projects comma-delimited list of `species/project_id` c_elegans/PRJNA13758,c_briggsae/PRJNA10731,c_tropicalis/PRJNA53597 --output Path of output folder /projects/b1059/data/ Software requirements \u00b6 Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env If running pipeline on Quest, you must first load singularity to access the docker container: module load singularity If running locally, Docker must be installed. For further instructions, check out our docker guide Usage \u00b6 The pipeline can be run locally or on Quest. For example: nextflow run main.nf -resume -profile local --wb_version=WS276 --projects=c_elegans/PRJNA13758 Parameters \u00b6 -profile (optional) \u00b6 Can be set to local or quest . The pipeline uses the andersenlab/genomes docker image built from env/genome.Dockerfile . The image is automatically built using github actions. See .github/workflows/build.yml for details. Note The default profile is set to -profile=quest Default usage: downloading genome files from Wormbase \u00b6 This is how the pipeline is mostly run, especially for C. elegans . --wb_version (optional) \u00b6 The wormbase version to build. For example, WS279 . Default is WS276 . --projects (optional) \u00b6 A comma-delimited list of species/project_id identifiers. A table below lists the current projects that can be downloaded. This table is regenerated as the first step of the pipeline, and stored as a file called project_species.tsv in the params.output folder ( ./genomes if working locally). By default, the pipeline will generate reference genome indices and annotations for: c_elegans/PRJNA13758 - N2 based reference genome c_briggsae/PRJNA10731 c_tropicalis/PRJNA53597 The current set of available species/projects that can be built are: species project b_xylophilus PRJEA64437 c_briggsae PRJNA10731 c_angaria PRJNA51225 a_ceylanicum PRJNA231479 a_suum PRJNA62057 a_suum PRJNA80881 b_malayi PRJNA10729 c_brenneri PRJNA20035 c_elegans PRJEB28388 c_elegans PRJNA13758 c_elegans PRJNA275000 c_latens PRJNA248912 c_remanei PRJNA248909 c_remanei PRJNA248911 c_remanei PRJNA53967 c_inopinata PRJDB5687 c_japonica PRJNA12591 c_sp11 PRJNA53597 c_sp5 PRJNA194557 c_nigoni PRJNA384657 c_sinica PRJNA194557 c_tropicalis PRJNA53597 d_immitis PRJEB1797 h_bacteriophora PRJNA13977 l_loa PRJNA60051 m_hapla PRJNA29083 m_incognita PRJEA28837 h_contortus PRJEB506 h_contortus PRJNA205202 n_americanus PRJNA72135 p_exspectatus PRJEB6009 o_tipulae PRJEB15512 p_redivivus PRJNA186477 s_ratti PRJEA62033 s_ratti PRJEB125 o_volvulus PRJEB513 p_pacificus PRJNA12644 t_muris PRJEB126 t_spiralis PRJNA12603 t_suis PRJNA208415 t_suis PRJNA208416 --output (optional) \u00b6 Path of output folder with results. Default is /projects/b1059/data/{species}/genomes/{projectID}/{WSbuild}/ Alternative usage: using manually selected genomes \u00b6 This step is mostly for making the snpEff database and making sure that the gff is in the proper format for BCSQ annotation when you have a manually curated genome/gff file. This is common for C. briggsae and C. tropicalis and might start to be used if we want to annotate C. elegans wild isolates like CB4856. --genome \u00b6 Path to manually curated genome (for genomes not downloaded from wormbase) --gff \u00b6 Path to manually curated gff generated using the above genome (for genomes not downloaded from wormbase) Output \u00b6 Outputs are nested under params.output with the following structure: c_elegans (species) \u2514\u2500\u2500 genomes \u2514\u2500\u2500 PRJNA13758 (project) \u2514\u2500\u2500 WS276 (build) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.dict (dict file) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz (fasta) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.amb (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.ann (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.bwt (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.fai (samtools faidx index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.gzi (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.pac (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.sa (bwa index) \u251c\u2500\u2500 csq \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz (CSQ annotation GFF3) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Length.tsv (protein lengths) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Scores.tsv (blosum and grantham scores) \u251c\u2500\u2500 lcr \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz (low complexity regions) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz (low complexity regions) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz.tbi (tabix index) \u2514\u2500\u2500 snpeff \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276 (tabix index) \u2502 \u251c\u2500\u2500 genes.gtf.gz (Reference GTF) \u2502 \u251c\u2500\u2500 sequences.fa (fasta genome (unzipped)) \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin (snpEff annotation db) \u2514\u2500\u2500 snpEff.config (snpEff configuration file) Notes \u00b6 The SNPeff databases are not collected together in one location as is often the case. Instead, they are stored individually with their own configuration files. The GFF3 files for some species are not as developed as C. elegans . As a consequence, the biotype is inferred from the Attributes column of the GFF. See bin/format_csq.R for more details. Warning The updated csq-formated gff script needs to be updated for other species besides C. elegans (if running the default mode)","title":"genomes-nf"},{"location":"pipelines/pipeline-genomes-nf/#genomes-nf","text":"genomes-nf Pipeline overview Software requirements Usage Parameters -profile (optional) Default usage: downloading genome files from Wormbase --wb_version (optional) --projects (optional) --output (optional) Alternative usage: using manually selected genomes --genome --gff Output Notes This repo contains a nextflow pipeline that downloads, indexes, and builds annotation databases for reference genomes from wormbase. The following outputs are created: A BWA Index SNPeff annotation database CSQ annotation database Samtools faidx index A GATK Sequence dictionary file Important When adding a new WormBase version reference genome, especially for c_elegans it is essential that you use this pipeline instead of downloading and adding the files to QUEST manually. These files and this file structure are essential to many other pipelines in the lab.","title":"genomes-nf"},{"location":"pipelines/pipeline-genomes-nf/#pipeline_overview","text":">AAGACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAA AGACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC GAAATA CGCG GA ATATA AAAA TCG TCGAT GC GGGC CGATCGA TAGAT GA TATATCGC TTAAC ACTAGAGGGG CTATCGAC CGAA CT GACTA CT GCG AT AGCATCACG TGGGGGCCTATC CGAC AA TCAGCTACGAAAT AGCCC TCTATCATAA TATAT T TCT TC AGCGA GA A A T TC ATAAAT TCGGGATCTAGC A CGC AT ATATATATGC GCGAT TCTAC AG GCGGGGGA AT TA AA AAGAC CG TC AT GC AGCTGGGGGC ACG GA TA AT GA CTATATATATCGC AATGC ACTAGAG GG CTATCGAC ACG A CT GACTA CT AGCGG AT AGCATCACGATGGG GCCTATC ACG C AA TCAGCTACGAAAT ACTCC TCTATCA AA AAATATAT TCTC TC AGCGA GA AAACT TC TTCATAAATCTCGG ATCTAGC ATCG AT TATATATATATGC TTAATA FCG GA ATATA AAA TCG TCGAT GC GG ACGATCGA TAGAT GA CTATATATATCGC AACACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAAT CTACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC parameters description Set/Default ========== =========== ======================== --wb_version wormbase version to build WS276 --projects comma-delimited list of `species/project_id` c_elegans/PRJNA13758,c_briggsae/PRJNA10731,c_tropicalis/PRJNA53597 --output Path of output folder /projects/b1059/data/","title":"Pipeline overview"},{"location":"pipelines/pipeline-genomes-nf/#software_requirements","text":"Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env If running pipeline on Quest, you must first load singularity to access the docker container: module load singularity If running locally, Docker must be installed. For further instructions, check out our docker guide","title":"Software requirements"},{"location":"pipelines/pipeline-genomes-nf/#usage","text":"The pipeline can be run locally or on Quest. For example: nextflow run main.nf -resume -profile local --wb_version=WS276 --projects=c_elegans/PRJNA13758","title":"Usage"},{"location":"pipelines/pipeline-genomes-nf/#parameters","text":"","title":"Parameters"},{"location":"pipelines/pipeline-genomes-nf/#-profile_optional","text":"Can be set to local or quest . The pipeline uses the andersenlab/genomes docker image built from env/genome.Dockerfile . The image is automatically built using github actions. See .github/workflows/build.yml for details. Note The default profile is set to -profile=quest","title":"-profile (optional)"},{"location":"pipelines/pipeline-genomes-nf/#default_usage_downloading_genome_files_from_wormbase","text":"This is how the pipeline is mostly run, especially for C. elegans .","title":"Default usage: downloading genome files from Wormbase"},{"location":"pipelines/pipeline-genomes-nf/#--wb_version_optional","text":"The wormbase version to build. For example, WS279 . Default is WS276 .","title":"--wb_version (optional)"},{"location":"pipelines/pipeline-genomes-nf/#--projects_optional","text":"A comma-delimited list of species/project_id identifiers. A table below lists the current projects that can be downloaded. This table is regenerated as the first step of the pipeline, and stored as a file called project_species.tsv in the params.output folder ( ./genomes if working locally). By default, the pipeline will generate reference genome indices and annotations for: c_elegans/PRJNA13758 - N2 based reference genome c_briggsae/PRJNA10731 c_tropicalis/PRJNA53597 The current set of available species/projects that can be built are: species project b_xylophilus PRJEA64437 c_briggsae PRJNA10731 c_angaria PRJNA51225 a_ceylanicum PRJNA231479 a_suum PRJNA62057 a_suum PRJNA80881 b_malayi PRJNA10729 c_brenneri PRJNA20035 c_elegans PRJEB28388 c_elegans PRJNA13758 c_elegans PRJNA275000 c_latens PRJNA248912 c_remanei PRJNA248909 c_remanei PRJNA248911 c_remanei PRJNA53967 c_inopinata PRJDB5687 c_japonica PRJNA12591 c_sp11 PRJNA53597 c_sp5 PRJNA194557 c_nigoni PRJNA384657 c_sinica PRJNA194557 c_tropicalis PRJNA53597 d_immitis PRJEB1797 h_bacteriophora PRJNA13977 l_loa PRJNA60051 m_hapla PRJNA29083 m_incognita PRJEA28837 h_contortus PRJEB506 h_contortus PRJNA205202 n_americanus PRJNA72135 p_exspectatus PRJEB6009 o_tipulae PRJEB15512 p_redivivus PRJNA186477 s_ratti PRJEA62033 s_ratti PRJEB125 o_volvulus PRJEB513 p_pacificus PRJNA12644 t_muris PRJEB126 t_spiralis PRJNA12603 t_suis PRJNA208415 t_suis PRJNA208416","title":"--projects (optional)"},{"location":"pipelines/pipeline-genomes-nf/#--output_optional","text":"Path of output folder with results. Default is /projects/b1059/data/{species}/genomes/{projectID}/{WSbuild}/","title":"--output (optional)"},{"location":"pipelines/pipeline-genomes-nf/#alternative_usage_using_manually_selected_genomes","text":"This step is mostly for making the snpEff database and making sure that the gff is in the proper format for BCSQ annotation when you have a manually curated genome/gff file. This is common for C. briggsae and C. tropicalis and might start to be used if we want to annotate C. elegans wild isolates like CB4856.","title":"Alternative usage: using manually selected genomes"},{"location":"pipelines/pipeline-genomes-nf/#--genome","text":"Path to manually curated genome (for genomes not downloaded from wormbase)","title":"--genome"},{"location":"pipelines/pipeline-genomes-nf/#--gff","text":"Path to manually curated gff generated using the above genome (for genomes not downloaded from wormbase)","title":"--gff"},{"location":"pipelines/pipeline-genomes-nf/#output","text":"Outputs are nested under params.output with the following structure: c_elegans (species) \u2514\u2500\u2500 genomes \u2514\u2500\u2500 PRJNA13758 (project) \u2514\u2500\u2500 WS276 (build) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.dict (dict file) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz (fasta) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.amb (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.ann (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.bwt (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.fai (samtools faidx index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.gzi (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.pac (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.sa (bwa index) \u251c\u2500\u2500 csq \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz (CSQ annotation GFF3) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Length.tsv (protein lengths) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Scores.tsv (blosum and grantham scores) \u251c\u2500\u2500 lcr \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz (low complexity regions) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz (low complexity regions) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz.tbi (tabix index) \u2514\u2500\u2500 snpeff \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276 (tabix index) \u2502 \u251c\u2500\u2500 genes.gtf.gz (Reference GTF) \u2502 \u251c\u2500\u2500 sequences.fa (fasta genome (unzipped)) \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin (snpEff annotation db) \u2514\u2500\u2500 snpEff.config (snpEff configuration file)","title":"Output"},{"location":"pipelines/pipeline-genomes-nf/#notes","text":"The SNPeff databases are not collected together in one location as is often the case. Instead, they are stored individually with their own configuration files. The GFF3 files for some species are not as developed as C. elegans . As a consequence, the biotype is inferred from the Attributes column of the GFF. See bin/format_csq.R for more details. Warning The updated csq-formated gff script needs to be updated for other species besides C. elegans (if running the default mode)","title":"Notes"},{"location":"pipelines/pipeline-impute/","text":"VCF Imputation \u00b6 VCF Imputation Pipeline overview Software Requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --debug --species --vcf --chrI|chrII|chrIII|chrIV|chrV|chrX|MtDNA (optional) --out (optional) Output The impute-nf pipeline subsets isotype reference strains from a hard-filter vcf file, creates a SNV-only VCF, and imputes a new VCF. This step is required for fine-mapping with NemaScan. This page details how to run the pipeline. Pipeline overview \u00b6 ########## # ## ## ##### # # ## # # ## # ## ## ### # # # ## # ## #### ## # # # # # # # # # # ### # # # ## # # # # # # # # ### # # # ## # # # # # # # # # # # # ########## # # # ### ### # ### # # # # # # parameters description Set/Default ========== =========== ======================== --debug Set to 'true' to test (optional) --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' (required) --vcf hard filtered vcf to calculate variant density (required) --out output folder name (optional) --chrI | chrII | chrIII... Window and overlap for each chromosome (optional) Software Requirements \u00b6 The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env Relevant Docker Images \u00b6 Note: Before 20220301, this pipeline was run using existing conda environments on QUEST and a script. However, these have since been migrated to docker images to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always find the script from an old commit of the dry guide prior to 20210827 andersenlab/beagle ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/beagle.Dockerfile or .github/workflows/build_beagle_docker.yml GitHub actions will create a new docker image and push if successful Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name> Usage \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. Testing on Rockfish \u00b6 This command uses a test dataset nextflow run -latest andersenlab/impute-nf --debug Running on Rockfish \u00b6 You should run this in a screen or tmux session. nextflow run -latest andersenlab/impute-nf --vcf <path_to_vcf> --species <species> Parameters \u00b6 -profile \u00b6 There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default --debug \u00b6 You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run -latest andersenlab/impute-nf --debug --species \u00b6 Options: c_elegans, c_briggsae, or c_tropicalis --vcf \u00b6 Path to the hard-filtered vcf output from wi-gatk . VCF should contain ALL strains. --chrI|chrII|chrIII|chrIV|chrV|chrX|MtDNA (optional) \u00b6 The window size and overlap to use as inputs to Beagle. These parameters have been checked and decided on by previous lab members and Erik. Some chromosomes might require a window size of 3 and an overlap of 1. In recent conversation with the person who manages Beagle, they mentioned we should probably use default values unless we have done simulations to show these values are better. Note for the future maybe. --out (optional) \u00b6 default - impute-YYYYMMDD A directory in which to output results. If you have set --debug , the default output directory will be impute-YYYYMMDD-debug . Output \u00b6 \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.20240718.hard-filter.isotype.SNV.vcf.gz \u251c\u2500\u2500 WI.20240718.hard-filter.isotype.SNV.vcf.gz.tbi \u251c\u2500\u2500 WI.20240718.impute.isotype.SNV.vcf.gz \u2514\u2500\u2500 WI.20240718.impute.isotype.SNV.vcf.gz.tbi","title":"imputation"},{"location":"pipelines/pipeline-impute/#vcf_imputation","text":"VCF Imputation Pipeline overview Software Requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --debug --species --vcf --chrI|chrII|chrIII|chrIV|chrV|chrX|MtDNA (optional) --out (optional) Output The impute-nf pipeline subsets isotype reference strains from a hard-filter vcf file, creates a SNV-only VCF, and imputes a new VCF. This step is required for fine-mapping with NemaScan. This page details how to run the pipeline.","title":"VCF Imputation"},{"location":"pipelines/pipeline-impute/#pipeline_overview","text":"########## # ## ## ##### # # ## # # ## # ## ## ### # # # ## # ## #### ## # # # # # # # # # # ### # # # ## # # # # # # # # ### # # # ## # # # # # # # # # # # # ########## # # # ### ### # ### # # # # # # parameters description Set/Default ========== =========== ======================== --debug Set to 'true' to test (optional) --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' (required) --vcf hard filtered vcf to calculate variant density (required) --out output folder name (optional) --chrI | chrII | chrIII... Window and overlap for each chromosome (optional)","title":"Pipeline overview"},{"location":"pipelines/pipeline-impute/#software_requirements","text":"The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env","title":"Software Requirements"},{"location":"pipelines/pipeline-impute/#relevant_docker_images","text":"Note: Before 20220301, this pipeline was run using existing conda environments on QUEST and a script. However, these have since been migrated to docker images to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always find the script from an old commit of the dry guide prior to 20210827 andersenlab/beagle ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/beagle.Dockerfile or .github/workflows/build_beagle_docker.yml GitHub actions will create a new docker image and push if successful Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name>","title":"Relevant Docker Images"},{"location":"pipelines/pipeline-impute/#usage","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page.","title":"Usage"},{"location":"pipelines/pipeline-impute/#testing_on_rockfish","text":"This command uses a test dataset nextflow run -latest andersenlab/impute-nf --debug","title":"Testing on Rockfish"},{"location":"pipelines/pipeline-impute/#running_on_rockfish","text":"You should run this in a screen or tmux session. nextflow run -latest andersenlab/impute-nf --vcf <path_to_vcf> --species <species>","title":"Running on Rockfish"},{"location":"pipelines/pipeline-impute/#parameters","text":"","title":"Parameters"},{"location":"pipelines/pipeline-impute/#-profile","text":"There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default","title":"-profile"},{"location":"pipelines/pipeline-impute/#--debug","text":"You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run -latest andersenlab/impute-nf --debug","title":"--debug"},{"location":"pipelines/pipeline-impute/#--species","text":"Options: c_elegans, c_briggsae, or c_tropicalis","title":"--species"},{"location":"pipelines/pipeline-impute/#--vcf","text":"Path to the hard-filtered vcf output from wi-gatk . VCF should contain ALL strains.","title":"--vcf"},{"location":"pipelines/pipeline-impute/#--chrichriichriiichrivchrvchrxmtdna_optional","text":"The window size and overlap to use as inputs to Beagle. These parameters have been checked and decided on by previous lab members and Erik. Some chromosomes might require a window size of 3 and an overlap of 1. In recent conversation with the person who manages Beagle, they mentioned we should probably use default values unless we have done simulations to show these values are better. Note for the future maybe.","title":"--chrI|chrII|chrIII|chrIV|chrV|chrX|MtDNA (optional)"},{"location":"pipelines/pipeline-impute/#--out_optional","text":"default - impute-YYYYMMDD A directory in which to output results. If you have set --debug , the default output directory will be impute-YYYYMMDD-debug .","title":"--out (optional)"},{"location":"pipelines/pipeline-impute/#output","text":"\u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.20240718.hard-filter.isotype.SNV.vcf.gz \u251c\u2500\u2500 WI.20240718.hard-filter.isotype.SNV.vcf.gz.tbi \u251c\u2500\u2500 WI.20240718.impute.isotype.SNV.vcf.gz \u2514\u2500\u2500 WI.20240718.impute.isotype.SNV.vcf.gz.tbi","title":"Output"},{"location":"pipelines/pipeline-nemascan/","text":"NemaScan \u00b6 GWA Mapping and Simulation with C. elegans, C. tropicalis, and C. briggsae Pipeline overview \u00b6 O~~~ O~~ O~ O~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~~ O~~ O~~ O~~ O~~ O~~~ O~~ O~~~ O~~ O~~ O~~ O~~ O~ O~ O~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~ O~~ O~~~~~O~ O~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~O~~ O~ O~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~~~ O~~ O~ O~~ O~~ O~~~ O~ O~~ O~~~ O~~ O~~~ O~~ O~~ parameters description Set/Default ========== =========== ======================== --traitfile Name of file containing strain and phenotype (required) --vcf Generally a CaeNDR release date or path to vcf (optional - 20231213) --download_vcf Fetch VCF files from CaeNDR (optional - false) --species c_elegans, c_briggsae, or c_tropicalis (optional - c_elegans) --mapping Run GWAS mapping (optional - true) --matrix Create genotype matrix (optional - false) --simulation Run GWAS mapping (optional - false) --out Name of folder that will contain the results (optional - Analysis_Results-{date}) Optional arguments (Mapping): --pca Use PCA as a covariate for mapping (optional - false) --finemap Perform fine-mapping (optional - true) --mediation Run mediation analysis (optional - false) --fix Filter and prune trait values (optional - true) Optional arguments (Mapping & Matrix): --strains File with set of strains to use (optional - strain_file.tsv) Optional arguments (Simulation): --simulate_nqtl Number of QTL to simulate per phenotype (optional - simulate_nqtl.csv) --simulate_h2 File with phenotype heritability (optional - simulate_h2.csv) --simulate_reps Number of replicates to simulate (optional - 2) --simulate_maf File of minor allele frequency thresholds (optional - simulate_maf.csv) --simulate_eff File of effect size range (e.g. 0.2-0.3) (optional - simulate_effect_sizes.csv) --simulate_strains File of strain names and strain lists (optional - simulate_strains.csv) --simulate_qtlloc File with genomic range where markers are pulled from (optional - whole genome) Optional arguments (Mapping & Simulation): --sthresh Significance threshold for QTL (optional - 'BF') --group_qtl QTL distance to combine the QTL into one (optional - 1000) --ci_size Number of SNVs used to define the QTL CI (optional - 150) --maf Minimum minor allele frequency to use (optional - 0.05) --sparse_cut Off-diagonal relatedness matrix value cutoff (optional - 0.05) Software Requirements \u00b6 The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env Relevant Docker Images \u00b6 andersenlab/nemascan ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/nemascan.Dockerfile or .github/workflows/build_nemascan_docker.yml GitHub actions will create a new docker image and push if successful. andersenlab/mediation ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/mediation.Dockerfile or .github/workflows/build_med_docker.yml GitHub actions will create a new docker image and push if successful. andersenlab/gcta ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/gcta.Dockerfile or .github/workflows/build_gcta_docker.yml GitHub actions will create a new docker image and push if successful. andersenlab/r_packages ( link ): Docker image is created manually, code can be found in the dockerfile repo. andersenlab/prep_sims ( link ): Docker image is created manually. andersenlab/assess_sims ( link ): Docker image is created manually. Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name> Usage \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. Testing on Rockfish \u00b6 This command uses a test dataset nextflow run -latest andersenlab/nemascan --debug Running on Rockfish \u00b6 You should run this in a screen or tmux session. nextflow run -latest andersenlab/nemascan -profile mappings --vcf 20210121 --traitfile input_data/c_elegans/phenotypes/PC1.tsv Parameters \u00b6 -profile \u00b6 There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default --debug \u00b6 You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run -latest andersenlab/nemascan --debug --traitfile \u00b6 A tab-delimited formatted (.tsv) file that contains trait information. Each phenotype file should be in the following format (replace trait_name with the phenotype of interest): strain trait_name_1 trait_name_2 JU258 32.73 19.34 ECA640 34.065378 12.32 ... ... ... ECA250 34.096 23.1 --vcf (default: 20231213) \u00b6 CaeNDR release date for the VCF file with variant data (i.e. \"20210121\") Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. If this flag is not used, the most recent VCF for the C. elegans species will be downloaded from CaeNDR . Note If you want to use a custom VCF, you may provide the full path to the vcf in place of the CaeNDR release date. This custom VCF will be used for BOTH GWA mapping and fine-mapping steps (instead of the imputed vcf). --download_vcf (default: false) \u00b6 Fetch VCF files from CaeNDR site rather than using local built-in file paths. --species (default: c_elegans) \u00b6 Choose between c_elegans , c_tropicalis or c_briggsae --mapping (default: true) \u00b6 Indicates to perform a genome-wide analysis of your trait of interest. --matrix (default: false) \u00b6 Indicates to run the matrix production step. This takes a list of strains and outputs the genotype matrix. --simulation (default: false) \u00b6 Indicates to run QTL simulations. This mode uses simulations to establish GWA performance benchmarks. Users can specify the heritability of simulated traits, the number of QTL underlying simulated traits of interest, the strains the user intends to use in a prospective GWA mapping experiment, or the location of previously detected QTL. Understanding the null expectations of GWA mappings within given parameter spaces may provide experimenters with additional guidance before initiating an experiment, or serve as a validation tool for previous mappings. --out (default: Analysis_Results-{date}) \u00b6 A user-specified output directory name. (Default: Analysis_Results-{date} ) Optional Mapping Parameters \u00b6 --pca \u00b6 Use 1st principle component as a covariate for mapping. (Default: false) --finemap \u00b6 Perform fine-mapping of QTL intervals. (Default: true) --mediation \u00b6 'true' or 'false' input for whether or not to run the mediation analysis (overlapping expression variation with phenotypic variation to drive a QTL). (Default: false) --fix \u00b6 Filter trait values by combining strain replicates and pruning extreme values. (Default: true) Optional Mapping & Matrix Parameter \u00b6 --strains \u00b6 A file (.tsv) that contains a list of strains used for generating the genotype matrix. There is no header: JU258 ECA640 ... ECA250 (Default: input_data/${params.species}/phenotypes/strain_file.tsv ) Optional Simulation Parameters \u00b6 --simulate_nqtl \u00b6 A single column CSV file that defines the number of QTL to simulate (format: one number per line, no column header) (Default is provided: input_data/all_species/simulate_nqtl.csv ). --simulate_h2 \u00b6 A CSV file with phenotype heritability. (format: one value per line, no column header) (Default is located: input_data/all_species/simulate_h2.csv ). --simulate_reps \u00b6 The number of replicates to simulate per number of QTL and heritability (Default: 2). --simulate_maf \u00b6 A single column CSV file that defines the minor allele frequency threshold used to filter the VCF prior to simulations (Default: 0.05). --simulate_eff \u00b6 A CSV file specifying a range of causal QTL effects. QTL effects will be drawn from a uniform distribution bound by these two values. If the user wants to specify Gamma distributed effects, the value in this file can be simply specified as \"gamma\". (format: one value per line, no column header) (Default is located: input_data/all_species/simulate_effect_sizes.csv). --simulate_strains \u00b6 A TSV file specifying the population in which to simulate GWA mappings. Multiple populations can be simulated at once, but causal QTL will be drawn independently for each population as a result of minor allele frequency and LD pruning prior to mapping. (format: one line per population; supplied population name and a comma-separated list of each strain in the population) (Default is located: input_data/all_species/simulate_strains.tsv). --simulate_qtlloc \u00b6 A .bed file specifying genomic regions from which causal QTL are to be drawn after MAF filtering and LD pruning. (format: CHROM START END for each genomic region, with no header. NOTE: CHROM is specified as NUMERIC, not roman numerals as is convention in C. elegans ) (Default is located: input_data/all_species/simulate_locations.bed). Optional Mapping and Simulation Parameters \u00b6 --sthresh \u00b6 This determines the signficance threshold required for performing post-mapping analysis of a QTL. BF corresponds to Bonferroni correction, EIGEN corresponds to correcting for the number of independent markers in your data set, and user-specified corresponds to a user-defined threshold, where you replace user-specified with a number. For example --sthresh=4 will set the threshold to a -log10(p) value of 4. We recommend using the strict BF correction as a first pass to see what the resulting data looks like. If the pipeline stops at the summarize_maps process, no significant QTL were discovered with the input threshold. You might want to consider lowering the threshold if this occurs. (Default: BF ) --group_qtl \u00b6 QTL within this number of markers from each other will be grouped as a single QTL by Find_GCTA_Intervals_*.R . (Default: 1000) --ci_size \u00b6 The number of markers for which the detection interval will be extended past the last significant marker in the interval. (Default: 150) --maf \u00b6 The minor allele frequency for filtering variants to use for gwas mapping (default 0.05) nextflow run -latest andersenlab/nemascan --matrix --vcf 20210121 --strains input_data/elegans/phenotypes/strain_file.tsv -sparse_cut \u00b6 Any off-diagonal value in the genetic relatedness matrix greater than this is set to 0 (Default: 0.05) Input Data Folder Structure ( NemaScan/input_data ) \u00b6 all_species \u251c\u2500\u2500 rename_chromosomes \u251c\u2500\u2500 simulate_effect_sizes.csv \u251c\u2500\u2500 simulate_h2.csv \u251c\u2500\u2500 simulate_maf.csv \u251c\u2500\u2500 simulate_nqtl.csv \u251c\u2500\u2500 simulate_strains.tsv \u2514\u2500\u2500 simulate_locations.bed c_elegans (repeated for c_tropicalis and c_briggsae) \u2514\u2500\u2500 genotypes \u251c\u2500\u2500 test_vcf \u251c\u2500\u2500 test_vcf_index \u2514\u2500\u2500 test_bcsq_annotation \u2514\u2500\u2500 phenotypes \u251c\u2500\u2500 PC1.tsv \u251c\u2500\u2500 strain_file.tsv \u2514\u2500\u2500 test_pheno.tsv \u2514\u2500\u2500 annotations \u251c\u2500\u2500 GTF file \u2514\u2500\u2500 refFlat file \u2514\u2500\u2500 isotypes \u251c\u2500\u2500 div_isotype_list.txt \u251c\u2500\u2500 divergent_bins.bed \u251c\u2500\u2500 divergent_df_isotype.bed \u251c\u2500\u2500 haplotype_df_isotype.bed \u2514\u2500\u2500 strain_isotype_lookup.tsv Mapping Output Folder Structure \u00b6 Phenotypes \u251c\u2500\u2500 strain_issues.txt \u2514\u2500\u2500 pr_traitname.tsv Genotype_Matrix \u251c\u2500\u2500 Genotype_Matrix.tsv \u2514\u2500\u2500 total_independent_tests.txt Mapping \u2514\u2500\u2500 Raw \u251c\u2500\u2500 traitname_lmm-exact_inbred.fastGWA \u2514\u2500\u2500 traitname_lmm-exact.loco.mlma \u2514\u2500\u2500 Processed \u251c\u2500\u2500 traitname_AGGREGATE_qtl_region.tsv \u251c\u2500\u2500 processed_traitname_AGGREGATE_mapping.tsv \u2514\u2500\u2500 QTL_peaks.tsv Plots \u2514\u2500\u2500 ManhattanPlots \u2514\u2500\u2500 traitname_manhattan.plot.png \u2514\u2500\u2500 LDPlots \u2514\u2500\u2500 traitname_LD.plot.png (if > 1 QTL detected) \u2514\u2500\u2500 EffectPlots \u251c\u2500\u2500 traitname_[QTL.INFO]_LOCO_effect.plot.png (if detected) \u2514\u2500\u2500 traitname_[QTL.INFO]_INBRED_effect.plot.png (if detected) Fine_Mappings \u2514\u2500\u2500 Data \u251c\u2500\u2500 traitname_[QTL.INFO]_bcsq_genes.tsv \u251c\u2500\u2500 traitname_[QTL.INFO]_ROI_Genotype_Matrix.tsv \u251c\u2500\u2500 traitname_[QTL.INFO]_finemap_inbred.fastGWA \u2514\u2500\u2500 traitname_[QTL.INFO]_LD.tsv \u2514\u2500\u2500 Plots \u251c\u2500\u2500 traitname_[QTL.INFO]_finemap_plot.pdf \u2514\u2500\u2500 traitname_[QTL.INFO]_gene_plot_bcsq.pdf Divergent_and_haplotype \u251c\u2500\u2500 all_QTL_bins.bed \u251c\u2500\u2500 all_QTL_div.bed \u251c\u2500\u2500 div_isotype_list.txt \u2514\u2500\u2500 haplotype_in_QTL_region.txt Reports \u251c\u2500\u2500 NemaScan_Report_traitname_main.html \u2514\u2500\u2500 NemaScan_Report_traitname_main.Rmd Phenotypes folder \u00b6 strain_issues.txt - Output of any strain names that were changed to match vcf (i.e. isotypes that are not reference strains) pr_traitname.tsv - Processed phenotype file for each trait. This is the file that goes into the mapping Genotype_Matrix folder \u00b6 Genotype_Matrix.tsv - LD-pruned genotype matrix used for GWAS and construction of kinship matrix total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix Mapping folder \u00b6 Raw \u00b6 traitname_lmm-exact_inbred.fastGWA - Raw mapping results from GCTA's fastGWA program using an inbred kinship matrix traitname_lmm-exact.loco.mlma - Raw mapping results from GCTA's mlma program using a kinship matrix constructed from all chromosomes except for the chromosome containing each tested variant Processed \u00b6 traitname_AGGREGATE_mapping.tsv - Combined processed mapping results from lmm-exact_inbred and lmm-exact.loco.mlma raw mappings. Contains additional information nested such as 1) rough intervals (see parameters for calculation) and estimates of the variance explained by the detected QTL 2) phenotype information and genotype status for each strain at the detected QTL. traitname_AGGREGATE_qtl_region.tsv - Contains only QTL information for each mapping. If no QTL are detected, an empty data frame is written. QTL_peaks.tsv - contains QTL information for each mapping for all traits combined. Plots \u00b6 traitname_manhattan.plot.png - Standard output for GWA; association of marker differences with phenotypic variation in the population. traitname_LD.plot.png - If more than 1 QTL are detected for a trait, a plot showing the linkage disequilibrium between each QTL is generated. traitname_[QTL.INFO]_INBRED_effect.plot.png - Phenotypes for each strain are plotted against their marker genotype at the peak marker for each QTL detected for a trait. The dot representing each strain is shaded according to the percentage of the chromosome containing the QTL that is characterized as a selective sweep region. Fine_Mappings folder \u00b6 Data \u00b6 traitname_bcsq_genes.tsv - Fine-mapping data frame for all significant QTL Plots \u00b6 traitname_qtlinterval_finemap_plot.pdf - Fine map plot of QTL interval, colored by marker LD with the peak QTL identified from the genome-wide scan traitname_qtlinterval_gene_plot.pdf - variant annotation plot overlaid with gene CDS for QTL interval Simulation Output Folder Structure \u00b6 Genotype_Matrix \u251c\u2500\u2500 [strain_set]_[MAF]_Genotype_Matrix.tsv \u2514\u2500\u2500 [strain_set]_[MAF]_total_independent_tests.txt Simulations \u251c\u2500\u2500 NemaScan_Performance.example_simulation_output.RData \u2514\u2500\u2500 [specified effect range (simulate_effect_sizes.csv)] \u2514\u2500\u2500 [specified number of simulated QTL (simulate_nqtl.csv)] \u2514\u2500\u2500 Mappings \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_processed_LMM_EXACT_INBRED_mapping.tsv \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_processed_LMM_EXACT_LOCO_mapping.tsv \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_lmm-exact_inbred.fastGWA \u2514\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_lmm-exact.loco.mlma \u2514\u2500\u2500 Phenotypes \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.phen \u2514\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.par \u2514\u2500\u2500 (if applicable) [NEXT specified effect range] \u2514\u2500\u2500 ... \u2514\u2500\u2500 (if applicable) [NEXT specified effect range] \u2514\u2500\u2500 ... Genotype_Matrix folder \u00b6 *Genotype_Matrix.tsv - pruned LD-pruned genotype matrix used for GWAS and construction of kinship matrix. This will be appended with the chosen minor allele frequency cutoff and strain set, as they are generated separately for each strain set. *total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix. This will be also be appended with the chosen minor allele frequency cutoff and strain set, as they are generated separately for each strain set. Simulations \u00b6 NemaScan_Performance.*.RData - RData file containing all simulated and detected QTL from each successful simulated mapping. Contains: 1. Simulated and Detected status for each QTL. 2. Minor allele frequency and simulated or estimated effect for each QTL. 3. Detection interval according to specified grouping size and CI extension. 4. Estimated variance explained for each detected QTL. 5. Simulation parameters and the algorithm used for that particular regime. Mappings \u00b6 As with the mapping profile, raw and processed mappings for each simulation regime are nested within folders corresponding each specified effect range and number of simulated QTL. QTL region files are not provided in the simulation profile; this information along with other information related to mapping performance are iteratively gathered in the generation of the performance .RData file. Phenotypes \u00b6 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.phen - Simulated strain phenotypes for each simulation regime. [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.par - Simulated QTL effects for each simulation regime. NOTE: Simulation regimes with identical numbers of simulated QTL, replicate indices, and simulated heritabilities should have identical simulated QTL and effects.","title":"NemaScan"},{"location":"pipelines/pipeline-nemascan/#nemascan","text":"GWA Mapping and Simulation with C. elegans, C. tropicalis, and C. briggsae","title":"NemaScan"},{"location":"pipelines/pipeline-nemascan/#pipeline_overview","text":"O~~~ O~~ O~ O~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~~ O~~ O~~ O~~ O~~ O~~~ O~~ O~~~ O~~ O~~ O~~ O~~ O~ O~ O~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~ O~~ O~~~~~O~ O~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~O~~ O~ O~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~~~ O~~ O~ O~~ O~~ O~~~ O~ O~~ O~~~ O~~ O~~~ O~~ O~~ parameters description Set/Default ========== =========== ======================== --traitfile Name of file containing strain and phenotype (required) --vcf Generally a CaeNDR release date or path to vcf (optional - 20231213) --download_vcf Fetch VCF files from CaeNDR (optional - false) --species c_elegans, c_briggsae, or c_tropicalis (optional - c_elegans) --mapping Run GWAS mapping (optional - true) --matrix Create genotype matrix (optional - false) --simulation Run GWAS mapping (optional - false) --out Name of folder that will contain the results (optional - Analysis_Results-{date}) Optional arguments (Mapping): --pca Use PCA as a covariate for mapping (optional - false) --finemap Perform fine-mapping (optional - true) --mediation Run mediation analysis (optional - false) --fix Filter and prune trait values (optional - true) Optional arguments (Mapping & Matrix): --strains File with set of strains to use (optional - strain_file.tsv) Optional arguments (Simulation): --simulate_nqtl Number of QTL to simulate per phenotype (optional - simulate_nqtl.csv) --simulate_h2 File with phenotype heritability (optional - simulate_h2.csv) --simulate_reps Number of replicates to simulate (optional - 2) --simulate_maf File of minor allele frequency thresholds (optional - simulate_maf.csv) --simulate_eff File of effect size range (e.g. 0.2-0.3) (optional - simulate_effect_sizes.csv) --simulate_strains File of strain names and strain lists (optional - simulate_strains.csv) --simulate_qtlloc File with genomic range where markers are pulled from (optional - whole genome) Optional arguments (Mapping & Simulation): --sthresh Significance threshold for QTL (optional - 'BF') --group_qtl QTL distance to combine the QTL into one (optional - 1000) --ci_size Number of SNVs used to define the QTL CI (optional - 150) --maf Minimum minor allele frequency to use (optional - 0.05) --sparse_cut Off-diagonal relatedness matrix value cutoff (optional - 0.05)","title":"Pipeline overview"},{"location":"pipelines/pipeline-nemascan/#software_requirements","text":"The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env","title":"Software Requirements"},{"location":"pipelines/pipeline-nemascan/#relevant_docker_images","text":"andersenlab/nemascan ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/nemascan.Dockerfile or .github/workflows/build_nemascan_docker.yml GitHub actions will create a new docker image and push if successful. andersenlab/mediation ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/mediation.Dockerfile or .github/workflows/build_med_docker.yml GitHub actions will create a new docker image and push if successful. andersenlab/gcta ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/gcta.Dockerfile or .github/workflows/build_gcta_docker.yml GitHub actions will create a new docker image and push if successful. andersenlab/r_packages ( link ): Docker image is created manually, code can be found in the dockerfile repo. andersenlab/prep_sims ( link ): Docker image is created manually. andersenlab/assess_sims ( link ): Docker image is created manually. Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name>","title":"Relevant Docker Images"},{"location":"pipelines/pipeline-nemascan/#usage","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page.","title":"Usage"},{"location":"pipelines/pipeline-nemascan/#testing_on_rockfish","text":"This command uses a test dataset nextflow run -latest andersenlab/nemascan --debug","title":"Testing on Rockfish"},{"location":"pipelines/pipeline-nemascan/#running_on_rockfish","text":"You should run this in a screen or tmux session. nextflow run -latest andersenlab/nemascan -profile mappings --vcf 20210121 --traitfile input_data/c_elegans/phenotypes/PC1.tsv","title":"Running on Rockfish"},{"location":"pipelines/pipeline-nemascan/#parameters","text":"","title":"Parameters"},{"location":"pipelines/pipeline-nemascan/#-profile","text":"There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default","title":"-profile"},{"location":"pipelines/pipeline-nemascan/#--debug","text":"You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run -latest andersenlab/nemascan --debug","title":"--debug"},{"location":"pipelines/pipeline-nemascan/#--traitfile","text":"A tab-delimited formatted (.tsv) file that contains trait information. Each phenotype file should be in the following format (replace trait_name with the phenotype of interest): strain trait_name_1 trait_name_2 JU258 32.73 19.34 ECA640 34.065378 12.32 ... ... ... ECA250 34.096 23.1","title":"--traitfile"},{"location":"pipelines/pipeline-nemascan/#--vcf_default_20231213","text":"CaeNDR release date for the VCF file with variant data (i.e. \"20210121\") Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. If this flag is not used, the most recent VCF for the C. elegans species will be downloaded from CaeNDR . Note If you want to use a custom VCF, you may provide the full path to the vcf in place of the CaeNDR release date. This custom VCF will be used for BOTH GWA mapping and fine-mapping steps (instead of the imputed vcf).","title":"--vcf (default: 20231213)"},{"location":"pipelines/pipeline-nemascan/#--download_vcf_default_false","text":"Fetch VCF files from CaeNDR site rather than using local built-in file paths.","title":"--download_vcf (default: false)"},{"location":"pipelines/pipeline-nemascan/#--species_default_c_elegans","text":"Choose between c_elegans , c_tropicalis or c_briggsae","title":"--species (default: c_elegans)"},{"location":"pipelines/pipeline-nemascan/#--mapping_default_true","text":"Indicates to perform a genome-wide analysis of your trait of interest.","title":"--mapping (default: true)"},{"location":"pipelines/pipeline-nemascan/#--matrix_default_false","text":"Indicates to run the matrix production step. This takes a list of strains and outputs the genotype matrix.","title":"--matrix (default: false)"},{"location":"pipelines/pipeline-nemascan/#--simulation_default_false","text":"Indicates to run QTL simulations. This mode uses simulations to establish GWA performance benchmarks. Users can specify the heritability of simulated traits, the number of QTL underlying simulated traits of interest, the strains the user intends to use in a prospective GWA mapping experiment, or the location of previously detected QTL. Understanding the null expectations of GWA mappings within given parameter spaces may provide experimenters with additional guidance before initiating an experiment, or serve as a validation tool for previous mappings.","title":"--simulation (default: false)"},{"location":"pipelines/pipeline-nemascan/#--out_default_analysis_results-date","text":"A user-specified output directory name. (Default: Analysis_Results-{date} )","title":"--out (default: Analysis_Results-{date})"},{"location":"pipelines/pipeline-nemascan/#optional_mapping_parameters","text":"","title":"Optional Mapping Parameters"},{"location":"pipelines/pipeline-nemascan/#--pca","text":"Use 1st principle component as a covariate for mapping. (Default: false)","title":"--pca"},{"location":"pipelines/pipeline-nemascan/#--finemap","text":"Perform fine-mapping of QTL intervals. (Default: true)","title":"--finemap"},{"location":"pipelines/pipeline-nemascan/#--mediation","text":"'true' or 'false' input for whether or not to run the mediation analysis (overlapping expression variation with phenotypic variation to drive a QTL). (Default: false)","title":"--mediation"},{"location":"pipelines/pipeline-nemascan/#--fix","text":"Filter trait values by combining strain replicates and pruning extreme values. (Default: true)","title":"--fix"},{"location":"pipelines/pipeline-nemascan/#optional_mapping_matrix_parameter","text":"","title":"Optional Mapping &amp; Matrix Parameter"},{"location":"pipelines/pipeline-nemascan/#--strains","text":"A file (.tsv) that contains a list of strains used for generating the genotype matrix. There is no header: JU258 ECA640 ... ECA250 (Default: input_data/${params.species}/phenotypes/strain_file.tsv )","title":"--strains"},{"location":"pipelines/pipeline-nemascan/#optional_simulation_parameters","text":"","title":"Optional Simulation Parameters"},{"location":"pipelines/pipeline-nemascan/#--simulate_nqtl","text":"A single column CSV file that defines the number of QTL to simulate (format: one number per line, no column header) (Default is provided: input_data/all_species/simulate_nqtl.csv ).","title":"--simulate_nqtl"},{"location":"pipelines/pipeline-nemascan/#--simulate_h2","text":"A CSV file with phenotype heritability. (format: one value per line, no column header) (Default is located: input_data/all_species/simulate_h2.csv ).","title":"--simulate_h2"},{"location":"pipelines/pipeline-nemascan/#--simulate_reps","text":"The number of replicates to simulate per number of QTL and heritability (Default: 2).","title":"--simulate_reps"},{"location":"pipelines/pipeline-nemascan/#--simulate_maf","text":"A single column CSV file that defines the minor allele frequency threshold used to filter the VCF prior to simulations (Default: 0.05).","title":"--simulate_maf"},{"location":"pipelines/pipeline-nemascan/#--simulate_eff","text":"A CSV file specifying a range of causal QTL effects. QTL effects will be drawn from a uniform distribution bound by these two values. If the user wants to specify Gamma distributed effects, the value in this file can be simply specified as \"gamma\". (format: one value per line, no column header) (Default is located: input_data/all_species/simulate_effect_sizes.csv).","title":"--simulate_eff"},{"location":"pipelines/pipeline-nemascan/#--simulate_strains","text":"A TSV file specifying the population in which to simulate GWA mappings. Multiple populations can be simulated at once, but causal QTL will be drawn independently for each population as a result of minor allele frequency and LD pruning prior to mapping. (format: one line per population; supplied population name and a comma-separated list of each strain in the population) (Default is located: input_data/all_species/simulate_strains.tsv).","title":"--simulate_strains"},{"location":"pipelines/pipeline-nemascan/#--simulate_qtlloc","text":"A .bed file specifying genomic regions from which causal QTL are to be drawn after MAF filtering and LD pruning. (format: CHROM START END for each genomic region, with no header. NOTE: CHROM is specified as NUMERIC, not roman numerals as is convention in C. elegans ) (Default is located: input_data/all_species/simulate_locations.bed).","title":"--simulate_qtlloc"},{"location":"pipelines/pipeline-nemascan/#optional_mapping_and_simulation_parameters","text":"","title":"Optional Mapping and Simulation Parameters"},{"location":"pipelines/pipeline-nemascan/#--sthresh","text":"This determines the signficance threshold required for performing post-mapping analysis of a QTL. BF corresponds to Bonferroni correction, EIGEN corresponds to correcting for the number of independent markers in your data set, and user-specified corresponds to a user-defined threshold, where you replace user-specified with a number. For example --sthresh=4 will set the threshold to a -log10(p) value of 4. We recommend using the strict BF correction as a first pass to see what the resulting data looks like. If the pipeline stops at the summarize_maps process, no significant QTL were discovered with the input threshold. You might want to consider lowering the threshold if this occurs. (Default: BF )","title":"--sthresh"},{"location":"pipelines/pipeline-nemascan/#--group_qtl","text":"QTL within this number of markers from each other will be grouped as a single QTL by Find_GCTA_Intervals_*.R . (Default: 1000)","title":"--group_qtl"},{"location":"pipelines/pipeline-nemascan/#--ci_size","text":"The number of markers for which the detection interval will be extended past the last significant marker in the interval. (Default: 150)","title":"--ci_size"},{"location":"pipelines/pipeline-nemascan/#--maf","text":"The minor allele frequency for filtering variants to use for gwas mapping (default 0.05) nextflow run -latest andersenlab/nemascan --matrix --vcf 20210121 --strains input_data/elegans/phenotypes/strain_file.tsv","title":"--maf"},{"location":"pipelines/pipeline-nemascan/#-sparse_cut","text":"Any off-diagonal value in the genetic relatedness matrix greater than this is set to 0 (Default: 0.05)","title":"-sparse_cut"},{"location":"pipelines/pipeline-nemascan/#input_data_folder_structure_nemascaninput_data","text":"all_species \u251c\u2500\u2500 rename_chromosomes \u251c\u2500\u2500 simulate_effect_sizes.csv \u251c\u2500\u2500 simulate_h2.csv \u251c\u2500\u2500 simulate_maf.csv \u251c\u2500\u2500 simulate_nqtl.csv \u251c\u2500\u2500 simulate_strains.tsv \u2514\u2500\u2500 simulate_locations.bed c_elegans (repeated for c_tropicalis and c_briggsae) \u2514\u2500\u2500 genotypes \u251c\u2500\u2500 test_vcf \u251c\u2500\u2500 test_vcf_index \u2514\u2500\u2500 test_bcsq_annotation \u2514\u2500\u2500 phenotypes \u251c\u2500\u2500 PC1.tsv \u251c\u2500\u2500 strain_file.tsv \u2514\u2500\u2500 test_pheno.tsv \u2514\u2500\u2500 annotations \u251c\u2500\u2500 GTF file \u2514\u2500\u2500 refFlat file \u2514\u2500\u2500 isotypes \u251c\u2500\u2500 div_isotype_list.txt \u251c\u2500\u2500 divergent_bins.bed \u251c\u2500\u2500 divergent_df_isotype.bed \u251c\u2500\u2500 haplotype_df_isotype.bed \u2514\u2500\u2500 strain_isotype_lookup.tsv","title":"Input Data Folder Structure (NemaScan/input_data)"},{"location":"pipelines/pipeline-nemascan/#mapping_output_folder_structure","text":"Phenotypes \u251c\u2500\u2500 strain_issues.txt \u2514\u2500\u2500 pr_traitname.tsv Genotype_Matrix \u251c\u2500\u2500 Genotype_Matrix.tsv \u2514\u2500\u2500 total_independent_tests.txt Mapping \u2514\u2500\u2500 Raw \u251c\u2500\u2500 traitname_lmm-exact_inbred.fastGWA \u2514\u2500\u2500 traitname_lmm-exact.loco.mlma \u2514\u2500\u2500 Processed \u251c\u2500\u2500 traitname_AGGREGATE_qtl_region.tsv \u251c\u2500\u2500 processed_traitname_AGGREGATE_mapping.tsv \u2514\u2500\u2500 QTL_peaks.tsv Plots \u2514\u2500\u2500 ManhattanPlots \u2514\u2500\u2500 traitname_manhattan.plot.png \u2514\u2500\u2500 LDPlots \u2514\u2500\u2500 traitname_LD.plot.png (if > 1 QTL detected) \u2514\u2500\u2500 EffectPlots \u251c\u2500\u2500 traitname_[QTL.INFO]_LOCO_effect.plot.png (if detected) \u2514\u2500\u2500 traitname_[QTL.INFO]_INBRED_effect.plot.png (if detected) Fine_Mappings \u2514\u2500\u2500 Data \u251c\u2500\u2500 traitname_[QTL.INFO]_bcsq_genes.tsv \u251c\u2500\u2500 traitname_[QTL.INFO]_ROI_Genotype_Matrix.tsv \u251c\u2500\u2500 traitname_[QTL.INFO]_finemap_inbred.fastGWA \u2514\u2500\u2500 traitname_[QTL.INFO]_LD.tsv \u2514\u2500\u2500 Plots \u251c\u2500\u2500 traitname_[QTL.INFO]_finemap_plot.pdf \u2514\u2500\u2500 traitname_[QTL.INFO]_gene_plot_bcsq.pdf Divergent_and_haplotype \u251c\u2500\u2500 all_QTL_bins.bed \u251c\u2500\u2500 all_QTL_div.bed \u251c\u2500\u2500 div_isotype_list.txt \u2514\u2500\u2500 haplotype_in_QTL_region.txt Reports \u251c\u2500\u2500 NemaScan_Report_traitname_main.html \u2514\u2500\u2500 NemaScan_Report_traitname_main.Rmd","title":"Mapping Output Folder Structure"},{"location":"pipelines/pipeline-nemascan/#phenotypes_folder","text":"strain_issues.txt - Output of any strain names that were changed to match vcf (i.e. isotypes that are not reference strains) pr_traitname.tsv - Processed phenotype file for each trait. This is the file that goes into the mapping","title":"Phenotypes folder"},{"location":"pipelines/pipeline-nemascan/#genotype_matrix_folder","text":"Genotype_Matrix.tsv - LD-pruned genotype matrix used for GWAS and construction of kinship matrix total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix","title":"Genotype_Matrix folder"},{"location":"pipelines/pipeline-nemascan/#mapping_folder","text":"","title":"Mapping folder"},{"location":"pipelines/pipeline-nemascan/#raw","text":"traitname_lmm-exact_inbred.fastGWA - Raw mapping results from GCTA's fastGWA program using an inbred kinship matrix traitname_lmm-exact.loco.mlma - Raw mapping results from GCTA's mlma program using a kinship matrix constructed from all chromosomes except for the chromosome containing each tested variant","title":"Raw"},{"location":"pipelines/pipeline-nemascan/#processed","text":"traitname_AGGREGATE_mapping.tsv - Combined processed mapping results from lmm-exact_inbred and lmm-exact.loco.mlma raw mappings. Contains additional information nested such as 1) rough intervals (see parameters for calculation) and estimates of the variance explained by the detected QTL 2) phenotype information and genotype status for each strain at the detected QTL. traitname_AGGREGATE_qtl_region.tsv - Contains only QTL information for each mapping. If no QTL are detected, an empty data frame is written. QTL_peaks.tsv - contains QTL information for each mapping for all traits combined.","title":"Processed"},{"location":"pipelines/pipeline-nemascan/#plots","text":"traitname_manhattan.plot.png - Standard output for GWA; association of marker differences with phenotypic variation in the population. traitname_LD.plot.png - If more than 1 QTL are detected for a trait, a plot showing the linkage disequilibrium between each QTL is generated. traitname_[QTL.INFO]_INBRED_effect.plot.png - Phenotypes for each strain are plotted against their marker genotype at the peak marker for each QTL detected for a trait. The dot representing each strain is shaded according to the percentage of the chromosome containing the QTL that is characterized as a selective sweep region.","title":"Plots"},{"location":"pipelines/pipeline-nemascan/#fine_mappings_folder","text":"","title":"Fine_Mappings folder"},{"location":"pipelines/pipeline-nemascan/#data","text":"traitname_bcsq_genes.tsv - Fine-mapping data frame for all significant QTL","title":"Data"},{"location":"pipelines/pipeline-nemascan/#plots_1","text":"traitname_qtlinterval_finemap_plot.pdf - Fine map plot of QTL interval, colored by marker LD with the peak QTL identified from the genome-wide scan traitname_qtlinterval_gene_plot.pdf - variant annotation plot overlaid with gene CDS for QTL interval","title":"Plots"},{"location":"pipelines/pipeline-nemascan/#simulation_output_folder_structure","text":"Genotype_Matrix \u251c\u2500\u2500 [strain_set]_[MAF]_Genotype_Matrix.tsv \u2514\u2500\u2500 [strain_set]_[MAF]_total_independent_tests.txt Simulations \u251c\u2500\u2500 NemaScan_Performance.example_simulation_output.RData \u2514\u2500\u2500 [specified effect range (simulate_effect_sizes.csv)] \u2514\u2500\u2500 [specified number of simulated QTL (simulate_nqtl.csv)] \u2514\u2500\u2500 Mappings \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_processed_LMM_EXACT_INBRED_mapping.tsv \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_processed_LMM_EXACT_LOCO_mapping.tsv \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_lmm-exact_inbred.fastGWA \u2514\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_lmm-exact.loco.mlma \u2514\u2500\u2500 Phenotypes \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.phen \u2514\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.par \u2514\u2500\u2500 (if applicable) [NEXT specified effect range] \u2514\u2500\u2500 ... \u2514\u2500\u2500 (if applicable) [NEXT specified effect range] \u2514\u2500\u2500 ...","title":"Simulation Output Folder Structure"},{"location":"pipelines/pipeline-nemascan/#genotype_matrix_folder_1","text":"*Genotype_Matrix.tsv - pruned LD-pruned genotype matrix used for GWAS and construction of kinship matrix. This will be appended with the chosen minor allele frequency cutoff and strain set, as they are generated separately for each strain set. *total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix. This will be also be appended with the chosen minor allele frequency cutoff and strain set, as they are generated separately for each strain set.","title":"Genotype_Matrix folder"},{"location":"pipelines/pipeline-nemascan/#simulations","text":"NemaScan_Performance.*.RData - RData file containing all simulated and detected QTL from each successful simulated mapping. Contains: 1. Simulated and Detected status for each QTL. 2. Minor allele frequency and simulated or estimated effect for each QTL. 3. Detection interval according to specified grouping size and CI extension. 4. Estimated variance explained for each detected QTL. 5. Simulation parameters and the algorithm used for that particular regime.","title":"Simulations"},{"location":"pipelines/pipeline-nemascan/#mappings","text":"As with the mapping profile, raw and processed mappings for each simulation regime are nested within folders corresponding each specified effect range and number of simulated QTL. QTL region files are not provided in the simulation profile; this information along with other information related to mapping performance are iteratively gathered in the generation of the performance .RData file.","title":"Mappings"},{"location":"pipelines/pipeline-nemascan/#phenotypes","text":"[nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.phen - Simulated strain phenotypes for each simulation regime. [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.par - Simulated QTL effects for each simulation regime. NOTE: Simulation regimes with identical numbers of simulated QTL, replicate indices, and simulated heritabilities should have identical simulated QTL and effects.","title":"Phenotypes"},{"location":"pipelines/pipeline-nil-ril/","text":"nil-ril-nf \u00b6 nil-ril-nf Overview Docker image Usage Requirements Profiles and Running the Pipeline Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Testing Parameters --fqs --vcf --reference --A, --B (optional) --debug (optional) --cores (optional) --cA, --cB (optional) --out (optional) --relative (optional) --cross_obj (optional) --tmpdir (optional) Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ Organizing final data Adding NIL sequence data to lab website The nil-ril-nf pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data. Overview \u00b6 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --relative use relative fastq prefix true --reference Reference Genome (required) --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default). Alignment - Performed using bwa-mem Merge Bams - Combines bam files aligned individually for each fastq-pair. Sambamba is actually used in place of samtools, but it's a drop-in, faster replacement. Bam Stats - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis. Mark Duplicates - Duplicate reads are marked using Picard. Call Variants individual - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population. Pull parental genotypes - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF. Call variants union - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls. Merge VCF - Merges in the parental VCF (which has been filtered only for variants with discordant calls). Call HMM - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data. Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible. Docker image \u00b6 The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image: andersenlab/nil-ril-nf The Dockerfile is stored in the root of the nil-ril-nf github repo and is automatically built on Dockerhub whenever the repo is pushed. Usage \u00b6 Requirements \u00b6 The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Profiles and Running the Pipeline \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. The nextflow.config file included with this pipeline contains four profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. travis - Used by travis-ci for testing purposes. Running the pipeline locally \u00b6 When running locally, the pipeline will run using the andersenlab/nil-ril-nf docker image. You must have docker installed. You will need to obtain a reference genome to run the alignment with as well. You can use the following command to obtain the reference: curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz Run the pipeline locally with: nextflow run andersenlab/nil-ril-nf -profile local -resume Debugging the pipeline on Quest \u00b6 If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run andersenlab/nil-ril-nf -profile quest_debug -resume Note There is no need to clone the git repo before running the pipeline. However, you may still choose to do so if you plan to manually track the git commit used to generate data. Running the pipeline on Quest \u00b6 If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity The pipeline can be run on Quest using the following command: nextflow run andersenlab/nil-ril-nf -profile quest -resume Testing \u00b6 If you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the -resume option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly. Additionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci here , and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail. The command below can be used to test the pipeline locally. # Downloads a pre-indexed reference curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz # Run nextflow nextflow run andersenlab/nil-ril-nf \\ -with-docker andersenlab/nil-ril-nf \\ --debug \\ --reference=WS276.fa.gz \\ -resume Note that the path to the vcf will change slightly in releases later than WI-20170531; See the wi-gatk pipeline for details. The command above will automatically place results in a folder: NIL-N2-CB4856-YYYY-MM-DD Parameters \u00b6 --fqs \u00b6 In order to process NIL/RIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq can be specified as relative or absolute . By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not include the parental strains in the fq_sheet. If you re-sequenced the parent strains and want to include them in the analysis as a control, you need to rename the parent strains to avoid an error in merging the VCFs (i.e. N2 becomes N2-1). --vcf \u00b6 Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, the most recent C. elegans VCF could be found here: /projects/b1059/data/c_elegans/WI/variation/20210121/vcf/WI.20210121.hard-filter.isotype.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20210121.hard-filter.isotype.vcf.gz --reference \u00b6 A fasta reference indexed with BWA. For example, the C. elegans reference could be found here: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz --A, --B (optional) \u00b6 Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details. --debug (optional) \u00b6 The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the Testing section for more information. --cores (optional) \u00b6 The number of cores to use during alignments and variant calling. Default is 4. --cA, --cB (optional) \u00b6 The color to use for parental strain A and B on plots. Default is orange and blue. --out (optional) \u00b6 A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains. --relative (optional) \u00b6 If you want to specify fastqs using an absolute path use --relative=false . Set to true by default. --cross_obj (optional) \u00b6 If you are running a set of RILs, you might want to add the --cross_obj true parameter. When true , the pipeline will run an additional step to pair down the total genetic variants to only the informative variants and output a smaller genotype matrix to input directly into a new cross object. An example of how to generate a cross object can be found in the bin . There is no need to run this option for NIL data. --tmpdir (optional) \u00b6 A directory for storing temporary data. Output \u00b6 The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u251c\u2500\u2500 SM_union_vcfs.txt \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u251c\u2500\u2500 gt_hmm.tsv \u2502 \u251c\u2500\u2500 gt_hmm_fill.tsv \u2502 \u251c\u2500\u2500 NIL.filtered.stats.txt \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u2502 \u2514\u2500\u2500 gt_hmm_genotypes.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u2514\u2500 sitelist \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi] log.txt \u00b6 A summary of the nextflow run. duplicates/ \u00b6 bam_duplicates.tsv - A summary of duplicate reads from aligned bams. fq/ \u00b6 fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq SM/ \u00b6 If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. SM_union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz hmm/ \u00b6 Important gt_hmm_fill.tsv is for visualization purposes only. To determine breakpoints you should use gt_hmm.tsv . The --infill and --endfill options are applied to the gt_hmm_fill.tsv file. You need to be cautious when examining this data as it is generated primarily for visualization purposes . gt_hmm.(png/svg) - Haplotype plot using --infill and --endfill . gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation . This file is used to generate the plots. gt_hmm.tsv - Haplotypes defined by region with associated information. Does not use --infill and --endfill gt_hmm_genotypes.tsv - Long form genotypes file. NIL/RIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL/RIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL/RIL.hmm.vcf.gz - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes. plots/ \u00b6 coverage_comparison.png - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/pdf) - Coverage vs. percent duplicated. unmapped_reads.png - Coverage vs. unmapped read percent. sitelist/ \u00b6 <A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. <A>.<B>.sitelist.vcf.gz[+.tbi] - A vcf of sites found to be different between both parental strains. Organizing final data \u00b6 After the run is complete and you are satisfied with the results, follow these steps to ensure correct data storage on QUEST: Move the raw fastq files to /projects/b1059/data/{species}/{NIL or RIL}/fastq/ . You might want to use mv -i to ensure no files are overwritten. Move the BAM files from the output folder to /projects/b1059/data/{species}/{NIL or RIL}/alignments/ . You might want to use mv -i to ensure no files are overwritten. Delete the now empty bam folder in the output directory. Move the sample sheet generated for analysis into the output directory. Make sure the output directory follows the default naming structure that is informative about the analysis (i.e. NIL-20200322-N2-CB4856 (if NIL/RIL analysis is performed for another lab, consider adding a -{LabName} like -Baugh to the end of the folder name)). Move the entire output folder to /projects/b1059/data/{species}/{NIL or RIL}/variation . Adding NIL sequence data to lab website \u00b6 If your sequencing was N2-CB4856 NILs (and maybe other C. elegans NILs as well...?) you probably want to add this sequencing data to the lab website to be accessed by everyone when looking for NIL genotypes. Check out this page for instructions on how to do that. Once done, you should be able to view your NILs on the NIL browser shiny app .","title":"nil-ril-nf"},{"location":"pipelines/pipeline-nil-ril/#nil-ril-nf","text":"nil-ril-nf Overview Docker image Usage Requirements Profiles and Running the Pipeline Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Testing Parameters --fqs --vcf --reference --A, --B (optional) --debug (optional) --cores (optional) --cA, --cB (optional) --out (optional) --relative (optional) --cross_obj (optional) --tmpdir (optional) Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ Organizing final data Adding NIL sequence data to lab website The nil-ril-nf pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data.","title":"nil-ril-nf"},{"location":"pipelines/pipeline-nil-ril/#overview","text":"\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --relative use relative fastq prefix true --reference Reference Genome (required) --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default). Alignment - Performed using bwa-mem Merge Bams - Combines bam files aligned individually for each fastq-pair. Sambamba is actually used in place of samtools, but it's a drop-in, faster replacement. Bam Stats - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis. Mark Duplicates - Duplicate reads are marked using Picard. Call Variants individual - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population. Pull parental genotypes - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF. Call variants union - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls. Merge VCF - Merges in the parental VCF (which has been filtered only for variants with discordant calls). Call HMM - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data. Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible.","title":"Overview"},{"location":"pipelines/pipeline-nil-ril/#docker_image","text":"The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image: andersenlab/nil-ril-nf The Dockerfile is stored in the root of the nil-ril-nf github repo and is automatically built on Dockerhub whenever the repo is pushed.","title":"Docker image"},{"location":"pipelines/pipeline-nil-ril/#usage","text":"","title":"Usage"},{"location":"pipelines/pipeline-nil-ril/#requirements","text":"The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env","title":"Requirements"},{"location":"pipelines/pipeline-nil-ril/#profiles_and_running_the_pipeline","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. The nextflow.config file included with this pipeline contains four profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. travis - Used by travis-ci for testing purposes.","title":"Profiles and Running the Pipeline"},{"location":"pipelines/pipeline-nil-ril/#running_the_pipeline_locally","text":"When running locally, the pipeline will run using the andersenlab/nil-ril-nf docker image. You must have docker installed. You will need to obtain a reference genome to run the alignment with as well. You can use the following command to obtain the reference: curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz Run the pipeline locally with: nextflow run andersenlab/nil-ril-nf -profile local -resume","title":"Running the pipeline locally"},{"location":"pipelines/pipeline-nil-ril/#debugging_the_pipeline_on_quest","text":"If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run andersenlab/nil-ril-nf -profile quest_debug -resume Note There is no need to clone the git repo before running the pipeline. However, you may still choose to do so if you plan to manually track the git commit used to generate data.","title":"Debugging the pipeline on Quest"},{"location":"pipelines/pipeline-nil-ril/#running_the_pipeline_on_quest","text":"If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity The pipeline can be run on Quest using the following command: nextflow run andersenlab/nil-ril-nf -profile quest -resume","title":"Running the pipeline on Quest"},{"location":"pipelines/pipeline-nil-ril/#testing","text":"If you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the -resume option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly. Additionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci here , and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail. The command below can be used to test the pipeline locally. # Downloads a pre-indexed reference curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz # Run nextflow nextflow run andersenlab/nil-ril-nf \\ -with-docker andersenlab/nil-ril-nf \\ --debug \\ --reference=WS276.fa.gz \\ -resume Note that the path to the vcf will change slightly in releases later than WI-20170531; See the wi-gatk pipeline for details. The command above will automatically place results in a folder: NIL-N2-CB4856-YYYY-MM-DD","title":"Testing"},{"location":"pipelines/pipeline-nil-ril/#parameters","text":"","title":"Parameters"},{"location":"pipelines/pipeline-nil-ril/#--fqs","text":"In order to process NIL/RIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq can be specified as relative or absolute . By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not include the parental strains in the fq_sheet. If you re-sequenced the parent strains and want to include them in the analysis as a control, you need to rename the parent strains to avoid an error in merging the VCFs (i.e. N2 becomes N2-1).","title":"--fqs"},{"location":"pipelines/pipeline-nil-ril/#--vcf","text":"Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, the most recent C. elegans VCF could be found here: /projects/b1059/data/c_elegans/WI/variation/20210121/vcf/WI.20210121.hard-filter.isotype.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20210121.hard-filter.isotype.vcf.gz","title":"--vcf"},{"location":"pipelines/pipeline-nil-ril/#--reference","text":"A fasta reference indexed with BWA. For example, the C. elegans reference could be found here: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz","title":"--reference"},{"location":"pipelines/pipeline-nil-ril/#--a_--b_optional","text":"Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details.","title":"--A, --B (optional)"},{"location":"pipelines/pipeline-nil-ril/#--debug_optional","text":"The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the Testing section for more information.","title":"--debug (optional)"},{"location":"pipelines/pipeline-nil-ril/#--cores_optional","text":"The number of cores to use during alignments and variant calling. Default is 4.","title":"--cores (optional)"},{"location":"pipelines/pipeline-nil-ril/#--ca_--cb_optional","text":"The color to use for parental strain A and B on plots. Default is orange and blue.","title":"--cA, --cB (optional)"},{"location":"pipelines/pipeline-nil-ril/#--out_optional","text":"A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains.","title":"--out (optional)"},{"location":"pipelines/pipeline-nil-ril/#--relative_optional","text":"If you want to specify fastqs using an absolute path use --relative=false . Set to true by default.","title":"--relative (optional)"},{"location":"pipelines/pipeline-nil-ril/#--cross_obj_optional","text":"If you are running a set of RILs, you might want to add the --cross_obj true parameter. When true , the pipeline will run an additional step to pair down the total genetic variants to only the informative variants and output a smaller genotype matrix to input directly into a new cross object. An example of how to generate a cross object can be found in the bin . There is no need to run this option for NIL data.","title":"--cross_obj (optional)"},{"location":"pipelines/pipeline-nil-ril/#--tmpdir_optional","text":"A directory for storing temporary data.","title":"--tmpdir (optional)"},{"location":"pipelines/pipeline-nil-ril/#output","text":"The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u251c\u2500\u2500 SM_union_vcfs.txt \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u251c\u2500\u2500 gt_hmm.tsv \u2502 \u251c\u2500\u2500 gt_hmm_fill.tsv \u2502 \u251c\u2500\u2500 NIL.filtered.stats.txt \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u2502 \u2514\u2500\u2500 gt_hmm_genotypes.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u2514\u2500 sitelist \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi]","title":"Output"},{"location":"pipelines/pipeline-nil-ril/#logtxt","text":"A summary of the nextflow run.","title":"log.txt"},{"location":"pipelines/pipeline-nil-ril/#duplicates","text":"bam_duplicates.tsv - A summary of duplicate reads from aligned bams.","title":"duplicates/"},{"location":"pipelines/pipeline-nil-ril/#fq","text":"fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq","title":"fq/"},{"location":"pipelines/pipeline-nil-ril/#sm","text":"If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. SM_union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz","title":"SM/"},{"location":"pipelines/pipeline-nil-ril/#hmm","text":"Important gt_hmm_fill.tsv is for visualization purposes only. To determine breakpoints you should use gt_hmm.tsv . The --infill and --endfill options are applied to the gt_hmm_fill.tsv file. You need to be cautious when examining this data as it is generated primarily for visualization purposes . gt_hmm.(png/svg) - Haplotype plot using --infill and --endfill . gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation . This file is used to generate the plots. gt_hmm.tsv - Haplotypes defined by region with associated information. Does not use --infill and --endfill gt_hmm_genotypes.tsv - Long form genotypes file. NIL/RIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL/RIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL/RIL.hmm.vcf.gz - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes.","title":"hmm/"},{"location":"pipelines/pipeline-nil-ril/#plots","text":"coverage_comparison.png - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/pdf) - Coverage vs. percent duplicated. unmapped_reads.png - Coverage vs. unmapped read percent.","title":"plots/"},{"location":"pipelines/pipeline-nil-ril/#sitelist","text":"<A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. <A>.<B>.sitelist.vcf.gz[+.tbi] - A vcf of sites found to be different between both parental strains.","title":"sitelist/"},{"location":"pipelines/pipeline-nil-ril/#organizing_final_data","text":"After the run is complete and you are satisfied with the results, follow these steps to ensure correct data storage on QUEST: Move the raw fastq files to /projects/b1059/data/{species}/{NIL or RIL}/fastq/ . You might want to use mv -i to ensure no files are overwritten. Move the BAM files from the output folder to /projects/b1059/data/{species}/{NIL or RIL}/alignments/ . You might want to use mv -i to ensure no files are overwritten. Delete the now empty bam folder in the output directory. Move the sample sheet generated for analysis into the output directory. Make sure the output directory follows the default naming structure that is informative about the analysis (i.e. NIL-20200322-N2-CB4856 (if NIL/RIL analysis is performed for another lab, consider adding a -{LabName} like -Baugh to the end of the folder name)). Move the entire output folder to /projects/b1059/data/{species}/{NIL or RIL}/variation .","title":"Organizing final data"},{"location":"pipelines/pipeline-nil-ril/#adding_nil_sequence_data_to_lab_website","text":"If your sequencing was N2-CB4856 NILs (and maybe other C. elegans NILs as well...?) you probably want to add this sequencing data to the lab website to be accessed by everyone when looking for NIL genotypes. Check out this page for instructions on how to do that. Once done, you should be able to view your NILs on the NIL browser shiny app .","title":"Adding NIL sequence data to lab website"},{"location":"pipelines/pipeline-overview/","text":"Pipeline Overview \u00b6 Pipeline Overview Wild isolate sequencing An overview of the sequencing pipelines is shown below. Wild isolate data are processed by multiple pipelines. NIL/RIL sequence data are only processed by one pipeline. Wild isolate sequencing \u00b6 Note The full protocol (in development) can be found here","title":"Overview"},{"location":"pipelines/pipeline-overview/#pipeline_overview","text":"Pipeline Overview Wild isolate sequencing An overview of the sequencing pipelines is shown below. Wild isolate data are processed by multiple pipelines. NIL/RIL sequence data are only processed by one pipeline.","title":"Pipeline Overview"},{"location":"pipelines/pipeline-overview/#wild_isolate_sequencing","text":"Note The full protocol (in development) can be found here","title":"Wild isolate sequencing"},{"location":"pipelines/pipeline-postGATK/","text":"post-gatk-nf \u00b6 post-gatk-nf Pipeline overview Software Requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --debug Debugging for PCA: --sample_sheet --vcf_folder --species (optional) PCA --postgatk (default: true) --pca (default: false) --snv_vcf (pca) --pops (pca) --eigen_ld (pca) --anc (pca) --output (optional) Output Data storage Cleanup Updating CaeNDR Updating NemaScan The post-gatk-nf pipeline performs population genetics analyses (such as identifying shared haplotypes and divergent regions) at the isotype level. The VCFs output from this pipeline are used within the lab and also released to the world via CaeNDR. This page details how to run the pipeline. Pipeline overview \u00b6 * * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * ** *** * * * * * * * * *** * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode (optional) --vcf_folder Folder to hard and soft filtered vcf (required) --sample_sheet TSV with column iso-ref strain, bam, bai (no header) (required) --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' c_elegans --output Output folder name. popgen-date (in current folder) --postgatk Run post-GATK steps true --pca Run PCA analysis false Software Requirements \u00b6 The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env Relevant Docker Images \u00b6 Note: Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker imgaes to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/post-gatk-nf -r 20220216-Release andersenlab/postgatk ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/postgatk.Dockerfile or .github/workflows/build_postgatk_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/tree ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/tree.Dockerfile or .github/workflows/build_tree_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/pca ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/pca.Dockerfile or .github/workflows/build_pca_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/r_packages ( link ): Docker image is created manually, code can be found in the dockerfile repo. Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name> Usage \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. Testing on Rockfish \u00b6 This command uses a test dataset nextflow run -latest andersenlab/post-gatk-nf --debug Running on Rockfish \u00b6 You should run this in a screen or tmux session. nextflow run -latest andersenlab/post-gatk-nf --vcf <path_to_vcf> --sample_sheet <path_to_sample_sheet> Parameters \u00b6 -profile \u00b6 There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default --debug \u00b6 You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run -latest andersenlab/post-gatk-nf --debug Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv Debugging for PCA: \u00b6 You can debug the PCA pipeline with the following data/command: nextflow run -latest andersenlab/post-gatk-nf --vcf ./test_data/WI.20220404.hard-filter.vcf.gz --species c_elegans --sample_sheet ./test_data/sample_sheet_2.tsv --eigen_ld 0.8,0.6 --anc XZ2019 --pca -resume --sample_sheet \u00b6 A custom sample sheet can be specified using --sample_sheet . The sample sheet is generated from the sample sheet used as input for wi-gatk with only columns for strain, bam, and bai subsetted. Make sure to remove any strains that you do not want to include in this analysis. ( i.e. subset to keep only ISOTYPE strains ) Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . Important There is no header for the sample sheet! The sample sheet has the following columns: strain - the name of the strain bam - name of the bam alignment file bai - name of the bam alignment index file Note As of 20210501, bam and bam.bai files for all strains of a particular species can be found in one singular location: /vast/eande106/data/{species}/WI/alignments/ so there is no longer need to provide the location of the bam files. --vcf_folder \u00b6 Path to the folder containing both the hard-filtered and soft-filtered vcf outputs from wi-gatk . VCF should contain ALL strains, the first step will be to subset isotype reference strains for further analysis. Note This should be the path to the folder , we want to isotype-subset both hard and soft filtered VCFs. For example: --vcf_folder /vast/eande106/projects/Katie/wi-gatk/WI-20210121/variation/ or --vcf_folder /vast/eande106/data/c_elegans/WI/variation/20210121/vcf/ --species (optional) \u00b6 default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis PCA \u00b6 The PCA steps can be run either with the full pipeline or independently. To run only PCA use -- pca true --postgatk false The input VCF is filtered to bi-alleleic snps with no missing genotypes. A LD filtering threshold is required and LD filtering is performed using plink. You can also filter for singletons by specifying the --singletons PCA is performed using smartPCA. Parameters to control outlier threshold or removal iterations are desribed below. --postgatk (default: true) \u00b6 Set to false to skip post-gatk steps --pca (default: false) \u00b6 Set to true to run PCA analysis --snv_vcf (pca) \u00b6 File path to SNV-filtered VCF --pops (pca) \u00b6 Strain list to filter VCF for PCA analysis. No header: AB1 CB4856 ECA788 Note If you run the standard profile with pca this file will be automatically generated to include all isotypes. --eigen_ld (pca) \u00b6 LD thresholds to test for PCA. Can provide multiple with --eigen_ld 0.8,0.6,0.4 --anc (pca) \u00b6 Ancestor strain to use for PCA. Note Make sure this strain is in your VCF --output (optional) \u00b6 default - popgen-YYYYMMDD A directory in which to output results. If you have set --debug , the default output directory will be popgen-YYYYMMDD-debug . Output \u00b6 \u251c\u2500\u2500 ANNOTATE_VCF \u2502 \u251c\u2500\u2500 ANC.bed.gz \u2502 \u251c\u2500\u2500 ANC.bed.gz.tbi \u2502 \u251c\u2500\u2500 Ce330_annotated.vcf.gz | \u2514\u2500\u2500 Ce330_annotated.vcf.tbi \u251c\u2500\u2500 EIGESTRAT \u2502 \u2514\u2500\u2500 LD_{eigen_ld} \u2502 \u251c\u2500\u2500 INPUT_FILES \u2502 \u2502 \u2514\u2500\u2500 * \u2502 \u251c\u2500\u2500 OUTLIER_REMOVAL \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed_relatedness \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed_relatedness.id \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed.evac \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed.eval \u2502 \u2502 \u251c\u2500\u2500 logfile_outlier.txt \u2502 \u2502 \u2514\u2500\u2500 TracyWidom_statistics_outlier_removal.tsv \u2502 \u2514\u2500\u2500 NO_REMOVAL \u2502 \u2514\u2500\u2500 same as outlier_removal \u251c\u2500\u2500 pca_report.html \u251c\u2500\u2500 divergent_regions \u2502 \u251c\u2500\u2500 Mask_DF \u2502 \u2502 \u2514\u2500\u2500 [strain]_Mask_DF.tsv | \u2514\u2500\u2500 divergent_regions_strain.bed \u251c\u2500\u2500 haplotype \u2502 \u251c\u2500\u2500 haplotype_length.pdf \u2502 \u251c\u2500\u2500 sweep_summary.tsv \u2502 \u251c\u2500\u2500 max_haplotype_genome_wide.pdf \u2502 \u251c\u2500\u2500 haplotype.pdf \u2502 \u251c\u2500\u2500 haplotype.tsv \u2502 \u251c\u2500\u2500 [chr].ibd \u2502 \u2514\u2500\u2500 haplotype_plot_df.Rda \u251c\u2500\u2500 tree \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.min4.tree \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.min4.tree.pdf \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.min4.tree \u2502 \u2514\u2500\u2500 WI.{date}.hard-filter.min4.tree.pdf \u251c\u2500\u2500 NemaScan \u2502 \u251c\u2500\u2500 strain_isotype_lookup.tsv \u2502 \u251c\u2500\u2500 div_isotype_list.txt \u2502 \u251c\u2500\u2500 haplotype_df_isotype.bed \u2502 \u251c\u2500\u2500 divergent_bins.bed \u2502 \u2514\u2500\u2500 divergent_df_isotype.bed \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.{date}.small.hard-filter.isotype.vcf.gz \u251c\u2500\u2500 WI.{date}.small.hard-filter.isotype.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.SNV.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.SNV.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.soft-filter.isotype.vcf.gz \u251c\u2500\u2500 WI.{date}.soft-filter.isotype.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz \u2514\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz.tbi Data storage \u00b6 Cleanup \u00b6 Once the pipeline has complete successfully and you are satisfied with the results, the final data can be moved to their final storage place on Quest accordingly: Everything in the haplotype foder can be moved to /vast/eande106/data/{species}/WI/haplotype/{date} Everything in the divergent_regions folder can be moved to /vast/eande106/data/{species}/WI/divergent_regions/{date} Everything in the tree folder can be moved to /vast/eande106/data/{species}/WI/tree/{date} Everything in the variation folder can be moved to /vast/eande106/data/{species}/WI/variation/{date}/vcf Everything in the NemaScan folder can replace the old verions in NemaScan when ready NemaScan/input_data/{species}/isotypes Everything in the EIGENSTRAT folder can be moved to /vast/eande106/data/{species}/WI/pca/{date} Updating CaeNDR \u00b6 Check out the CaeNDR page for more information about updating a new data release for CaeNDR. Updating NemaScan \u00b6 Once a new CaeNDR release is ready, it is important to also update the genome-wide association mapping packages to ensure users can appropriately analyze data from new strains as well as old strains. Here is a list of things that need to be updated: The default vcf should be changed to the newest release date (i.e. from 20200815 to 20210121). Users will still have the option to use an earlier vcf. Everything in the NemaScan folder can replace the old verions in NemaScan when ready NemaScan/input_data/{species}/isotypes Be sure that the small-vcf is stored in the proper file location to be accessed, both on Rockfish and on GCP for CaeNDR/local users. Note Although users will have the option to use an older vcf, the divergent region data will always be pulled from the most recent release. There could be minor changes from release to release. If this is a concern, we could switch to pulling the divergent data directly from /vast/eande106/data instead of including it in the bin/ of the mapping pipeline.","title":"post-gatk-nf"},{"location":"pipelines/pipeline-postGATK/#post-gatk-nf","text":"post-gatk-nf Pipeline overview Software Requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --debug Debugging for PCA: --sample_sheet --vcf_folder --species (optional) PCA --postgatk (default: true) --pca (default: false) --snv_vcf (pca) --pops (pca) --eigen_ld (pca) --anc (pca) --output (optional) Output Data storage Cleanup Updating CaeNDR Updating NemaScan The post-gatk-nf pipeline performs population genetics analyses (such as identifying shared haplotypes and divergent regions) at the isotype level. The VCFs output from this pipeline are used within the lab and also released to the world via CaeNDR. This page details how to run the pipeline.","title":"post-gatk-nf"},{"location":"pipelines/pipeline-postGATK/#pipeline_overview","text":"* * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * ** *** * * * * * * * * *** * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode (optional) --vcf_folder Folder to hard and soft filtered vcf (required) --sample_sheet TSV with column iso-ref strain, bam, bai (no header) (required) --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' c_elegans --output Output folder name. popgen-date (in current folder) --postgatk Run post-GATK steps true --pca Run PCA analysis false","title":"Pipeline overview"},{"location":"pipelines/pipeline-postGATK/#software_requirements","text":"The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env","title":"Software Requirements"},{"location":"pipelines/pipeline-postGATK/#relevant_docker_images","text":"Note: Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker imgaes to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/post-gatk-nf -r 20220216-Release andersenlab/postgatk ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/postgatk.Dockerfile or .github/workflows/build_postgatk_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/tree ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/tree.Dockerfile or .github/workflows/build_tree_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/pca ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/pca.Dockerfile or .github/workflows/build_pca_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/r_packages ( link ): Docker image is created manually, code can be found in the dockerfile repo. Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name>","title":"Relevant Docker Images"},{"location":"pipelines/pipeline-postGATK/#usage","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page.","title":"Usage"},{"location":"pipelines/pipeline-postGATK/#testing_on_rockfish","text":"This command uses a test dataset nextflow run -latest andersenlab/post-gatk-nf --debug","title":"Testing on Rockfish"},{"location":"pipelines/pipeline-postGATK/#running_on_rockfish","text":"You should run this in a screen or tmux session. nextflow run -latest andersenlab/post-gatk-nf --vcf <path_to_vcf> --sample_sheet <path_to_sample_sheet>","title":"Running on Rockfish"},{"location":"pipelines/pipeline-postGATK/#parameters","text":"","title":"Parameters"},{"location":"pipelines/pipeline-postGATK/#-profile","text":"There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default","title":"-profile"},{"location":"pipelines/pipeline-postGATK/#--debug","text":"You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run -latest andersenlab/post-gatk-nf --debug Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv","title":"--debug"},{"location":"pipelines/pipeline-postGATK/#debugging_for_pca","text":"You can debug the PCA pipeline with the following data/command: nextflow run -latest andersenlab/post-gatk-nf --vcf ./test_data/WI.20220404.hard-filter.vcf.gz --species c_elegans --sample_sheet ./test_data/sample_sheet_2.tsv --eigen_ld 0.8,0.6 --anc XZ2019 --pca -resume","title":"Debugging for PCA:"},{"location":"pipelines/pipeline-postGATK/#--sample_sheet","text":"A custom sample sheet can be specified using --sample_sheet . The sample sheet is generated from the sample sheet used as input for wi-gatk with only columns for strain, bam, and bai subsetted. Make sure to remove any strains that you do not want to include in this analysis. ( i.e. subset to keep only ISOTYPE strains ) Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . Important There is no header for the sample sheet! The sample sheet has the following columns: strain - the name of the strain bam - name of the bam alignment file bai - name of the bam alignment index file Note As of 20210501, bam and bam.bai files for all strains of a particular species can be found in one singular location: /vast/eande106/data/{species}/WI/alignments/ so there is no longer need to provide the location of the bam files.","title":"--sample_sheet"},{"location":"pipelines/pipeline-postGATK/#--vcf_folder","text":"Path to the folder containing both the hard-filtered and soft-filtered vcf outputs from wi-gatk . VCF should contain ALL strains, the first step will be to subset isotype reference strains for further analysis. Note This should be the path to the folder , we want to isotype-subset both hard and soft filtered VCFs. For example: --vcf_folder /vast/eande106/projects/Katie/wi-gatk/WI-20210121/variation/ or --vcf_folder /vast/eande106/data/c_elegans/WI/variation/20210121/vcf/","title":"--vcf_folder"},{"location":"pipelines/pipeline-postGATK/#--species_optional","text":"default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis","title":"--species (optional)"},{"location":"pipelines/pipeline-postGATK/#pca","text":"The PCA steps can be run either with the full pipeline or independently. To run only PCA use -- pca true --postgatk false The input VCF is filtered to bi-alleleic snps with no missing genotypes. A LD filtering threshold is required and LD filtering is performed using plink. You can also filter for singletons by specifying the --singletons PCA is performed using smartPCA. Parameters to control outlier threshold or removal iterations are desribed below.","title":"PCA"},{"location":"pipelines/pipeline-postGATK/#--postgatk_default_true","text":"Set to false to skip post-gatk steps","title":"--postgatk (default: true)"},{"location":"pipelines/pipeline-postGATK/#--pca_default_false","text":"Set to true to run PCA analysis","title":"--pca (default: false)"},{"location":"pipelines/pipeline-postGATK/#--snv_vcf_pca","text":"File path to SNV-filtered VCF","title":"--snv_vcf (pca)"},{"location":"pipelines/pipeline-postGATK/#--pops_pca","text":"Strain list to filter VCF for PCA analysis. No header: AB1 CB4856 ECA788 Note If you run the standard profile with pca this file will be automatically generated to include all isotypes.","title":"--pops (pca)"},{"location":"pipelines/pipeline-postGATK/#--eigen_ld_pca","text":"LD thresholds to test for PCA. Can provide multiple with --eigen_ld 0.8,0.6,0.4","title":"--eigen_ld (pca)"},{"location":"pipelines/pipeline-postGATK/#--anc_pca","text":"Ancestor strain to use for PCA. Note Make sure this strain is in your VCF","title":"--anc (pca)"},{"location":"pipelines/pipeline-postGATK/#--output_optional","text":"default - popgen-YYYYMMDD A directory in which to output results. If you have set --debug , the default output directory will be popgen-YYYYMMDD-debug .","title":"--output (optional)"},{"location":"pipelines/pipeline-postGATK/#output","text":"\u251c\u2500\u2500 ANNOTATE_VCF \u2502 \u251c\u2500\u2500 ANC.bed.gz \u2502 \u251c\u2500\u2500 ANC.bed.gz.tbi \u2502 \u251c\u2500\u2500 Ce330_annotated.vcf.gz | \u2514\u2500\u2500 Ce330_annotated.vcf.tbi \u251c\u2500\u2500 EIGESTRAT \u2502 \u2514\u2500\u2500 LD_{eigen_ld} \u2502 \u251c\u2500\u2500 INPUT_FILES \u2502 \u2502 \u2514\u2500\u2500 * \u2502 \u251c\u2500\u2500 OUTLIER_REMOVAL \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed_relatedness \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed_relatedness.id \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed.evac \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed.eval \u2502 \u2502 \u251c\u2500\u2500 logfile_outlier.txt \u2502 \u2502 \u2514\u2500\u2500 TracyWidom_statistics_outlier_removal.tsv \u2502 \u2514\u2500\u2500 NO_REMOVAL \u2502 \u2514\u2500\u2500 same as outlier_removal \u251c\u2500\u2500 pca_report.html \u251c\u2500\u2500 divergent_regions \u2502 \u251c\u2500\u2500 Mask_DF \u2502 \u2502 \u2514\u2500\u2500 [strain]_Mask_DF.tsv | \u2514\u2500\u2500 divergent_regions_strain.bed \u251c\u2500\u2500 haplotype \u2502 \u251c\u2500\u2500 haplotype_length.pdf \u2502 \u251c\u2500\u2500 sweep_summary.tsv \u2502 \u251c\u2500\u2500 max_haplotype_genome_wide.pdf \u2502 \u251c\u2500\u2500 haplotype.pdf \u2502 \u251c\u2500\u2500 haplotype.tsv \u2502 \u251c\u2500\u2500 [chr].ibd \u2502 \u2514\u2500\u2500 haplotype_plot_df.Rda \u251c\u2500\u2500 tree \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.min4.tree \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.min4.tree.pdf \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.min4.tree \u2502 \u2514\u2500\u2500 WI.{date}.hard-filter.min4.tree.pdf \u251c\u2500\u2500 NemaScan \u2502 \u251c\u2500\u2500 strain_isotype_lookup.tsv \u2502 \u251c\u2500\u2500 div_isotype_list.txt \u2502 \u251c\u2500\u2500 haplotype_df_isotype.bed \u2502 \u251c\u2500\u2500 divergent_bins.bed \u2502 \u2514\u2500\u2500 divergent_df_isotype.bed \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.{date}.small.hard-filter.isotype.vcf.gz \u251c\u2500\u2500 WI.{date}.small.hard-filter.isotype.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.SNV.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.SNV.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.soft-filter.isotype.vcf.gz \u251c\u2500\u2500 WI.{date}.soft-filter.isotype.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz \u2514\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz.tbi","title":"Output"},{"location":"pipelines/pipeline-postGATK/#data_storage","text":"","title":"Data storage"},{"location":"pipelines/pipeline-postGATK/#cleanup","text":"Once the pipeline has complete successfully and you are satisfied with the results, the final data can be moved to their final storage place on Quest accordingly: Everything in the haplotype foder can be moved to /vast/eande106/data/{species}/WI/haplotype/{date} Everything in the divergent_regions folder can be moved to /vast/eande106/data/{species}/WI/divergent_regions/{date} Everything in the tree folder can be moved to /vast/eande106/data/{species}/WI/tree/{date} Everything in the variation folder can be moved to /vast/eande106/data/{species}/WI/variation/{date}/vcf Everything in the NemaScan folder can replace the old verions in NemaScan when ready NemaScan/input_data/{species}/isotypes Everything in the EIGENSTRAT folder can be moved to /vast/eande106/data/{species}/WI/pca/{date}","title":"Cleanup"},{"location":"pipelines/pipeline-postGATK/#updating_caendr","text":"Check out the CaeNDR page for more information about updating a new data release for CaeNDR.","title":"Updating CaeNDR"},{"location":"pipelines/pipeline-postGATK/#updating_nemascan","text":"Once a new CaeNDR release is ready, it is important to also update the genome-wide association mapping packages to ensure users can appropriately analyze data from new strains as well as old strains. Here is a list of things that need to be updated: The default vcf should be changed to the newest release date (i.e. from 20200815 to 20210121). Users will still have the option to use an earlier vcf. Everything in the NemaScan folder can replace the old verions in NemaScan when ready NemaScan/input_data/{species}/isotypes Be sure that the small-vcf is stored in the proper file location to be accessed, both on Rockfish and on GCP for CaeNDR/local users. Note Although users will have the option to use an older vcf, the divergent region data will always be pulled from the most recent release. There could be minor changes from release to release. If this is a concern, we could switch to pulling the divergent data directly from /vast/eande106/data instead of including it in the bin/ of the mapping pipeline.","title":"Updating NemaScan"},{"location":"pipelines/pipeline-trimming/","text":"trim-fq-nf \u00b6 trim-fq-nf Pipeline overview Software requirements Relevant Docker Images Usage Testing the pipeline on Rockfish Running the pipeline on Rockfish Profiles -profile rockfish (Default) -profile quest Parameters --help --debug --trim false --species_check false --fastq_folder --raw_path (optional) --processed_path (optional) --genome_sheet (optional) --out (optional) --subsample_read_count (optional) Output Data storage Backup Poor quality data Cleanup The trim-fq-nf workflow performs FASTQ trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. You should not use the trim-fq-nf workflow on low-coverage NIL or RIL data. Recent updates in 2021 also include running a species check on the FASTQ and generating a sample sheet of high-quality, species-confirmed samples for alignment . Pipeline overview \u00b6 ____ .__ _____ _____ _/ |_ _______ |__| _____ _/ ____\\ ______ ____ _/ ____\\ \\ __\\\\_ __ \\| | / \\ ______ \\ __\\ / ____/ ______ / \\ \\ __\\ | | | | \\/| || Y Y \\ /_____/ | | | |_| | /_____/ | | \\ | | |__| |__| |__||__|_| / |__| \\__ | |___| / |__| \\/ |__| \\/ parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode ${params.debug} --fastq_folder Name of the raw fastq folder ${params.fastq_folder} --raw_path Path to raw fastq folder ${params.raw_path} --processed_path Path to processed fastq folder (output) ${params.processed_path} --genome_sheet File with fasta locations for species check ${params.genome_sheet} --out Folder name to write results ${params.out} --subsample_read_count How many reads to use for species check ${params.subsample_read_count} You have downloaded FASTQ Data to a subdirectory within a raw directory. For wild isolates this will be /vast/eande106/data/transfer/raw/<folder_name> FASTQs must end in a .fq.gz extension for the pipeline to work. You have modified FASTQ names if necessary to add strain names or other identifying information. You have installed software-requirements (see below for more info) Software requirements \u00b6 Nextflow v23+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env Note All FASTQs should end with a _R[1|2]_001.fastq.gz or a _[1|2].fq.gz . You can rename FASTQs using the rename command: for I in *.fq.gz; do mv $I $(echo $I | sed -e \"s/_1/_R1_001/\" -e \"s/_2/_R2_001/\" -e \"s/fq/fastq/\"); done Relevant Docker Images \u00b6 Note Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker images to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/alignment-nf -r 20220216-Release andersenlab/trim-fq ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/trim-fq.Dockerfile or .github/workflows/build_trimfq_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/multiqc ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/multiqc.Dockerfile or .github/workflows/build_multiqc_docker.yml GitHub actions will create a new docker image and push if successful Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name> Usage \u00b6 Testing the pipeline on Rockfish \u00b6 To see the running options and verify that paths are set correctly, you can use the --help parameter to see parameter settings. nextflow run -latest andersenlab/trim-fq-nf --debug This command uses a test dataset Running the pipeline on Rockfish \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. nextflow run -latest andersenlab/trim-fq-nf --fastq_folder <name_of_folder> Important The pipeline expects the folder containing raw fastq files to be located at /vast/eande106/data/transfer/raw/ . And all processed fastq files will be output to /vast/eande106/data/transfer/processed/ Profiles \u00b6 -profile rockfish (Default) \u00b6 If no profile is designated, the rockfish profile will run -profile quest \u00b6 This profile sets parameters for the Quest computing center Parameters \u00b6 --help \u00b6 This will print out all required and optional parameters along with their current values and then exit. --debug \u00b6 You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data/raw folder). For example: nextflow run andersenlab/trim-fq-nf --debug -resume Using --debug will automatically set the fastq_folder to test_data/raw/20210406_test1 --trim false \u00b6 This will cause the workflow to skip the fastq trimming --species_check false \u00b6 This will cause the workflow to skip the species check --fastq_folder \u00b6 This should be the name of the folder containing all fastq files located at /vast/eande106/data/transfer/raw/ . As long as there are no overlapping file names (be sure to check this first), you can combine multiple pools sequenced at the same time into one larger folder at this step. --raw_path (optional) \u00b6 The path to the fastq_folder if not default ( /vast/eande106/data/transfer/raw/ ) --processed_path (optional) \u00b6 The path to output folder if not default ( /vast/eande106/data/transfer/processed/ ) --genome_sheet (optional) \u00b6 Path to a tsv file listing project IDs for species. Default is located in trim-fq-nf/bin/genome_sheet.tsv --out (optional) \u00b6 Name of output folder with results. Default is \"processFQ-{fastq_folder}\" --subsample_read_count (optional) \u00b6 How many reads to use for species check. Default = 10,000 Output \u00b6 \u251c\u2500\u2500 /vast/eande106/data/transfer/processed/ \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}.fq.gz - - - - - - - - - - - - - - - - - - - - - - - - - - - - \u251c\u2500\u2500 multi_QC \u2502 \u251c\u2500\u2500 multiqc_data \u2502 \u2502 \u251c\u2500\u2500 *.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_data.json \u2502 \u251c\u2500\u2500 multiqc_report.html \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}_fastp.html \u251c\u2500\u2500 multiqc_data \u2502 \u2514\u2500\u2500 multiqc_samtools_stats.txt \u251c\u2500\u2500 sample_sheet \u2502 \u251c\u2500\u2500 sample_sheet_{species}_{date}_ALL.tsv \u2502 \u2514\u2500\u2500 sample_sheet_{species}_{date}_NEW.tsv \u251c\u2500\u2500 species_check \u2502 \u251c\u2500\u2500 species_check_{fastq_folder}.html \u2502 \u251c\u2500\u2500 {library}_multiple_librarires.tsv \u2502 \u251c\u2500\u2500 {library}_strains_most_likely_species.tsv \u2502 \u251c\u2500\u2500 {library}_strains_not_in_master_sheet.tsv \u2502 \u251c\u2500\u2500 {library}_strains_possibly_diff_species.tsv \u2502 \u2514\u2500\u2500 WI_all_{date}.tsv \u251c\u2500\u2500 sample_sheet_{fastq_folder}_all_temp.tsv \u2514\u2500\u2500 log.txt The resulting trimmed FASTQs will be output in the /vast/eande106/data/transfer/processed directory. The rest of the output files and reports will be generated in a new folder in the directory in which you ran the nextflow pipeline, labeled by processFQ-{fastq_folder} . MultiQC multi_QC/{strain}_{library}_fastp.html - fastp html report detailing trimming and quality multi_QC/multiqc_report.html - aggregate multi QC report for all strains pre and post trimming multi_QC/multiqc_data/*.txt or .json - files used to make previous reports Species check If a species check is run, the multiqc_data/multiqc_samtools_stats.txt will contain the results of reads mapped to each species. Furthermore, several reports and sample sheets will be generated: species_check/species_check_{date}_{library}.html is an HTML report showing how many strains have issues (not in master sheet, possibly different species, etc.) species_check/{library}_multiple_libraries.tsv - strains sequenced in multiple libraries species_check/{library}_strains_most_likely_species.tsv - list of all strains in library labeled by (1) the species in record and (2) most likely species by sequencing species_check/{library}_strains_not_in_master.tsv - list of strains not found in Robyn's wild isolate master sheets for CE, CB, or CT species_check/{library}_strains_possibly_diff_species.tsv - list of strains whose species in record does not match the likely species by sequencing species_check/WI_all_{date}.tsv - copy of all strains (CE, CB, and CT) and species designation in record sample_sheet_{date}_{library}_all_temp.tsv - temporary sample sheet with all strains for all species combined. DO NOT USE THIS FOR ALIGNMENT. Sample sheets If a species check is run, the species_check/sample_sheet folder will also contain 6 sample sheets to be used for alignment: sample_sheet/sample_sheet_{species}_{date}_ALL.tsv - sample sheet for alignment-nf using ALL strains of a particular species (i.e. c_elegans). This is useful for species we have not performed any alignments for or when we update the reference genome and need to re-align all strains. sample_sheet/sample_sheet_{species}_{date}_NEW.tsv - sample sheet for alignment-nf using all fastq from any library for ONLY strains sequenced in this particular library of a particular species (i.e. c_elegans, RET63). This is useful when the reference genome does not change and there is no need to re-align thousands of strains to save on computational power. Note The \"new\" sample sheet will still contain old fastq sequenced in a previous pool (i.e. RET55) if that strain was re-sequenced in the current pool (i.e. RET63). After running alignment-nf , this will create a new BAM file incorporating all fastq for that strain. Data storage \u00b6 Backup \u00b6 Once you have completed the trim-fq-nf pipeline you should backup the raw FASTQs. More information on this is available in the backup Poor quality data \u00b6 If you observe poor quality sequence data you should notify Robyn through the appropriate channels and then remove the data from further analysis. Cleanup \u00b6 If you have triple-checked everything and are satisfied with the results, the original raw sequence data can be deleted. The processed sequence data (FASTQ files) should be moved to their appropriate location, split by species ( /vast/eande106/data/{species}/WI/fastq/dna/ ). The following line can be used to move processed fastq prior to running alignment-nf : # change directories into the folder containing the processed fastq files cd /vast/eande106/data/transfer/processed/20210510_RET63/ # move files one species at a time (might be a more efficient line of code for this, but it works...) # !!!! make sure to change the file name !!!!! # file name ~ - ~ CHANGE THIS ~ - ~ file='/vast/eande106/Katie/trim-fq-nf/20210510_RET63/species_check/sample_sheet/sample_sheet_c_tropicalis_20201222a_NEW.tsv' # species sp=\"c_`echo $file | xargs -n1 basename | awk -F[__] '{print $4}'`\" # get list of files to move from file awk NR\\>1 $file > temp.tsv cat temp.tsv | awk '{print $4}' > files_to_move.txt cat temp.tsv | awk '{print $5}' >> files_to_move.txt # move files cat files_to_move.txt | while read line do mv $line /vast/eande106/data/$sp/WI/fastq/dna/ done # remove temp file rm files_to_move.txt rm temp.tsv Note The sample sheets ONLY contain strains that species in record matches most likely species by sequencing. If, after moving all the FASTQ for each species to their proper folder, you have FASTQ remaining, these are likely to be found in strains_possibly_diff_species.tsv . You should notify Robyn and Erik about these strains through the appropriate channels and delete the FASTQ or move to another temporary location until it can be re-sequenced.","title":"trim-fq-nf"},{"location":"pipelines/pipeline-trimming/#trim-fq-nf","text":"trim-fq-nf Pipeline overview Software requirements Relevant Docker Images Usage Testing the pipeline on Rockfish Running the pipeline on Rockfish Profiles -profile rockfish (Default) -profile quest Parameters --help --debug --trim false --species_check false --fastq_folder --raw_path (optional) --processed_path (optional) --genome_sheet (optional) --out (optional) --subsample_read_count (optional) Output Data storage Backup Poor quality data Cleanup The trim-fq-nf workflow performs FASTQ trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. You should not use the trim-fq-nf workflow on low-coverage NIL or RIL data. Recent updates in 2021 also include running a species check on the FASTQ and generating a sample sheet of high-quality, species-confirmed samples for alignment .","title":"trim-fq-nf"},{"location":"pipelines/pipeline-trimming/#pipeline_overview","text":"____ .__ _____ _____ _/ |_ _______ |__| _____ _/ ____\\ ______ ____ _/ ____\\ \\ __\\\\_ __ \\| | / \\ ______ \\ __\\ / ____/ ______ / \\ \\ __\\ | | | | \\/| || Y Y \\ /_____/ | | | |_| | /_____/ | | \\ | | |__| |__| |__||__|_| / |__| \\__ | |___| / |__| \\/ |__| \\/ parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode ${params.debug} --fastq_folder Name of the raw fastq folder ${params.fastq_folder} --raw_path Path to raw fastq folder ${params.raw_path} --processed_path Path to processed fastq folder (output) ${params.processed_path} --genome_sheet File with fasta locations for species check ${params.genome_sheet} --out Folder name to write results ${params.out} --subsample_read_count How many reads to use for species check ${params.subsample_read_count} You have downloaded FASTQ Data to a subdirectory within a raw directory. For wild isolates this will be /vast/eande106/data/transfer/raw/<folder_name> FASTQs must end in a .fq.gz extension for the pipeline to work. You have modified FASTQ names if necessary to add strain names or other identifying information. You have installed software-requirements (see below for more info)","title":"Pipeline overview"},{"location":"pipelines/pipeline-trimming/#software_requirements","text":"Nextflow v23+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env Note All FASTQs should end with a _R[1|2]_001.fastq.gz or a _[1|2].fq.gz . You can rename FASTQs using the rename command: for I in *.fq.gz; do mv $I $(echo $I | sed -e \"s/_1/_R1_001/\" -e \"s/_2/_R2_001/\" -e \"s/fq/fastq/\"); done","title":"Software requirements"},{"location":"pipelines/pipeline-trimming/#relevant_docker_images","text":"Note Before 20220301, this pipeline was run using existing conda environments on QUEST. However, these have since been migrated to docker images to allow for better control and reproducibility across platforms. If you need to access the conda version, you can always run an old commit with nextflow run andersenlab/alignment-nf -r 20220216-Release andersenlab/trim-fq ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/trim-fq.Dockerfile or .github/workflows/build_trimfq_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/multiqc ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/multiqc.Dockerfile or .github/workflows/build_multiqc_docker.yml GitHub actions will create a new docker image and push if successful Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name>","title":"Relevant Docker Images"},{"location":"pipelines/pipeline-trimming/#usage","text":"","title":"Usage"},{"location":"pipelines/pipeline-trimming/#testing_the_pipeline_on_rockfish","text":"To see the running options and verify that paths are set correctly, you can use the --help parameter to see parameter settings. nextflow run -latest andersenlab/trim-fq-nf --debug This command uses a test dataset","title":"Testing the pipeline on Rockfish"},{"location":"pipelines/pipeline-trimming/#running_the_pipeline_on_rockfish","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. nextflow run -latest andersenlab/trim-fq-nf --fastq_folder <name_of_folder> Important The pipeline expects the folder containing raw fastq files to be located at /vast/eande106/data/transfer/raw/ . And all processed fastq files will be output to /vast/eande106/data/transfer/processed/","title":"Running the pipeline on Rockfish"},{"location":"pipelines/pipeline-trimming/#profiles","text":"","title":"Profiles"},{"location":"pipelines/pipeline-trimming/#-profile_rockfish_default","text":"If no profile is designated, the rockfish profile will run","title":"-profile rockfish (Default)"},{"location":"pipelines/pipeline-trimming/#-profile_quest","text":"This profile sets parameters for the Quest computing center","title":"-profile quest"},{"location":"pipelines/pipeline-trimming/#parameters","text":"","title":"Parameters"},{"location":"pipelines/pipeline-trimming/#--help","text":"This will print out all required and optional parameters along with their current values and then exit.","title":"--help"},{"location":"pipelines/pipeline-trimming/#--debug","text":"You should use --debug for testing/debugging purposes. This will run the debug test set (located in the test_data/raw folder). For example: nextflow run andersenlab/trim-fq-nf --debug -resume Using --debug will automatically set the fastq_folder to test_data/raw/20210406_test1","title":"--debug"},{"location":"pipelines/pipeline-trimming/#--trim_false","text":"This will cause the workflow to skip the fastq trimming","title":"--trim false"},{"location":"pipelines/pipeline-trimming/#--species_check_false","text":"This will cause the workflow to skip the species check","title":"--species_check false"},{"location":"pipelines/pipeline-trimming/#--fastq_folder","text":"This should be the name of the folder containing all fastq files located at /vast/eande106/data/transfer/raw/ . As long as there are no overlapping file names (be sure to check this first), you can combine multiple pools sequenced at the same time into one larger folder at this step.","title":"--fastq_folder"},{"location":"pipelines/pipeline-trimming/#--raw_path_optional","text":"The path to the fastq_folder if not default ( /vast/eande106/data/transfer/raw/ )","title":"--raw_path (optional)"},{"location":"pipelines/pipeline-trimming/#--processed_path_optional","text":"The path to output folder if not default ( /vast/eande106/data/transfer/processed/ )","title":"--processed_path (optional)"},{"location":"pipelines/pipeline-trimming/#--genome_sheet_optional","text":"Path to a tsv file listing project IDs for species. Default is located in trim-fq-nf/bin/genome_sheet.tsv","title":"--genome_sheet (optional)"},{"location":"pipelines/pipeline-trimming/#--out_optional","text":"Name of output folder with results. Default is \"processFQ-{fastq_folder}\"","title":"--out (optional)"},{"location":"pipelines/pipeline-trimming/#--subsample_read_count_optional","text":"How many reads to use for species check. Default = 10,000","title":"--subsample_read_count (optional)"},{"location":"pipelines/pipeline-trimming/#output","text":"\u251c\u2500\u2500 /vast/eande106/data/transfer/processed/ \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}.fq.gz - - - - - - - - - - - - - - - - - - - - - - - - - - - - \u251c\u2500\u2500 multi_QC \u2502 \u251c\u2500\u2500 multiqc_data \u2502 \u2502 \u251c\u2500\u2500 *.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_data.json \u2502 \u251c\u2500\u2500 multiqc_report.html \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}_fastp.html \u251c\u2500\u2500 multiqc_data \u2502 \u2514\u2500\u2500 multiqc_samtools_stats.txt \u251c\u2500\u2500 sample_sheet \u2502 \u251c\u2500\u2500 sample_sheet_{species}_{date}_ALL.tsv \u2502 \u2514\u2500\u2500 sample_sheet_{species}_{date}_NEW.tsv \u251c\u2500\u2500 species_check \u2502 \u251c\u2500\u2500 species_check_{fastq_folder}.html \u2502 \u251c\u2500\u2500 {library}_multiple_librarires.tsv \u2502 \u251c\u2500\u2500 {library}_strains_most_likely_species.tsv \u2502 \u251c\u2500\u2500 {library}_strains_not_in_master_sheet.tsv \u2502 \u251c\u2500\u2500 {library}_strains_possibly_diff_species.tsv \u2502 \u2514\u2500\u2500 WI_all_{date}.tsv \u251c\u2500\u2500 sample_sheet_{fastq_folder}_all_temp.tsv \u2514\u2500\u2500 log.txt The resulting trimmed FASTQs will be output in the /vast/eande106/data/transfer/processed directory. The rest of the output files and reports will be generated in a new folder in the directory in which you ran the nextflow pipeline, labeled by processFQ-{fastq_folder} . MultiQC multi_QC/{strain}_{library}_fastp.html - fastp html report detailing trimming and quality multi_QC/multiqc_report.html - aggregate multi QC report for all strains pre and post trimming multi_QC/multiqc_data/*.txt or .json - files used to make previous reports Species check If a species check is run, the multiqc_data/multiqc_samtools_stats.txt will contain the results of reads mapped to each species. Furthermore, several reports and sample sheets will be generated: species_check/species_check_{date}_{library}.html is an HTML report showing how many strains have issues (not in master sheet, possibly different species, etc.) species_check/{library}_multiple_libraries.tsv - strains sequenced in multiple libraries species_check/{library}_strains_most_likely_species.tsv - list of all strains in library labeled by (1) the species in record and (2) most likely species by sequencing species_check/{library}_strains_not_in_master.tsv - list of strains not found in Robyn's wild isolate master sheets for CE, CB, or CT species_check/{library}_strains_possibly_diff_species.tsv - list of strains whose species in record does not match the likely species by sequencing species_check/WI_all_{date}.tsv - copy of all strains (CE, CB, and CT) and species designation in record sample_sheet_{date}_{library}_all_temp.tsv - temporary sample sheet with all strains for all species combined. DO NOT USE THIS FOR ALIGNMENT. Sample sheets If a species check is run, the species_check/sample_sheet folder will also contain 6 sample sheets to be used for alignment: sample_sheet/sample_sheet_{species}_{date}_ALL.tsv - sample sheet for alignment-nf using ALL strains of a particular species (i.e. c_elegans). This is useful for species we have not performed any alignments for or when we update the reference genome and need to re-align all strains. sample_sheet/sample_sheet_{species}_{date}_NEW.tsv - sample sheet for alignment-nf using all fastq from any library for ONLY strains sequenced in this particular library of a particular species (i.e. c_elegans, RET63). This is useful when the reference genome does not change and there is no need to re-align thousands of strains to save on computational power. Note The \"new\" sample sheet will still contain old fastq sequenced in a previous pool (i.e. RET55) if that strain was re-sequenced in the current pool (i.e. RET63). After running alignment-nf , this will create a new BAM file incorporating all fastq for that strain.","title":"Output"},{"location":"pipelines/pipeline-trimming/#data_storage","text":"","title":"Data storage"},{"location":"pipelines/pipeline-trimming/#backup","text":"Once you have completed the trim-fq-nf pipeline you should backup the raw FASTQs. More information on this is available in the backup","title":"Backup"},{"location":"pipelines/pipeline-trimming/#poor_quality_data","text":"If you observe poor quality sequence data you should notify Robyn through the appropriate channels and then remove the data from further analysis.","title":"Poor quality data"},{"location":"pipelines/pipeline-trimming/#cleanup","text":"If you have triple-checked everything and are satisfied with the results, the original raw sequence data can be deleted. The processed sequence data (FASTQ files) should be moved to their appropriate location, split by species ( /vast/eande106/data/{species}/WI/fastq/dna/ ). The following line can be used to move processed fastq prior to running alignment-nf : # change directories into the folder containing the processed fastq files cd /vast/eande106/data/transfer/processed/20210510_RET63/ # move files one species at a time (might be a more efficient line of code for this, but it works...) # !!!! make sure to change the file name !!!!! # file name ~ - ~ CHANGE THIS ~ - ~ file='/vast/eande106/Katie/trim-fq-nf/20210510_RET63/species_check/sample_sheet/sample_sheet_c_tropicalis_20201222a_NEW.tsv' # species sp=\"c_`echo $file | xargs -n1 basename | awk -F[__] '{print $4}'`\" # get list of files to move from file awk NR\\>1 $file > temp.tsv cat temp.tsv | awk '{print $4}' > files_to_move.txt cat temp.tsv | awk '{print $5}' >> files_to_move.txt # move files cat files_to_move.txt | while read line do mv $line /vast/eande106/data/$sp/WI/fastq/dna/ done # remove temp file rm files_to_move.txt rm temp.tsv Note The sample sheets ONLY contain strains that species in record matches most likely species by sequencing. If, after moving all the FASTQ for each species to their proper folder, you have FASTQ remaining, these are likely to be found in strains_possibly_diff_species.tsv . You should notify Robyn and Erik about these strains through the appropriate channels and delete the FASTQ or move to another temporary location until it can be re-sequenced.","title":"Cleanup"},{"location":"pipelines/pipeline-wiGATK/","text":"wi-gatk \u00b6 wi-gatk Pipeline Overview Software Requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --sample_sheet --bam_location (optional) --species (optional) --project (optional) --ws_build (optional) --reference (optional) --mito_name (optional) --output (optional) Output Data Storage Cleanup The wi-gatk pipeline filters and calls variants from wild isolate sequence data. Pipeline Overview \u00b6 _______ _______ _______ __ __ _______ _______ | __| _ |_ _| |/ | | | | ___| | | | | | | | < | | ___| |_______|___|___| |___| |__|\\__| |__|____|___| parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --output Release Directory WI-{date} --sample_sheet Sample sheet null --bam_location Directory of bam files /projects/b1059/data/{species}/WI/alignments/ --mito_name Contig not to polarize hetero sites MtDNA Reference Genome --------------- --reference_base Location of ref genomes /projects/b1059/data/{species}/genomes/ --species/project/build These 4 params form --reference {species} / {project} / {ws_build} Variant Filters --------------- --min_depth Minimum variant depth 5 --qual Variant QUAL score 30 --strand_odds_ratio SOR_strand_odds_ratio 5 --quality_by_depth QD_quality_by_depth 20 --fisherstrand FS_fisher_strand 100 --high_missing Max % missing genotypes 0.95 --high_heterozygosity Max % max heterozygosity 0.10 Software Requirements \u00b6 The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env Relevant Docker Images \u00b6 andersenlab/gatk4 ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/gatk4.Dockerfile or .github/workflows/build_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/r_packages ( link ): Docker image is created manually, code can be found in the dockerfile repo. Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name> Usage \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. Testing on Rockfish \u00b6 This command uses a test dataset nextflow run -latest andersenlab/wi-gatk --debug Running on Rockfish \u00b6 You should run this in a screen or tmux session. Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. nextflow run -latest andersenlab/wi-gatk --sample_sheet <path_to_sample_sheet> --species c_elegans Parameters \u00b6 -profile \u00b6 There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default --sample_sheet \u00b6 The sample sheet is automatically generated from alignment-nf . The sample sheet contains 5 columns as detailed below: strain bam bai coverage percent_mapped AB1 AB1.bam AB1.bam.bai 64 99.4 AB4 AB4.bam AB4.bam.bai 52 99.2 BRC20067 BRC20067.bam BRC20067.bam.bai 30 92.5 Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced. Note The sample sheet produced from alignment-nf is only for strains that you ran in the alignment pipeline most recently. If you want to combine old strains with new strains, you will have to combine two or more sample sheets. If you are running a species-wide analysis for CaeNDR, please follow the notes in the full WI protocol here --bam_location (optional) \u00b6 Path to directory holding all the alignment files for strains in the analysis. Defaults to /vast/eande106/data/{species}/WI/alignments/ Important Remember to move your bam files output from alignment-nf to this location prior to running wi-gatk . In most cases, you will want to run wi-gatk on all samples, new and old combined. --species (optional) \u00b6 default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis --project (optional) \u00b6 default = PRJNA13758 WormBase project ID for selected species. Choose from some examples here --ws_build (optional) \u00b6 default = WS283 WormBase version to use for reference genome. --reference (optional) \u00b6 A fasta reference indexed with BWA. On Rockfish, the reference is available here: /vast/eande106/data/c_elegans/genomes/PRJNA13758/WS283/c_elegans.PRJNA13758.WS283.genome.fa.gz Note If running on Rockfish, instead of changing the reference parameter, opt to change the species , project , and ws_build for other species like c_briggsae (and then the reference will change automatically) --mito_name (optional) \u00b6 Name of contig to skip het polarization. Might need to change for other species besides c_elegans if the mitochondria contig is named differently. Defaults to MtDNA . --output (optional) \u00b6 A directory in which to output results. By default it will be WI-YYYYMMDD where YYYYMMDD is todays date. Output \u00b6 The final output directory looks like this: \u251c\u2500\u2500 variation \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.hard-filter.stats.txt \u2502 \u251c\u2500\u2500 *.hard-filter.filter_stats.txt \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.soft-filter.stats.txt \u2502 \u2514\u2500\u2500 *.soft-filter.filter_stats.txt \u2514\u2500\u2500 report \u251c\u2500\u2500 multiqc.html \u2514\u2500\u2500 multiqc_data \u2514\u2500\u2500 multiqc_*.json Data Storage \u00b6 Cleanup \u00b6 The hard-filter.vcf is the input for both the concordance-nf pipeline and the post-gatk-nf pipeline. Once both pipelines have been completed successfully, the hard and soft filter vcf and index files (everything output in the variation folder) can be moved to /vast/eande106/data/{species}/WI/variation/{date}/vcf .","title":"wi-gatk"},{"location":"pipelines/pipeline-wiGATK/#wi-gatk","text":"wi-gatk Pipeline Overview Software Requirements Relevant Docker Images Usage Testing on Rockfish Running on Rockfish Parameters -profile --sample_sheet --bam_location (optional) --species (optional) --project (optional) --ws_build (optional) --reference (optional) --mito_name (optional) --output (optional) Output Data Storage Cleanup The wi-gatk pipeline filters and calls variants from wild isolate sequence data.","title":"wi-gatk"},{"location":"pipelines/pipeline-wiGATK/#pipeline_overview","text":"_______ _______ _______ __ __ _______ _______ | __| _ |_ _| |/ | | | | ___| | | | | | | | < | | ___| |_______|___|___| |___| |__|\\__| |__|____|___| parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --output Release Directory WI-{date} --sample_sheet Sample sheet null --bam_location Directory of bam files /projects/b1059/data/{species}/WI/alignments/ --mito_name Contig not to polarize hetero sites MtDNA Reference Genome --------------- --reference_base Location of ref genomes /projects/b1059/data/{species}/genomes/ --species/project/build These 4 params form --reference {species} / {project} / {ws_build} Variant Filters --------------- --min_depth Minimum variant depth 5 --qual Variant QUAL score 30 --strand_odds_ratio SOR_strand_odds_ratio 5 --quality_by_depth QD_quality_by_depth 20 --fisherstrand FS_fisher_strand 100 --high_missing Max % missing genotypes 0.95 --high_heterozygosity Max % max heterozygosity 0.10","title":"Pipeline Overview"},{"location":"pipelines/pipeline-wiGATK/#software_requirements","text":"The latest update requires Nextflow version 23+. On Rockfish, you can access this version by loading the nf23_env conda environment prior to running the pipeline command: module load python/anaconda source activate /data/eande106/software/conda_envs/nf23_env","title":"Software Requirements"},{"location":"pipelines/pipeline-wiGATK/#relevant_docker_images","text":"andersenlab/gatk4 ( link ): Docker image is created within this pipeline using GitHub actions. Whenever a change is made to env/gatk4.Dockerfile or .github/workflows/build_docker.yml GitHub actions will create a new docker image and push if successful andersenlab/r_packages ( link ): Docker image is created manually, code can be found in the dockerfile repo. Make sure that you add the following code to your ~/.bash_profile . This line makes sure that any singularity images you download will go to a shared location on /vast/eande106 for other users to take advantage of (without them also having to download the same image). # add singularity cache export SINGULARITY_CACHEDIR='/vast/eande106/singularity/' Note If you need to work with the docker container, you will need to create an interactive session as singularity can't be run on Rockfish login nodes. interact -n1 -pexpress module load singularity singularity shell [--bind local_dir:container_dir] /vast/eande106/singularity/<image_name>","title":"Relevant Docker Images"},{"location":"pipelines/pipeline-wiGATK/#usage","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page.","title":"Usage"},{"location":"pipelines/pipeline-wiGATK/#testing_on_rockfish","text":"This command uses a test dataset nextflow run -latest andersenlab/wi-gatk --debug","title":"Testing on Rockfish"},{"location":"pipelines/pipeline-wiGATK/#running_on_rockfish","text":"You should run this in a screen or tmux session. Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. nextflow run -latest andersenlab/wi-gatk --sample_sheet <path_to_sample_sheet> --species c_elegans","title":"Running on Rockfish"},{"location":"pipelines/pipeline-wiGATK/#parameters","text":"","title":"Parameters"},{"location":"pipelines/pipeline-wiGATK/#-profile","text":"There are three configuration profiles for this pipeline. rockfish - Used for running on Rockfish (default). quest - Used for running on Quest. local - Used for local development. Note If you forget to add a -profile , the rockfish profile will be chosen as default","title":"-profile"},{"location":"pipelines/pipeline-wiGATK/#--sample_sheet","text":"The sample sheet is automatically generated from alignment-nf . The sample sheet contains 5 columns as detailed below: strain bam bai coverage percent_mapped AB1 AB1.bam AB1.bam.bai 64 99.4 AB4 AB4.bam AB4.bam.bai 52 99.2 BRC20067 BRC20067.bam BRC20067.bam.bai 30 92.5 Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced. Note The sample sheet produced from alignment-nf is only for strains that you ran in the alignment pipeline most recently. If you want to combine old strains with new strains, you will have to combine two or more sample sheets. If you are running a species-wide analysis for CaeNDR, please follow the notes in the full WI protocol here","title":"--sample_sheet"},{"location":"pipelines/pipeline-wiGATK/#--bam_location_optional","text":"Path to directory holding all the alignment files for strains in the analysis. Defaults to /vast/eande106/data/{species}/WI/alignments/ Important Remember to move your bam files output from alignment-nf to this location prior to running wi-gatk . In most cases, you will want to run wi-gatk on all samples, new and old combined.","title":"--bam_location (optional)"},{"location":"pipelines/pipeline-wiGATK/#--species_optional","text":"default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis","title":"--species (optional)"},{"location":"pipelines/pipeline-wiGATK/#--project_optional","text":"default = PRJNA13758 WormBase project ID for selected species. Choose from some examples here","title":"--project (optional)"},{"location":"pipelines/pipeline-wiGATK/#--ws_build_optional","text":"default = WS283 WormBase version to use for reference genome.","title":"--ws_build (optional)"},{"location":"pipelines/pipeline-wiGATK/#--reference_optional","text":"A fasta reference indexed with BWA. On Rockfish, the reference is available here: /vast/eande106/data/c_elegans/genomes/PRJNA13758/WS283/c_elegans.PRJNA13758.WS283.genome.fa.gz Note If running on Rockfish, instead of changing the reference parameter, opt to change the species , project , and ws_build for other species like c_briggsae (and then the reference will change automatically)","title":"--reference (optional)"},{"location":"pipelines/pipeline-wiGATK/#--mito_name_optional","text":"Name of contig to skip het polarization. Might need to change for other species besides c_elegans if the mitochondria contig is named differently. Defaults to MtDNA .","title":"--mito_name (optional)"},{"location":"pipelines/pipeline-wiGATK/#--output_optional","text":"A directory in which to output results. By default it will be WI-YYYYMMDD where YYYYMMDD is todays date.","title":"--output (optional)"},{"location":"pipelines/pipeline-wiGATK/#output","text":"The final output directory looks like this: \u251c\u2500\u2500 variation \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.hard-filter.stats.txt \u2502 \u251c\u2500\u2500 *.hard-filter.filter_stats.txt \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.soft-filter.stats.txt \u2502 \u2514\u2500\u2500 *.soft-filter.filter_stats.txt \u2514\u2500\u2500 report \u251c\u2500\u2500 multiqc.html \u2514\u2500\u2500 multiqc_data \u2514\u2500\u2500 multiqc_*.json","title":"Output"},{"location":"pipelines/pipeline-wiGATK/#data_storage","text":"","title":"Data Storage"},{"location":"pipelines/pipeline-wiGATK/#cleanup","text":"The hard-filter.vcf is the input for both the concordance-nf pipeline and the post-gatk-nf pipeline. Once both pipelines have been completed successfully, the hard and soft filter vcf and index files (everything output in the variation folder) can be moved to /vast/eande106/data/{species}/WI/variation/{date}/vcf .","title":"Cleanup"},{"location":"quest/b1059/","text":"Data storage on b1059 \u00b6 Data storage on b1059 analysis data docs modules projects software to_be_deleted workflows In 2021, the lab underwent a large restructuring of the data storage on b1059. The goal of this restructure was 1) to make it easy and clear to find data of a specific type and 2) to allow the expansion from mostly C. elegans data to other species, particularly C. briggsae and C. tropicalis . b1059 is broken down into several main parts, here I will describe them all: analysis \u00b6 This directory contains non-data output and analyses from general lab pipelines (i.e. wi-gatk or alignment-nf ). It is organized by pipeline and then by analysis type-date. If you are running these pipelines (including nil-ril-nf ) it is important that you move your analysis folder here once complete (and out of your personal folder) so that everyone has access to the results. data \u00b6 This directory contains all the lab data -- split by species. The goal is to create the same file structure across all species. This not only makes it easy to find the file you are looking for, but also makes it easy to script file locations. A general example of a species' file structure is below: \u251c\u2500\u2500 genomes \u2502 \u251c\u2500\u2500 genetic_map \u2502 \u2502 \u2514\u2500\u2500 {chr}.map \u2502 \u251c\u2500\u2500 {project_ID} \u2502 \u2502 \u2514\u2500\u2500 {WS_build} \u2502 \u2502 \u251c\u2500\u2500 *.genome.fa* \u2502 \u2502 \u251c\u2500\u2500 csq \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Length.tsv \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Scores.tsv \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.csq.gff3.gz \u2502 \u2502 \u251c\u2500\u2500 lcr \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.dust.bed.gz \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.repeat_masker.bed.gz \u2502 \u2502 \u2514\u2500\u2500 snpeff \u2502 \u2502 \u251c\u2500\u2500 {species}.{project}.{ws_build} \u2502 \u2502 \u2502 \u251c\u2500\u2500 genes.gtf.gz \u2502 \u2502 \u2502 \u251c\u2500\u2500 sequences.fa \u2502 \u2502 \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin \u2502 \u2502 \u2514\u2500\u2500 snpEff.config \u2502 \u2514\u2500\u2500 WI_PacBio_assemblies \u251c\u2500\u2500 RIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 NIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 WI \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u251c\u2500\u2500 _bam_not_for_cendr \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 concordance \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 divergent_regions \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u251c\u2500\u2500 dna \u2502 \u2502 \u2502 \u251c\u2500\u2500 _fastq_not_for_cendr \u2502 \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2502 \u2514\u2500\u2500 rna \u2502 \u2502 \u2514\u2500\u2500 {project} \u2502 \u251c\u2500\u2500 haplotype \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tree \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tracks \u2502 \u2514\u2500\u2500 vcf \u2502 \u251c\u2500\u2500 strain_vcf \u2502 \u2502 \u2514\u2500\u2500 {strain}.{date}.vcf.gz \u2502 \u2514\u2500\u2500 WI.{date}.*.vcf.gz \u2514\u2500\u2500 {other - i.e. BSA, MUTANT, HiC, PacBio} \u2514\u2500\u2500 *currently could look like anything in here - organized by project* Note This restructure is still not 100% complete and things might continue to change as we expand our genomes and data types. If you see something that is not organized well, let Erik or Katie know. docs \u00b6 This directory should probably be removed and the docs inside stored elsewhere. Currently, shows the process for fastq SRA submission from 2019 and then again from 2021. modules \u00b6 I think this directory can maybe be deleted? projects \u00b6 Projects is where most people will spend most of their time. Each user with access to quest should create a user-specific folder (named with their name) in the projects directory. Inside your folder, you can do all your work: analyzing data, writing new scripts, temporary storage, etc. Note Please be aware of how much space you are using in your personal folder. Of course some projects might require more space than others, and some projects require a lot of temporary space that can be deleted once completed. However, if you find that you have > 500-1000 GB of used space in your folder, please take a look if there is any unused data or scripts. Either way, it is good practice to clean out your folder every few months to avoid storage pile-up. You can check how much space you are using with du -hs * or ask Katie. software \u00b6 General This is a great location to download any software tools or packages that you need to use that you cannot access with Quest or create a conda environment for. Especially important if they are software packages that other people in the lab might also use, as it is a shared space. Conda environments Inside the directory conda_envs you can find all the shared lab conda environments necessary for running certain nextflow pipelines. If you create a shared conda environment, it is important that you update the README.md with the code you used to create the environment for reproducible purposes . You can create a conda environment in this directory with: conda create -p /projects/b1059/software/conda_envs/<name_of_env> You can also update your ~/.condarc file to point to this directory so that you can easily load conda environments just by the name (i.e. source activate nf20_env instead of source activate /projects/b1059/software/conda_envs/nf20_env ): channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /projects/b1059/software/conda_envs/ Important It is very important that you do not update any packages or software while running a shared conda environment. This is especially an issue with Nextflow and the nf20_env environment. Updating nextflow whilie running this environment will update the version in the environment -- and it needs to be v20.0 for many of the pipelines to run successfully If you want to see what all is loaded in a particular conda environment, or re-create an environment, you can use: # make an exact copy conda create --clone py35 --name py35-2 # list all packages and versions conda list --explicit > bio-env.txt # list a history of revisions conda list --revisions # go back to a previous revision conda install --revision 2 # create environment from file conda env create --file bio-env.txt Also, check out this cheat sheet or our conda page for more. Note Most of the legacy Nextflow pipelines are written using module load python/anaconda3.6; source activate nf20_env , so if you are having trouble running with conda, try loading the right environment first. R libraries Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. One way we have gotten around this is by installing the proper versions of R packages to a shared location ( /projects/b1059/software/R_lib_3.6.0 ). Check out the Using R on Quest section of the R page to learn more about installing a package to this folder and using it in a script. Important It is very important that you do not update any packages in this location unless absolutely necessary and proceed with extreme caution (especially if it is tidyverse !!!). Updating packages could break pipelines that rely on them. Docker images This is not available yet, but might be a good idea to store shared docker images here. Note: can change the singularity_cache parameter in nextflow scripts to store images here in the future . to_be_deleted \u00b6 This directory is a temporary holding place for large files/data that can be deleted. Think of it as a soft-trash bin. If, after a few months of living in to_be_deleted , it is likely the data in fact can be deleted without being missed. This was a temporary solution for the large restructure in 2021 and can likely be removed in the future. workflows \u00b6 This directory is being phased out. Workflows will now be hosted on GitHub and any lab members who wish to run a shared workflow should run remotely (if nextflow script) or clone the repo into their personal folders.","title":"Data on b1059"},{"location":"quest/b1059/#data_storage_on_b1059","text":"Data storage on b1059 analysis data docs modules projects software to_be_deleted workflows In 2021, the lab underwent a large restructuring of the data storage on b1059. The goal of this restructure was 1) to make it easy and clear to find data of a specific type and 2) to allow the expansion from mostly C. elegans data to other species, particularly C. briggsae and C. tropicalis . b1059 is broken down into several main parts, here I will describe them all:","title":"Data storage on b1059"},{"location":"quest/b1059/#analysis","text":"This directory contains non-data output and analyses from general lab pipelines (i.e. wi-gatk or alignment-nf ). It is organized by pipeline and then by analysis type-date. If you are running these pipelines (including nil-ril-nf ) it is important that you move your analysis folder here once complete (and out of your personal folder) so that everyone has access to the results.","title":"analysis"},{"location":"quest/b1059/#data","text":"This directory contains all the lab data -- split by species. The goal is to create the same file structure across all species. This not only makes it easy to find the file you are looking for, but also makes it easy to script file locations. A general example of a species' file structure is below: \u251c\u2500\u2500 genomes \u2502 \u251c\u2500\u2500 genetic_map \u2502 \u2502 \u2514\u2500\u2500 {chr}.map \u2502 \u251c\u2500\u2500 {project_ID} \u2502 \u2502 \u2514\u2500\u2500 {WS_build} \u2502 \u2502 \u251c\u2500\u2500 *.genome.fa* \u2502 \u2502 \u251c\u2500\u2500 csq \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Length.tsv \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Scores.tsv \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.csq.gff3.gz \u2502 \u2502 \u251c\u2500\u2500 lcr \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.dust.bed.gz \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.repeat_masker.bed.gz \u2502 \u2502 \u2514\u2500\u2500 snpeff \u2502 \u2502 \u251c\u2500\u2500 {species}.{project}.{ws_build} \u2502 \u2502 \u2502 \u251c\u2500\u2500 genes.gtf.gz \u2502 \u2502 \u2502 \u251c\u2500\u2500 sequences.fa \u2502 \u2502 \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin \u2502 \u2502 \u2514\u2500\u2500 snpEff.config \u2502 \u2514\u2500\u2500 WI_PacBio_assemblies \u251c\u2500\u2500 RIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 NIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 WI \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u251c\u2500\u2500 _bam_not_for_cendr \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 concordance \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 divergent_regions \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u251c\u2500\u2500 dna \u2502 \u2502 \u2502 \u251c\u2500\u2500 _fastq_not_for_cendr \u2502 \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2502 \u2514\u2500\u2500 rna \u2502 \u2502 \u2514\u2500\u2500 {project} \u2502 \u251c\u2500\u2500 haplotype \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tree \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tracks \u2502 \u2514\u2500\u2500 vcf \u2502 \u251c\u2500\u2500 strain_vcf \u2502 \u2502 \u2514\u2500\u2500 {strain}.{date}.vcf.gz \u2502 \u2514\u2500\u2500 WI.{date}.*.vcf.gz \u2514\u2500\u2500 {other - i.e. BSA, MUTANT, HiC, PacBio} \u2514\u2500\u2500 *currently could look like anything in here - organized by project* Note This restructure is still not 100% complete and things might continue to change as we expand our genomes and data types. If you see something that is not organized well, let Erik or Katie know.","title":"data"},{"location":"quest/b1059/#docs","text":"This directory should probably be removed and the docs inside stored elsewhere. Currently, shows the process for fastq SRA submission from 2019 and then again from 2021.","title":"docs"},{"location":"quest/b1059/#modules","text":"I think this directory can maybe be deleted?","title":"modules"},{"location":"quest/b1059/#projects","text":"Projects is where most people will spend most of their time. Each user with access to quest should create a user-specific folder (named with their name) in the projects directory. Inside your folder, you can do all your work: analyzing data, writing new scripts, temporary storage, etc. Note Please be aware of how much space you are using in your personal folder. Of course some projects might require more space than others, and some projects require a lot of temporary space that can be deleted once completed. However, if you find that you have > 500-1000 GB of used space in your folder, please take a look if there is any unused data or scripts. Either way, it is good practice to clean out your folder every few months to avoid storage pile-up. You can check how much space you are using with du -hs * or ask Katie.","title":"projects"},{"location":"quest/b1059/#software","text":"General This is a great location to download any software tools or packages that you need to use that you cannot access with Quest or create a conda environment for. Especially important if they are software packages that other people in the lab might also use, as it is a shared space. Conda environments Inside the directory conda_envs you can find all the shared lab conda environments necessary for running certain nextflow pipelines. If you create a shared conda environment, it is important that you update the README.md with the code you used to create the environment for reproducible purposes . You can create a conda environment in this directory with: conda create -p /projects/b1059/software/conda_envs/<name_of_env> You can also update your ~/.condarc file to point to this directory so that you can easily load conda environments just by the name (i.e. source activate nf20_env instead of source activate /projects/b1059/software/conda_envs/nf20_env ): channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /projects/b1059/software/conda_envs/ Important It is very important that you do not update any packages or software while running a shared conda environment. This is especially an issue with Nextflow and the nf20_env environment. Updating nextflow whilie running this environment will update the version in the environment -- and it needs to be v20.0 for many of the pipelines to run successfully If you want to see what all is loaded in a particular conda environment, or re-create an environment, you can use: # make an exact copy conda create --clone py35 --name py35-2 # list all packages and versions conda list --explicit > bio-env.txt # list a history of revisions conda list --revisions # go back to a previous revision conda install --revision 2 # create environment from file conda env create --file bio-env.txt Also, check out this cheat sheet or our conda page for more. Note Most of the legacy Nextflow pipelines are written using module load python/anaconda3.6; source activate nf20_env , so if you are having trouble running with conda, try loading the right environment first. R libraries Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. One way we have gotten around this is by installing the proper versions of R packages to a shared location ( /projects/b1059/software/R_lib_3.6.0 ). Check out the Using R on Quest section of the R page to learn more about installing a package to this folder and using it in a script. Important It is very important that you do not update any packages in this location unless absolutely necessary and proceed with extreme caution (especially if it is tidyverse !!!). Updating packages could break pipelines that rely on them. Docker images This is not available yet, but might be a good idea to store shared docker images here. Note: can change the singularity_cache parameter in nextflow scripts to store images here in the future .","title":"software"},{"location":"quest/b1059/#to_be_deleted","text":"This directory is a temporary holding place for large files/data that can be deleted. Think of it as a soft-trash bin. If, after a few months of living in to_be_deleted , it is likely the data in fact can be deleted without being missed. This was a temporary solution for the large restructure in 2021 and can likely be removed in the future.","title":"to_be_deleted"},{"location":"quest/b1059/#workflows","text":"This directory is being phased out. Workflows will now be hosted on GitHub and any lab members who wish to run a shared workflow should run remotely (if nextflow script) or clone the repo into their personal folders.","title":"workflows"},{"location":"quest/quest-conda/","text":"Using conda on Quest \u00b6 Using conda on Quest Why Conda Setting up Conda on Quest Using Conda Running Nextflow with conda Notes on conda versions on Quest Why Conda \u00b6 Computational Reproducibility is the ability to reproduce an analysis exactly. In order for computational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub (for help, see the Github page ). Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software, package and environment managers such as Conda and Docker are very useful. Conda works similarly to brew or pyenv that were used in the legacy Andersen-Lab-Env. Note The software environments on Mac and Linux are not exactly identical...but they are very close. Setting up Conda on Quest \u00b6 Anaconda is a Python distribution that contains many packages including conda. Miniconda is a more compact version of Anaconda that also includes conda. So in order to install conda, we usually either install Miniconda or Anaconda. On Quest, Anaconda is already installed. However there are many versions of Anaconda, each can have a different version of Python and Conda. The current lab environments mainly used module load python/anaconda3.6 (See Notes below for more info). In your home directory ~/ , create a file called .condarc and put the following lines into it. It sets the channel priority when conda searches for packages. If possible, in one environment it is good to use packages from the same channel. channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /projects/b1059/software/conda_envs/ Using Conda \u00b6 Conda Documentation Conda Cheatsheet After loading the Anaconda module, one can create an environment and install packages into that environment: # Create conda environment named \"name_of_env\" conda create --name name_of_env # Create conda environment in a specific folder conda create -p /projects/b1059/software/conda_envs/test # activate conda environment # for some `conda activate` works and for others `source activate` conda activate test source activate test # install package into conda environment conda install bcftools When looking to install a package, one resource to check out is anaconda.org , search for your package and run the install command listed on the page. Note Remember to keep in mind the different versions of software/packages. You could have bcftools-v1.10 or bcftools-v.1.12, so make sure you install the correct one! Important You can also install R packages with conda, however conda and R don't always work well together. Check out our quick fix here Running Nextflow with conda \u00b6 When running Nextflow, conda environments can be specified as part of a process or in the nextflow.config file to apply to the entire pipeline (check out the documentation ): Conda within a process: process foo { conda '/projects/b1059/software/conda_envs/cegwas2-nf_env' ''' your_command --here ''' } Conda for the entire pipeline: // in the nextflow.config file: conda { conda.enabled = true conda.cacheDir = \".env\" } process { conda = \"/projects/b1059/software/conda_envs/cegwas2-nf_env\" } Notes on conda versions on Quest \u00b6 tl;dr; If having trouble with conda, or Nextflow gives conda-related errors, try to load a different version of anaconda on Quest. At some point it may be worth re-creating all conda environments in the lab with a consistent version of conda. As of the end of 2020, existing conda environments for the lab were mostly created by module load python/anaconda (which got automatically loaded with module git by accident). It loads Python version 2.7.18 and conda 4.5.2. The other environments were created with module load python/anaconda3.6 which loads Python 3.6.0 and conda 4.3.30. To see versions, use conda info or conda -V . Once you activate an environment with conda activate env_name or source activate env_name , the default conda usually get re-directed to the conda that were originally used to create the environment. This is good because it helps ensure that all packages in the same environment uses the same version of conda. One can go to cd ~/.conda/env_name/bin and readlink -f conda or readlink -f activate to see which version of conda is used by this environment. This is exactly how Nextflow determines which conda to use when using an existing conda environment. Quest people recommended module load python-anaconda3 , but that version does not mix well with the versions mentioned above. But if one were to re-install all environments we have, this is probably the version to stick to.","title":"Conda"},{"location":"quest/quest-conda/#using_conda_on_quest","text":"Using conda on Quest Why Conda Setting up Conda on Quest Using Conda Running Nextflow with conda Notes on conda versions on Quest","title":"Using conda on Quest"},{"location":"quest/quest-conda/#why_conda","text":"Computational Reproducibility is the ability to reproduce an analysis exactly. In order for computational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub (for help, see the Github page ). Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software, package and environment managers such as Conda and Docker are very useful. Conda works similarly to brew or pyenv that were used in the legacy Andersen-Lab-Env. Note The software environments on Mac and Linux are not exactly identical...but they are very close.","title":"Why Conda"},{"location":"quest/quest-conda/#setting_up_conda_on_quest","text":"Anaconda is a Python distribution that contains many packages including conda. Miniconda is a more compact version of Anaconda that also includes conda. So in order to install conda, we usually either install Miniconda or Anaconda. On Quest, Anaconda is already installed. However there are many versions of Anaconda, each can have a different version of Python and Conda. The current lab environments mainly used module load python/anaconda3.6 (See Notes below for more info). In your home directory ~/ , create a file called .condarc and put the following lines into it. It sets the channel priority when conda searches for packages. If possible, in one environment it is good to use packages from the same channel. channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /projects/b1059/software/conda_envs/","title":"Setting up Conda on Quest"},{"location":"quest/quest-conda/#using_conda","text":"Conda Documentation Conda Cheatsheet After loading the Anaconda module, one can create an environment and install packages into that environment: # Create conda environment named \"name_of_env\" conda create --name name_of_env # Create conda environment in a specific folder conda create -p /projects/b1059/software/conda_envs/test # activate conda environment # for some `conda activate` works and for others `source activate` conda activate test source activate test # install package into conda environment conda install bcftools When looking to install a package, one resource to check out is anaconda.org , search for your package and run the install command listed on the page. Note Remember to keep in mind the different versions of software/packages. You could have bcftools-v1.10 or bcftools-v.1.12, so make sure you install the correct one! Important You can also install R packages with conda, however conda and R don't always work well together. Check out our quick fix here","title":"Using Conda"},{"location":"quest/quest-conda/#running_nextflow_with_conda","text":"When running Nextflow, conda environments can be specified as part of a process or in the nextflow.config file to apply to the entire pipeline (check out the documentation ): Conda within a process: process foo { conda '/projects/b1059/software/conda_envs/cegwas2-nf_env' ''' your_command --here ''' } Conda for the entire pipeline: // in the nextflow.config file: conda { conda.enabled = true conda.cacheDir = \".env\" } process { conda = \"/projects/b1059/software/conda_envs/cegwas2-nf_env\" }","title":"Running Nextflow with conda"},{"location":"quest/quest-conda/#notes_on_conda_versions_on_quest","text":"tl;dr; If having trouble with conda, or Nextflow gives conda-related errors, try to load a different version of anaconda on Quest. At some point it may be worth re-creating all conda environments in the lab with a consistent version of conda. As of the end of 2020, existing conda environments for the lab were mostly created by module load python/anaconda (which got automatically loaded with module git by accident). It loads Python version 2.7.18 and conda 4.5.2. The other environments were created with module load python/anaconda3.6 which loads Python 3.6.0 and conda 4.3.30. To see versions, use conda info or conda -V . Once you activate an environment with conda activate env_name or source activate env_name , the default conda usually get re-directed to the conda that were originally used to create the environment. This is good because it helps ensure that all packages in the same environment uses the same version of conda. One can go to cd ~/.conda/env_name/bin and readlink -f conda or readlink -f activate to see which version of conda is used by this environment. This is exactly how Nextflow determines which conda to use when using an existing conda environment. Quest people recommended module load python-anaconda3 , but that version does not mix well with the versions mentioned above. But if one were to re-install all environments we have, this is probably the version to stick to.","title":"Notes on conda versions on Quest"},{"location":"quest/quest-intro/","text":"Introduction \u00b6 Introduction New Users Signing into Quest Login Nodes Home Directory Projects and Queues Running interactive jobs on Quest Using screen or nohup to keep jobs from timing out Using packages already installed on Quest Submitting jobs to Quest Monitoring SLURM Jobs on Quest The Andersen Lab makes use of Quest, the supercomputer at Northwestern, although it has almost fully transitioned to Johns Hopkins' Rockfish . Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up: Quest Documentation Note New to the command line? Check out the quick and dirty bash intro or this introduction first! New Users \u00b6 To gain access to Quest: Register new user with your NetID here . Apply to be added to partition b1059 here . Allocation manager: Erik Andersen, erik.andersen@northwestern.edu Apply to be added to partition b1042 here . Allocation manager: Janna Nugent, janna.nugent@northwestern.edu Quest has its Slack channel: genomics-rcs.slack.com ( here ) and help email: quest-help@northwestern.edu for users to get help. Signing into Quest \u00b6 After you gain access to the cluster you can login using: ssh <netid>@quest.it.northwestern.edu To avoid typing in the password everytime, one can set up a ssh key . I recommend setting an alias in your ~/.bash_profile to make logging in quicker: alias quest=\"ssh <netid>@quest.it.northwestern.edu\" The above line makes it so you simply type quest and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below). Login Nodes \u00b6 There are four login nodes we use: quser21-24. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired ( e.g. ssh quser21 ). Warning When using screen to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those screen sessions. Home Directory \u00b6 Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 80 Gb. More information on quotas, storage, etc . You can check your storgae space using the following command: du -hs * More information is provided below to help install and use software. Projects and Queues \u00b6 Quest is broadly organized into projects (or partitions). Projects have associated with them storage, nodes, and users. The Andersen lab has access to two projects. b1042 - The 'Genomics' Project has 200 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at /projects/b1042/AndersenLab/ . By default, files are deleted after 30 days. One can submit jobs to --partition=genomicsguestA to use this partition, with a max job duration of 48hr. b1059 - The Andersen Lab Project. b1059 has 5 computing nodes qnode9031 - qnode9033 (192Gb of RAM and 40 cores eacy) and qnode0277 - qnode0278 (192Gb of RAM and 52 cores each), and has 77 Tb of storage. b1059 storage is located at: /projects/b1059/ (Check out this page to learn more about how data is stored on b1059 ). One can submit jobs to --partition=b1059 to use this partition, with no limit on job duration. Note Anyone who uses quest should build your own project folder under /projects/b1059/projects with your name. You should only write and revise files under your project folder. You can read/copy data from b1059 but don't write any data out of your project folder. Important It is important that we keep the 77 Tb of storage space on b1059 from filling up with extraneous or intermediate files. It is good practice to clean up old data/files and backup important data at least every few months. You can check the percent of space remaining with checkproject b1059 Running interactive jobs on Quest \u00b6 If you are running a few simple commands or want to experiment with files directly you can start an interactive session on Quest. The command below will give you access to a node where you can run your commands srun -A b1042 --partition=genomicsguestA -N 1 -n 24 --mem=64G --time=12:00:00 --pty bash -i Important Do not run commands for big data on quser21-24 . These are login nodes and are not meant for running heavy-load workflows. Using screen or nohup to keep jobs from timing out \u00b6 If you have ever tried to run a pipeline or script that takes a long time (think NemaScan ), you know that if you close down your terminal or if your QUEST session logs out, it will cause your jobs to end prematurely. There are several ways to avoid this: screen Perhaps the most common way to deal with scripts that run for a long time is screen . For the most simple case use, type screen to open a new screen session and then run your script like normal. Below are some more intermediate commands for taking full advantage of screen : screen -S <some_descriptive_name> : Use this command to name your screen session. Especially useful if you have several scren sessions running and/or want to get back to this particular one later. Ctrl+a follwed by Ctrl+d to detach from the current screen session (NOT Ctrl+a+d !) exit to end the current screen session screen -ls lists the IDs of all screen sessions currently running screen -r <screen_id> : Use this command to resume a particular screen session. If you only have one session running you can simply use screen -r Important When using screen on QUEST, take note that screen sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh quser22 ). nohup Another way to avoid SIGHUP errors is to use nohup . nohup is very simple to use, simply type nohup before your normal command and that's it! nohup nextflow run main.nf Running a command with nohup will look a little different than usual because it will not print anything to the console. Instead, it prints all console outputs into a file in the current directory called nohup.out . You can redirect the output to another filename using: nohup nextflow run main.nf > cegwas_{date}_output.txt When you exit this window and open a new session, you can always look at the contents of the output file using cat nohup.out to see the progress. Note You can also run nohup in the background to continue using the same window for other processes by running nohup nextflow run main.nf & . Using packages already installed on Quest \u00b6 Quest has a collection of packages installed. You can run module avail to see what packages are currently available on Quest. You can use module load bcftools or module load bcftools/1.10.1 to load packages with the default or a specific version ( module add does the same thing). If you do echo $PATH before and after loading modules, you can see what module does is simply appending paths to the packages into your $PATH so the packages can be found in your environment. module purge will remove all loaded packages and can be helpful to try when troubleshooting. Note You can also load/install software packages into conda environments for use with a specific project/pipeline. Check out the conda tutorial here . Submitting jobs to Quest \u00b6 Jobs on Quest are managed by SLURM . While most of our pipelines are managed by Nextflow, it is useful to know how to submit jobs directly to SLURM. Below is a template to submit an array job to SLURM that will run 10 parallel jobs. One can run it with sbatch script.sh . If you dig into the Nextflow working directory, you can see Nextflow actually generates such scripts and submits them to SLURM on your behalf. Thanks, Nextflow! #!/bin/bash #SBATCH -J name # Name of job #SBATCH -A b1042 # Allocation #SBATCH -p genomicsguestA # Queue #SBATCH -t 24:00:00 # Walltime/duration of the job (hh:mm:ss) #SBATCH --cpus-per-task=1 # Number of cores (= processors = cpus) for each task #SBATCH --mem-per-cpu=3G # Memory per core needed for a job #SBATCH --array=0-9 # number of parallel jobs to run counting from 0. Make sure to change this to match total number of files. # make a list of all the files you want to process # this script will run a parallel process for each file in this list name_list=(*.bam) # take the nth ($SGE_TASK_ID-th) file in name_list # don't change this line Input=${name_list[$SLURM_ARRAY_TASK_ID]} # then do your operation on this file Output=`echo $Input | sed 's/.bam/.sam/'` Monitoring SLURM Jobs on Quest \u00b6 Here are some commmon commands used to interact with SLURM and check on job status. For more, check out this page . squeue -u <netid> to check all jobs of a user. scancel <jobid> to cancel a job. scancel {30000..32000} to cancel a range of jobs (job id between 30000 and 32000). sinfo | grep b1059 to check status of nodes. alloc means all cores on a node are completed engaged; mix means some cores on a node are engaged; idle means all cores on a node are available to accept jobs.","title":"Introduction"},{"location":"quest/quest-intro/#introduction","text":"Introduction New Users Signing into Quest Login Nodes Home Directory Projects and Queues Running interactive jobs on Quest Using screen or nohup to keep jobs from timing out Using packages already installed on Quest Submitting jobs to Quest Monitoring SLURM Jobs on Quest The Andersen Lab makes use of Quest, the supercomputer at Northwestern, although it has almost fully transitioned to Johns Hopkins' Rockfish . Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up: Quest Documentation Note New to the command line? Check out the quick and dirty bash intro or this introduction first!","title":"Introduction"},{"location":"quest/quest-intro/#new_users","text":"To gain access to Quest: Register new user with your NetID here . Apply to be added to partition b1059 here . Allocation manager: Erik Andersen, erik.andersen@northwestern.edu Apply to be added to partition b1042 here . Allocation manager: Janna Nugent, janna.nugent@northwestern.edu Quest has its Slack channel: genomics-rcs.slack.com ( here ) and help email: quest-help@northwestern.edu for users to get help.","title":"New Users"},{"location":"quest/quest-intro/#signing_into_quest","text":"After you gain access to the cluster you can login using: ssh <netid>@quest.it.northwestern.edu To avoid typing in the password everytime, one can set up a ssh key . I recommend setting an alias in your ~/.bash_profile to make logging in quicker: alias quest=\"ssh <netid>@quest.it.northwestern.edu\" The above line makes it so you simply type quest and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below).","title":"Signing into Quest"},{"location":"quest/quest-intro/#login_nodes","text":"There are four login nodes we use: quser21-24. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired ( e.g. ssh quser21 ). Warning When using screen to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those screen sessions.","title":"Login Nodes"},{"location":"quest/quest-intro/#home_directory","text":"Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 80 Gb. More information on quotas, storage, etc . You can check your storgae space using the following command: du -hs * More information is provided below to help install and use software.","title":"Home Directory"},{"location":"quest/quest-intro/#projects_and_queues","text":"Quest is broadly organized into projects (or partitions). Projects have associated with them storage, nodes, and users. The Andersen lab has access to two projects. b1042 - The 'Genomics' Project has 200 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at /projects/b1042/AndersenLab/ . By default, files are deleted after 30 days. One can submit jobs to --partition=genomicsguestA to use this partition, with a max job duration of 48hr. b1059 - The Andersen Lab Project. b1059 has 5 computing nodes qnode9031 - qnode9033 (192Gb of RAM and 40 cores eacy) and qnode0277 - qnode0278 (192Gb of RAM and 52 cores each), and has 77 Tb of storage. b1059 storage is located at: /projects/b1059/ (Check out this page to learn more about how data is stored on b1059 ). One can submit jobs to --partition=b1059 to use this partition, with no limit on job duration. Note Anyone who uses quest should build your own project folder under /projects/b1059/projects with your name. You should only write and revise files under your project folder. You can read/copy data from b1059 but don't write any data out of your project folder. Important It is important that we keep the 77 Tb of storage space on b1059 from filling up with extraneous or intermediate files. It is good practice to clean up old data/files and backup important data at least every few months. You can check the percent of space remaining with checkproject b1059","title":"Projects and Queues"},{"location":"quest/quest-intro/#running_interactive_jobs_on_quest","text":"If you are running a few simple commands or want to experiment with files directly you can start an interactive session on Quest. The command below will give you access to a node where you can run your commands srun -A b1042 --partition=genomicsguestA -N 1 -n 24 --mem=64G --time=12:00:00 --pty bash -i Important Do not run commands for big data on quser21-24 . These are login nodes and are not meant for running heavy-load workflows.","title":"Running interactive jobs on Quest"},{"location":"quest/quest-intro/#using_screen_or_nohup_to_keep_jobs_from_timing_out","text":"If you have ever tried to run a pipeline or script that takes a long time (think NemaScan ), you know that if you close down your terminal or if your QUEST session logs out, it will cause your jobs to end prematurely. There are several ways to avoid this: screen Perhaps the most common way to deal with scripts that run for a long time is screen . For the most simple case use, type screen to open a new screen session and then run your script like normal. Below are some more intermediate commands for taking full advantage of screen : screen -S <some_descriptive_name> : Use this command to name your screen session. Especially useful if you have several scren sessions running and/or want to get back to this particular one later. Ctrl+a follwed by Ctrl+d to detach from the current screen session (NOT Ctrl+a+d !) exit to end the current screen session screen -ls lists the IDs of all screen sessions currently running screen -r <screen_id> : Use this command to resume a particular screen session. If you only have one session running you can simply use screen -r Important When using screen on QUEST, take note that screen sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh quser22 ). nohup Another way to avoid SIGHUP errors is to use nohup . nohup is very simple to use, simply type nohup before your normal command and that's it! nohup nextflow run main.nf Running a command with nohup will look a little different than usual because it will not print anything to the console. Instead, it prints all console outputs into a file in the current directory called nohup.out . You can redirect the output to another filename using: nohup nextflow run main.nf > cegwas_{date}_output.txt When you exit this window and open a new session, you can always look at the contents of the output file using cat nohup.out to see the progress. Note You can also run nohup in the background to continue using the same window for other processes by running nohup nextflow run main.nf & .","title":"Using screen or nohup to keep jobs from timing out"},{"location":"quest/quest-intro/#using_packages_already_installed_on_quest","text":"Quest has a collection of packages installed. You can run module avail to see what packages are currently available on Quest. You can use module load bcftools or module load bcftools/1.10.1 to load packages with the default or a specific version ( module add does the same thing). If you do echo $PATH before and after loading modules, you can see what module does is simply appending paths to the packages into your $PATH so the packages can be found in your environment. module purge will remove all loaded packages and can be helpful to try when troubleshooting. Note You can also load/install software packages into conda environments for use with a specific project/pipeline. Check out the conda tutorial here .","title":"Using packages already installed on Quest"},{"location":"quest/quest-intro/#submitting_jobs_to_quest","text":"Jobs on Quest are managed by SLURM . While most of our pipelines are managed by Nextflow, it is useful to know how to submit jobs directly to SLURM. Below is a template to submit an array job to SLURM that will run 10 parallel jobs. One can run it with sbatch script.sh . If you dig into the Nextflow working directory, you can see Nextflow actually generates such scripts and submits them to SLURM on your behalf. Thanks, Nextflow! #!/bin/bash #SBATCH -J name # Name of job #SBATCH -A b1042 # Allocation #SBATCH -p genomicsguestA # Queue #SBATCH -t 24:00:00 # Walltime/duration of the job (hh:mm:ss) #SBATCH --cpus-per-task=1 # Number of cores (= processors = cpus) for each task #SBATCH --mem-per-cpu=3G # Memory per core needed for a job #SBATCH --array=0-9 # number of parallel jobs to run counting from 0. Make sure to change this to match total number of files. # make a list of all the files you want to process # this script will run a parallel process for each file in this list name_list=(*.bam) # take the nth ($SGE_TASK_ID-th) file in name_list # don't change this line Input=${name_list[$SLURM_ARRAY_TASK_ID]} # then do your operation on this file Output=`echo $Input | sed 's/.bam/.sam/'`","title":"Submitting jobs to Quest"},{"location":"quest/quest-intro/#monitoring_slurm_jobs_on_quest","text":"Here are some commmon commands used to interact with SLURM and check on job status. For more, check out this page . squeue -u <netid> to check all jobs of a user. scancel <jobid> to cancel a job. scancel {30000..32000} to cancel a range of jobs (job id between 30000 and 32000). sinfo | grep b1059 to check status of nodes. alloc means all cores on a node are completed engaged; mix means some cores on a node are engaged; idle means all cores on a node are available to accept jobs.","title":"Monitoring SLURM Jobs on Quest"},{"location":"quest/quest-nextflow/","text":"Nextflow \u00b6 Nextflow Installation Quest cluster configuration Global Configuration: ~/.nextflow/config Running Nextflow Running Nextflow from a remote directory Running a different branch of a pipeline Running a specific commit Troubleshooting remote pipelines Running a custom Nextflow pipeline version Resume Getting an email or text when complete Caching singularity images on QUEST Writing Nextflow Pipelines Nextflow is an awesome program that allows you to write a computational pipeline by making it simpler to put together many different tasks, maybe even using different programming languages. Nextflow makes parallelism easy and works with SLURM to schedule jobs so you don't have to! Nextflow also supports docker containers and conda enviornments to streamline reproducible pipelines and can be easily adapted to run on different systems - including Google Cloud! Not convinced? Check out this intro . Installation \u00b6 There are two primary ways to get Nextflow on QUEST. Load the pre-built module Load the pre-build conda environment All of the lab Nextflow pipelines are currently written to work with Nextflow version 20 (legacy, pre-April 2024) or version 23. Version 20 is available as a conda environment: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Nextflow version 23 is available as a module: module load nextflow/23.10.1 Important If you run the nf20 conda environment, please do not try to update nextflow, this will cause permission denied errors for other lab members . You can deactivate the conda environment with source deactivate (You can check your Nextflow version with nextflow -v or nextflow --version ) Note If you load this conda environment, it is not necessary to have Nextflow version 20 installed on your system -- you don't even need to have Nextflow installed at all! Because different versions might have updates that affect the running of Nextflow, it is important to keep track of the version of Nextflow you are using, as well as all software packages. Quest cluster configuration \u00b6 Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config . Global Configuration: ~/.nextflow/config \u00b6 In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { executor = 'slurm' queue = 'genomicsguestA' clusterOptions = '-A b1042 -t 24:00:00 -e errlog.txt' } workDir = \"/projects/b1042/AndersenLab/work/<your folder>\" tmpDir = \"/projects/b1042/AndersenLab/tmp\" This configuration file does the following: Sets the executor to slurm (which is what Quest uses) Sets the queue to genomicsguestA which submits jobs to genomics nodes. The genomicsguestA will submit jobs to our dedicated nodes first, which we have high priority. If our dedicated nodes are full, it will submit to other nodes we don't have priority. So far, our lab have 2 dedicated nodes, with 28 cores and related memory (close to 1:5) for each dedicated node. We will have more in the future. clusterOptions - Sets the account to b1042 ; granting access to genomics-dedicated scratch space. workDir - Sets the working directory to scratch space on b1042. To better organization, Please build your own folder under /projects/b1042/AndersenLab/work/ , and define it here. Note: replace '< your folder >' with your name tmpDir - Creates a temporary working directory. This can be used within workflows when necessary. Running Nextflow \u00b6 Theoretically, running a Nextflow pipeline should be very straightforward (although with any developing pipelines there are always bugs to work out). Running Nextflow from a remote directory \u00b6 The prefered (and sometimes easiest) way to run a nextflow pipeline can be done in just one single step. When this works (see troubleshooting section below), pipelines can be run without first cloning the git repo. You can just tell Nextflow which git repo to use and it will do the rest! This can be helpful to reduce clutter and avoid making changes to the actual pipeline. Additionally, this method allows you to control which branch and/or commit of the pipeline to run, and Nextflow will automatically track that information for you allowing for great reproducibility. Note There is no need to clone a copy of the repo to run pipelines this way. Behind the scenes, nextflow is actually cloning the repo to your home directory and keeping track of branches/commits. # example command to run the latest version of NemaScan nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 # Note: can also write in one line, the \\ were used to make the code more readable: nextflow run andersenlab/nemascan --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv --vcf 20210121 Note Parameters or arguments to Nextflow scripts are designated with the -- . In the case above, there is a parameter called \"in\" which we are setting to be the test.tsv file. When Nextflow is running, it will print to the console the name of each process in the pipeline as well as update on the progress of the script: For example, in the above screenshot from a NemaScan run, there are 14 different processes in the pipeline. Notice that the fix_strain_names_bulk process has already completed! Meanwhile, the vcf_t_geno_matrix process is still running. You can also see that the prepare_gcta_files is actually run 4 times (in this case, because it is run once per 4 traits in my dataset). Another important piece of information from this Nextflow run is the hash that designates the working directory of each process. the [ad/eea615] next to the fix_strain_names_bulk process indicates that the working directory is located at path_to_nextflow_working_directory/ad/eea615... . This can be helpful if you want to go into that directory to see what is actually happening or trouble shoot errors. Note I highly recommend adding this function to your ~/.bash_profile to easily access the Nexflow working directory: gw() {cd /projects/b1042/AndersenLab/work/<your_name>/$1*} so that when you type gw 3f/6a21a5 (the hash Nextflow shows that indicates the specific working directory for a process) you will go to that folder automatically. Running a different branch of a pipeline \u00b6 Instead of cloning the repo and then remembering to switch branches, you can specify the branch in your nextflow call. This is most beneficial because the pipeline will then keep a record of which branch you used so future you doesn't have to remember. # the following command runs the \"develop\" branch of nemascan # (not recommended unless you know what you are doing, might break) nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r develop Running a specific commit \u00b6 Sometimes we want to keep the version of a pipeline the same across several different runs (maybe preparing for a manuscript etc.). To do this, you can again use the -r argument and just provide the commit ID which is a long alphanumeric code that can be found on github or in the log of your previous nextflow run. # the following command runs will always run the exact same code, no matter how many changes to nemascan there are in the future nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r 769a2b75545d7f870a880447dd31482cfc628792 Troubleshooting remote pipelines \u00b6 For some reason, running nextflow remotely sometimes throws errors and I am not 100% sure why. Here are some of the most common errors I see and how to fix them. 1. It doesn't pull the latest commit Go to github.com and find the latest commit ID and use that with -r XXX (see above on running specific commits) If that doesn't work, you might have to clone the repo and run it locally (see below) 2. It says the \"repository may be corrupt\" I don't understand this error, but I think it happens when you try to run different branches/commits. Usually, I just go to the path where the error says is corrupt and delete the folder with rm -rf . For example: rm -rf /home/kek973/.nextflow/assets/andersenlab/nemascan Warning You should always be careful when using rm , especially rm -rf . There is no going back. Make certain you want to delete the folder and everything inside it. In this case, it will be fine because nextflow will just clone it again fresh. Running a custom Nextflow pipeline version \u00b6 If you need to run a custom version of a pipeline, there are two approached you can use. Clone the repo onto Quest and make local changes. Then run Nextflow from the local pipeline. Clone the repo onto your computer, create a new branch, and make changes to your new branch. Then push those changes and run Nextflow on your branch of the pipeline. You should NOT do the first, as this means that your analysis will not be reproducible the moment you delete that local repo, nor will it be available to other people. By creating your own branch, you are able to track changes and allow others to run the same code as you with only a couple simple extra steps. On your machine (or technically Quest if you want, but I recommend locally) where is whatever branch name you want: git clone https://github.com/AndersenLab/NemaScan.git cd NemaScan git checkout -b <your-branch> ## Make your local changes ## git commit -a -m \"This is my branch!\" # or some more meaningful message git push --set-upstream origin <your-branch> To run your custom pipeline on Quest: nextflow run andersenlab/nemascan -r <your-branch> ... # plus whatever arguments and parameters are needed for this pipeline Resume \u00b6 Another great thing about Nextflow is that it caches all the processes that completed successfully, so if you have an error at the middle or end of your pipeline, you can re-run the same Nextflow command with -resume and it will start where it finished last time! nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -resume There is usually no downside to adding -resume , so you can get into the habit of always adding it if you want. Important There is some confusion with how the -resume works and sometimes it doesn't work as expected. Check out this and this other guide for more help. One thing I've learned is there is a difference between process.cache = 'deep' vs. 'lenient' . Getting an email or text when complete \u00b6 If you would like to receive an email or text from Nextflow when your pipeline finishes (either successfully or with error), all you need to do is add -N <email> to your code. Most phone companies have a way to \"email\" your phone as an SMS, so you can use this email address to get a text alert. For example: # send email nextflow run andersenlab/nemascan --debug -N kathryn.evans@northwestern.edu # send text to 801-867-5309 with verizon nextflow run andersenlab/nemascan --debut -N 8018675309@vtext.com Caching singularity images on QUEST \u00b6 To make the most out of using a shared cache directory for singularity on b1059, make sure to add this line to your ~/.bash_profile before you run a pipeline for the first time (Note: this is not needed to USE a previously cached image, but only when you ADD a new one). export SINGULARITY_CACHEDIR='/projects/b1059/singularity/' Writing Nextflow Pipelines \u00b6 See this page for tips on how to get started with your own nextflow pipeline. Also check out the Nextflow documentation for help getting started!","title":"Nextflow"},{"location":"quest/quest-nextflow/#nextflow","text":"Nextflow Installation Quest cluster configuration Global Configuration: ~/.nextflow/config Running Nextflow Running Nextflow from a remote directory Running a different branch of a pipeline Running a specific commit Troubleshooting remote pipelines Running a custom Nextflow pipeline version Resume Getting an email or text when complete Caching singularity images on QUEST Writing Nextflow Pipelines Nextflow is an awesome program that allows you to write a computational pipeline by making it simpler to put together many different tasks, maybe even using different programming languages. Nextflow makes parallelism easy and works with SLURM to schedule jobs so you don't have to! Nextflow also supports docker containers and conda enviornments to streamline reproducible pipelines and can be easily adapted to run on different systems - including Google Cloud! Not convinced? Check out this intro .","title":"Nextflow"},{"location":"quest/quest-nextflow/#installation","text":"There are two primary ways to get Nextflow on QUEST. Load the pre-built module Load the pre-build conda environment All of the lab Nextflow pipelines are currently written to work with Nextflow version 20 (legacy, pre-April 2024) or version 23. Version 20 is available as a conda environment: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Nextflow version 23 is available as a module: module load nextflow/23.10.1 Important If you run the nf20 conda environment, please do not try to update nextflow, this will cause permission denied errors for other lab members . You can deactivate the conda environment with source deactivate (You can check your Nextflow version with nextflow -v or nextflow --version ) Note If you load this conda environment, it is not necessary to have Nextflow version 20 installed on your system -- you don't even need to have Nextflow installed at all! Because different versions might have updates that affect the running of Nextflow, it is important to keep track of the version of Nextflow you are using, as well as all software packages.","title":"Installation"},{"location":"quest/quest-nextflow/#quest_cluster_configuration","text":"Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config .","title":"Quest cluster configuration"},{"location":"quest/quest-nextflow/#global_configuration_nextflowconfig","text":"In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { executor = 'slurm' queue = 'genomicsguestA' clusterOptions = '-A b1042 -t 24:00:00 -e errlog.txt' } workDir = \"/projects/b1042/AndersenLab/work/<your folder>\" tmpDir = \"/projects/b1042/AndersenLab/tmp\" This configuration file does the following: Sets the executor to slurm (which is what Quest uses) Sets the queue to genomicsguestA which submits jobs to genomics nodes. The genomicsguestA will submit jobs to our dedicated nodes first, which we have high priority. If our dedicated nodes are full, it will submit to other nodes we don't have priority. So far, our lab have 2 dedicated nodes, with 28 cores and related memory (close to 1:5) for each dedicated node. We will have more in the future. clusterOptions - Sets the account to b1042 ; granting access to genomics-dedicated scratch space. workDir - Sets the working directory to scratch space on b1042. To better organization, Please build your own folder under /projects/b1042/AndersenLab/work/ , and define it here. Note: replace '< your folder >' with your name tmpDir - Creates a temporary working directory. This can be used within workflows when necessary.","title":"Global Configuration: ~/.nextflow/config"},{"location":"quest/quest-nextflow/#running_nextflow","text":"Theoretically, running a Nextflow pipeline should be very straightforward (although with any developing pipelines there are always bugs to work out).","title":"Running Nextflow"},{"location":"quest/quest-nextflow/#running_nextflow_from_a_remote_directory","text":"The prefered (and sometimes easiest) way to run a nextflow pipeline can be done in just one single step. When this works (see troubleshooting section below), pipelines can be run without first cloning the git repo. You can just tell Nextflow which git repo to use and it will do the rest! This can be helpful to reduce clutter and avoid making changes to the actual pipeline. Additionally, this method allows you to control which branch and/or commit of the pipeline to run, and Nextflow will automatically track that information for you allowing for great reproducibility. Note There is no need to clone a copy of the repo to run pipelines this way. Behind the scenes, nextflow is actually cloning the repo to your home directory and keeping track of branches/commits. # example command to run the latest version of NemaScan nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 # Note: can also write in one line, the \\ were used to make the code more readable: nextflow run andersenlab/nemascan --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv --vcf 20210121 Note Parameters or arguments to Nextflow scripts are designated with the -- . In the case above, there is a parameter called \"in\" which we are setting to be the test.tsv file. When Nextflow is running, it will print to the console the name of each process in the pipeline as well as update on the progress of the script: For example, in the above screenshot from a NemaScan run, there are 14 different processes in the pipeline. Notice that the fix_strain_names_bulk process has already completed! Meanwhile, the vcf_t_geno_matrix process is still running. You can also see that the prepare_gcta_files is actually run 4 times (in this case, because it is run once per 4 traits in my dataset). Another important piece of information from this Nextflow run is the hash that designates the working directory of each process. the [ad/eea615] next to the fix_strain_names_bulk process indicates that the working directory is located at path_to_nextflow_working_directory/ad/eea615... . This can be helpful if you want to go into that directory to see what is actually happening or trouble shoot errors. Note I highly recommend adding this function to your ~/.bash_profile to easily access the Nexflow working directory: gw() {cd /projects/b1042/AndersenLab/work/<your_name>/$1*} so that when you type gw 3f/6a21a5 (the hash Nextflow shows that indicates the specific working directory for a process) you will go to that folder automatically.","title":"Running Nextflow from a remote directory"},{"location":"quest/quest-nextflow/#running_a_different_branch_of_a_pipeline","text":"Instead of cloning the repo and then remembering to switch branches, you can specify the branch in your nextflow call. This is most beneficial because the pipeline will then keep a record of which branch you used so future you doesn't have to remember. # the following command runs the \"develop\" branch of nemascan # (not recommended unless you know what you are doing, might break) nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r develop","title":"Running a different branch of a pipeline"},{"location":"quest/quest-nextflow/#running_a_specific_commit","text":"Sometimes we want to keep the version of a pipeline the same across several different runs (maybe preparing for a manuscript etc.). To do this, you can again use the -r argument and just provide the commit ID which is a long alphanumeric code that can be found on github or in the log of your previous nextflow run. # the following command runs will always run the exact same code, no matter how many changes to nemascan there are in the future nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r 769a2b75545d7f870a880447dd31482cfc628792","title":"Running a specific commit"},{"location":"quest/quest-nextflow/#troubleshooting_remote_pipelines","text":"For some reason, running nextflow remotely sometimes throws errors and I am not 100% sure why. Here are some of the most common errors I see and how to fix them. 1. It doesn't pull the latest commit Go to github.com and find the latest commit ID and use that with -r XXX (see above on running specific commits) If that doesn't work, you might have to clone the repo and run it locally (see below) 2. It says the \"repository may be corrupt\" I don't understand this error, but I think it happens when you try to run different branches/commits. Usually, I just go to the path where the error says is corrupt and delete the folder with rm -rf . For example: rm -rf /home/kek973/.nextflow/assets/andersenlab/nemascan Warning You should always be careful when using rm , especially rm -rf . There is no going back. Make certain you want to delete the folder and everything inside it. In this case, it will be fine because nextflow will just clone it again fresh.","title":"Troubleshooting remote pipelines"},{"location":"quest/quest-nextflow/#running_a_custom_nextflow_pipeline_version","text":"If you need to run a custom version of a pipeline, there are two approached you can use. Clone the repo onto Quest and make local changes. Then run Nextflow from the local pipeline. Clone the repo onto your computer, create a new branch, and make changes to your new branch. Then push those changes and run Nextflow on your branch of the pipeline. You should NOT do the first, as this means that your analysis will not be reproducible the moment you delete that local repo, nor will it be available to other people. By creating your own branch, you are able to track changes and allow others to run the same code as you with only a couple simple extra steps. On your machine (or technically Quest if you want, but I recommend locally) where is whatever branch name you want: git clone https://github.com/AndersenLab/NemaScan.git cd NemaScan git checkout -b <your-branch> ## Make your local changes ## git commit -a -m \"This is my branch!\" # or some more meaningful message git push --set-upstream origin <your-branch> To run your custom pipeline on Quest: nextflow run andersenlab/nemascan -r <your-branch> ... # plus whatever arguments and parameters are needed for this pipeline","title":"Running a custom Nextflow pipeline version"},{"location":"quest/quest-nextflow/#resume","text":"Another great thing about Nextflow is that it caches all the processes that completed successfully, so if you have an error at the middle or end of your pipeline, you can re-run the same Nextflow command with -resume and it will start where it finished last time! nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -resume There is usually no downside to adding -resume , so you can get into the habit of always adding it if you want. Important There is some confusion with how the -resume works and sometimes it doesn't work as expected. Check out this and this other guide for more help. One thing I've learned is there is a difference between process.cache = 'deep' vs. 'lenient' .","title":"Resume"},{"location":"quest/quest-nextflow/#getting_an_email_or_text_when_complete","text":"If you would like to receive an email or text from Nextflow when your pipeline finishes (either successfully or with error), all you need to do is add -N <email> to your code. Most phone companies have a way to \"email\" your phone as an SMS, so you can use this email address to get a text alert. For example: # send email nextflow run andersenlab/nemascan --debug -N kathryn.evans@northwestern.edu # send text to 801-867-5309 with verizon nextflow run andersenlab/nemascan --debut -N 8018675309@vtext.com","title":"Getting an email or text when complete"},{"location":"quest/quest-nextflow/#caching_singularity_images_on_quest","text":"To make the most out of using a shared cache directory for singularity on b1059, make sure to add this line to your ~/.bash_profile before you run a pipeline for the first time (Note: this is not needed to USE a previously cached image, but only when you ADD a new one). export SINGULARITY_CACHEDIR='/projects/b1059/singularity/'","title":"Caching singularity images on QUEST"},{"location":"quest/quest-nextflow/#writing_nextflow_pipelines","text":"See this page for tips on how to get started with your own nextflow pipeline. Also check out the Nextflow documentation for help getting started!","title":"Writing Nextflow Pipelines"},{"location":"rockfish/VAST/","text":"Data storage on Rockfish \u00b6 Data storage on Rockfish /vast/eande106 (vast-eande106) data (vast-eande106) docs (vast-eande106) projects (vast-eande106) singularity (vast-eande106) to_be_deleted (vast-eande106) workflows (vast-eande106) /data/eande106 (data-eande106) analysis (data-eande106) software (data-eande106) /scratch4/eande106 (scr4-eande106) In 2023, the Andersen lab began moving its computing resources from QUEST at Northwestern to Rockfish at Johns Hopkins. The goal was to have a seamless transition by maintaining the file system and structure in Rockfish as similar as possible to QUEST. The primary file system that the Andersen lab uses in Rockfish is called VAST. The VAST partition purchased by the lab has a file quota of 120TB. In addition, the lab has access to two secondary partitions: data-eande106 and scr4-eande106 , with file quotas of 10TB and 1TB respectively. Once granted access to the Andersen lab security group and allocation, you will find symbolic links for vast-eande106 , data-eande106 , and scr4-eande106 partitions attached to your home directory (e.g. /home/<user>/vast-eande106 ). Symbolic links act like shortcuts that will allow you to access all partitions without having to use intricate file paths that point to the true location of the partitions in the Rockfish file system. /vast/eande106 (vast-eande106) \u00b6 vast-eande106 is where the vast (hehe, get it?) majority of files will exist. A breakdown of the directories of this partition is described below. Files stored in vast cannot have special characters (e.g. !@#$^&*()?:;) and have a few protected directory names that are not allowed (e.g. aux , prn , con ) data (vast-eande106) \u00b6 This directory contains all the lab data -- split by species. The goal is to create the same file structure across all species. This not only makes it easy to find the file you are looking for, but also makes it easy to script file locations. A general example of a species' file structure is below: \u251c\u2500\u2500 genomes \u2502 \u251c\u2500\u2500 genetic_map \u2502 \u2502 \u2514\u2500\u2500 {chr}.map \u2502 \u251c\u2500\u2500 {project_ID} \u2502 \u2502 \u2514\u2500\u2500 {WS_build} \u2502 \u2502 \u251c\u2500\u2500 *.genome.fa* \u2502 \u2502 \u251c\u2500\u2500 csq \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Length.tsv \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Scores.tsv \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.csq.gff3.gz \u2502 \u2502 \u251c\u2500\u2500 lcr \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.dust.bed.gz \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.repeat_masker.bed.gz \u2502 \u2502 \u2514\u2500\u2500 snpeff \u2502 \u2502 \u251c\u2500\u2500 {species}.{project}.{ws_build} \u2502 \u2502 \u2502 \u251c\u2500\u2500 genes.gtf.gz \u2502 \u2502 \u2502 \u251c\u2500\u2500 sequences.fa \u2502 \u2502 \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin \u2502 \u2502 \u2514\u2500\u2500 snpEff.config \u2502 \u2514\u2500\u2500 WI_PacBio_assemblies \u251c\u2500\u2500 RIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 NIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 WI \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u251c\u2500\u2500 _bam_not_for_cendr \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 concordance \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 divergent_regions \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u251c\u2500\u2500 dna \u2502 \u2502 \u2502 \u251c\u2500\u2500 _fastq_not_for_cendr \u2502 \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2502 \u2514\u2500\u2500 rna \u2502 \u2502 \u2514\u2500\u2500 {project} \u2502 \u251c\u2500\u2500 haplotype \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tree \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tracks \u2502 \u2514\u2500\u2500 vcf \u2502 \u251c\u2500\u2500 strain_vcf \u2502 \u2502 \u2514\u2500\u2500 {strain}.{date}.vcf.gz \u2502 \u2514\u2500\u2500 WI.{date}.*.vcf.gz \u2514\u2500\u2500 {other - i.e. BSA, MUTANT, HiC, PacBio} \u2514\u2500\u2500 *currently could look like anything in here - organized by project* Note This restructure is still not 100% complete and things might continue to change as we expand our genomes and data types. If you see something that is not organized well, let Erik or Mike know. docs (vast-eande106) \u00b6 This directory should probably be removed and the docs inside stored elsewhere. Currently, shows the process for fastq SRA submission from 2019 and then again from 2021. projects (vast-eande106) \u00b6 Projects is where most people will spend most of their time. Each user with access to quest should create a user-specific folder (named with their name) in the projects directory. Inside your folder, you can do all your work: analyzing data, writing new scripts, temporary storage, etc. Note Please be aware of how much space you are using in your personal folder. Of course some projects might require more space than others, and some projects require a lot of temporary space that can be deleted once completed. However, if you find that you have > 500-1000 GB of used space in your folder, please take a look if there is any unused data or scripts. Either way, it is good practice to clean out your folder every few months to avoid storage pile-up. You can check how much space you are using with du -hs * or ask Katie. singularity (vast-eande106) \u00b6 This directory contains docker images suitable for specific nextflow workflows as well as those created for specific past analyses. to_be_deleted (vast-eande106) \u00b6 This directory is a temporary holding place for large files/data that can be deleted. Think of it as a soft-trash bin. If, after a few months of living in to_be_deleted , it is likely the data in fact can be deleted without being missed. This was a temporary solution for the large restructure in 2021 and can likely be removed in the future. workflows (vast-eande106) \u00b6 This directory is being phased out. Workflows will now be hosted on GitHub and any lab members who wish to run a shared workflow should run remotely (if nextflow script) or clone the repo into their personal folders. /data/eande106 (data-eande106) \u00b6 data-eande106 is where software and pipelines will live. Software and pipeline files often contain special characters (e.g. packages with :: delimiters) or directories with protected names. Any future software must be installed here. A breakdown of the directories of this partition is described below. analysis (data-eande106) \u00b6 This directory contains non-data output and analyses from general lab pipelines (i.e. wi-gatk or alignment-nf ). It is organized by pipeline and then by analysis type-date. If you are running these pipelines (including nil-ril-nf ) it is important that you move your analysis folder here once complete (and out of your personal folder) so that everyone has access to the results. software (data-eande106) \u00b6 General This is a great location to download any software tools or packages that you need to use that you cannot access with Rockfish or create a conda environment for. Especially important if they are software packages that other people in the lab might also use, as it is a shared space. Conda environments Inside the directory conda_envs you can find all the shared lab conda environments necessary for running certain nextflow pipelines. If you create a shared conda environment, it is important that you update the README.md with the code you used to create the environment for reproducible purposes . You can create a conda environment in this directory with: conda create -p /home/<user>/data_eande106/software/conda_envs/<name_of_env> You can also update your ~/.condarc file to point to this directory so that you can easily load conda environments just by the name (i.e. source activate nf23_env instead of source activate /home/<user>/data-eande106/software/conda_envs/nf23_env ): channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /home/<user>/data-eande106/software/conda_envs/ Important It is very important that you do not update any packages or software while running a shared conda environment. This is especially an issue with Nextflow and the nf23_env environment. Updating nextflow whilie running this environment will update the version in the environment -- and it needs to be v23.10 for many of the pipelines to run successfully If you want to see what all is loaded in a particular conda environment, or re-create an environment, you can use: # make an exact copy conda create --clone py35 --name py35-2 # list all packages and versions conda list --explicit > bio-env.txt # list a history of revisions conda list --revisions # go back to a previous revision conda install --revision 2 # create environment from file conda env create --file bio-env.txt Also, check out this cheat sheet or our conda page for more. Note Most of the nextflow pipelines are written using module load anaconda3/2022.05; source activate nf23_env , so if you are having trouble running with conda, try loading the right environment first. R libraries Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. One way we have gotten around this is by installing the proper versions of R packages to a shared location ( /data/eande106/software/R_lib_3.6.0 ). Check out the Using R on Rockfish section of the R page to learn more about installing a package to this folder and using it in a script. Important It is very important that you do not update any packages in this location unless absolutely necessary and proceed with extreme caution (especially if it is tidyverse !!!). Updating packages could break pipelines that rely on them. /scratch4/eande106 (scr4-eande106) \u00b6 scr4-eande106 is a small partition primarily used for scratch space. It's currently contains a series of folders with two-digit hexadecimal numbers that are used for Nextflow working files, and it's supposed to be used only to hold small test files and scripts with the aim to eventually delete such test files and reuse this space.** If you need to use the scratch partition for something other than running nextflow, receate a directory under your own name ( ~/scr4-eande106/<your name> ).","title":"Data on Rockfish"},{"location":"rockfish/VAST/#data_storage_on_rockfish","text":"Data storage on Rockfish /vast/eande106 (vast-eande106) data (vast-eande106) docs (vast-eande106) projects (vast-eande106) singularity (vast-eande106) to_be_deleted (vast-eande106) workflows (vast-eande106) /data/eande106 (data-eande106) analysis (data-eande106) software (data-eande106) /scratch4/eande106 (scr4-eande106) In 2023, the Andersen lab began moving its computing resources from QUEST at Northwestern to Rockfish at Johns Hopkins. The goal was to have a seamless transition by maintaining the file system and structure in Rockfish as similar as possible to QUEST. The primary file system that the Andersen lab uses in Rockfish is called VAST. The VAST partition purchased by the lab has a file quota of 120TB. In addition, the lab has access to two secondary partitions: data-eande106 and scr4-eande106 , with file quotas of 10TB and 1TB respectively. Once granted access to the Andersen lab security group and allocation, you will find symbolic links for vast-eande106 , data-eande106 , and scr4-eande106 partitions attached to your home directory (e.g. /home/<user>/vast-eande106 ). Symbolic links act like shortcuts that will allow you to access all partitions without having to use intricate file paths that point to the true location of the partitions in the Rockfish file system.","title":"Data storage on Rockfish"},{"location":"rockfish/VAST/#vasteande106_vast-eande106","text":"vast-eande106 is where the vast (hehe, get it?) majority of files will exist. A breakdown of the directories of this partition is described below. Files stored in vast cannot have special characters (e.g. !@#$^&*()?:;) and have a few protected directory names that are not allowed (e.g. aux , prn , con )","title":"/vast/eande106 (vast-eande106)"},{"location":"rockfish/VAST/#data_vast-eande106","text":"This directory contains all the lab data -- split by species. The goal is to create the same file structure across all species. This not only makes it easy to find the file you are looking for, but also makes it easy to script file locations. A general example of a species' file structure is below: \u251c\u2500\u2500 genomes \u2502 \u251c\u2500\u2500 genetic_map \u2502 \u2502 \u2514\u2500\u2500 {chr}.map \u2502 \u251c\u2500\u2500 {project_ID} \u2502 \u2502 \u2514\u2500\u2500 {WS_build} \u2502 \u2502 \u251c\u2500\u2500 *.genome.fa* \u2502 \u2502 \u251c\u2500\u2500 csq \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Length.tsv \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Scores.tsv \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.csq.gff3.gz \u2502 \u2502 \u251c\u2500\u2500 lcr \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.dust.bed.gz \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.repeat_masker.bed.gz \u2502 \u2502 \u2514\u2500\u2500 snpeff \u2502 \u2502 \u251c\u2500\u2500 {species}.{project}.{ws_build} \u2502 \u2502 \u2502 \u251c\u2500\u2500 genes.gtf.gz \u2502 \u2502 \u2502 \u251c\u2500\u2500 sequences.fa \u2502 \u2502 \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin \u2502 \u2502 \u2514\u2500\u2500 snpEff.config \u2502 \u2514\u2500\u2500 WI_PacBio_assemblies \u251c\u2500\u2500 RIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 NIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 WI \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u251c\u2500\u2500 _bam_not_for_cendr \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 concordance \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 divergent_regions \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u251c\u2500\u2500 dna \u2502 \u2502 \u2502 \u251c\u2500\u2500 _fastq_not_for_cendr \u2502 \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2502 \u2514\u2500\u2500 rna \u2502 \u2502 \u2514\u2500\u2500 {project} \u2502 \u251c\u2500\u2500 haplotype \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tree \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tracks \u2502 \u2514\u2500\u2500 vcf \u2502 \u251c\u2500\u2500 strain_vcf \u2502 \u2502 \u2514\u2500\u2500 {strain}.{date}.vcf.gz \u2502 \u2514\u2500\u2500 WI.{date}.*.vcf.gz \u2514\u2500\u2500 {other - i.e. BSA, MUTANT, HiC, PacBio} \u2514\u2500\u2500 *currently could look like anything in here - organized by project* Note This restructure is still not 100% complete and things might continue to change as we expand our genomes and data types. If you see something that is not organized well, let Erik or Mike know.","title":"data (vast-eande106)"},{"location":"rockfish/VAST/#docs_vast-eande106","text":"This directory should probably be removed and the docs inside stored elsewhere. Currently, shows the process for fastq SRA submission from 2019 and then again from 2021.","title":"docs (vast-eande106)"},{"location":"rockfish/VAST/#projects_vast-eande106","text":"Projects is where most people will spend most of their time. Each user with access to quest should create a user-specific folder (named with their name) in the projects directory. Inside your folder, you can do all your work: analyzing data, writing new scripts, temporary storage, etc. Note Please be aware of how much space you are using in your personal folder. Of course some projects might require more space than others, and some projects require a lot of temporary space that can be deleted once completed. However, if you find that you have > 500-1000 GB of used space in your folder, please take a look if there is any unused data or scripts. Either way, it is good practice to clean out your folder every few months to avoid storage pile-up. You can check how much space you are using with du -hs * or ask Katie.","title":"projects (vast-eande106)"},{"location":"rockfish/VAST/#singularity_vast-eande106","text":"This directory contains docker images suitable for specific nextflow workflows as well as those created for specific past analyses.","title":"singularity (vast-eande106)"},{"location":"rockfish/VAST/#to_be_deleted_vast-eande106","text":"This directory is a temporary holding place for large files/data that can be deleted. Think of it as a soft-trash bin. If, after a few months of living in to_be_deleted , it is likely the data in fact can be deleted without being missed. This was a temporary solution for the large restructure in 2021 and can likely be removed in the future.","title":"to_be_deleted (vast-eande106)"},{"location":"rockfish/VAST/#workflows_vast-eande106","text":"This directory is being phased out. Workflows will now be hosted on GitHub and any lab members who wish to run a shared workflow should run remotely (if nextflow script) or clone the repo into their personal folders.","title":"workflows (vast-eande106)"},{"location":"rockfish/VAST/#dataeande106_data-eande106","text":"data-eande106 is where software and pipelines will live. Software and pipeline files often contain special characters (e.g. packages with :: delimiters) or directories with protected names. Any future software must be installed here. A breakdown of the directories of this partition is described below.","title":"/data/eande106 (data-eande106)"},{"location":"rockfish/VAST/#analysis_data-eande106","text":"This directory contains non-data output and analyses from general lab pipelines (i.e. wi-gatk or alignment-nf ). It is organized by pipeline and then by analysis type-date. If you are running these pipelines (including nil-ril-nf ) it is important that you move your analysis folder here once complete (and out of your personal folder) so that everyone has access to the results.","title":"analysis (data-eande106)"},{"location":"rockfish/VAST/#software_data-eande106","text":"General This is a great location to download any software tools or packages that you need to use that you cannot access with Rockfish or create a conda environment for. Especially important if they are software packages that other people in the lab might also use, as it is a shared space. Conda environments Inside the directory conda_envs you can find all the shared lab conda environments necessary for running certain nextflow pipelines. If you create a shared conda environment, it is important that you update the README.md with the code you used to create the environment for reproducible purposes . You can create a conda environment in this directory with: conda create -p /home/<user>/data_eande106/software/conda_envs/<name_of_env> You can also update your ~/.condarc file to point to this directory so that you can easily load conda environments just by the name (i.e. source activate nf23_env instead of source activate /home/<user>/data-eande106/software/conda_envs/nf23_env ): channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /home/<user>/data-eande106/software/conda_envs/ Important It is very important that you do not update any packages or software while running a shared conda environment. This is especially an issue with Nextflow and the nf23_env environment. Updating nextflow whilie running this environment will update the version in the environment -- and it needs to be v23.10 for many of the pipelines to run successfully If you want to see what all is loaded in a particular conda environment, or re-create an environment, you can use: # make an exact copy conda create --clone py35 --name py35-2 # list all packages and versions conda list --explicit > bio-env.txt # list a history of revisions conda list --revisions # go back to a previous revision conda install --revision 2 # create environment from file conda env create --file bio-env.txt Also, check out this cheat sheet or our conda page for more. Note Most of the nextflow pipelines are written using module load anaconda3/2022.05; source activate nf23_env , so if you are having trouble running with conda, try loading the right environment first. R libraries Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. One way we have gotten around this is by installing the proper versions of R packages to a shared location ( /data/eande106/software/R_lib_3.6.0 ). Check out the Using R on Rockfish section of the R page to learn more about installing a package to this folder and using it in a script. Important It is very important that you do not update any packages in this location unless absolutely necessary and proceed with extreme caution (especially if it is tidyverse !!!). Updating packages could break pipelines that rely on them.","title":"software (data-eande106)"},{"location":"rockfish/VAST/#scratch4eande106_scr4-eande106","text":"scr4-eande106 is a small partition primarily used for scratch space. It's currently contains a series of folders with two-digit hexadecimal numbers that are used for Nextflow working files, and it's supposed to be used only to hold small test files and scripts with the aim to eventually delete such test files and reuse this space.** If you need to use the scratch partition for something other than running nextflow, receate a directory under your own name ( ~/scr4-eande106/<your name> ).","title":"/scratch4/eande106 (scr4-eande106)"},{"location":"rockfish/rf-conda/","text":"Using conda on Rockfish \u00b6 Using conda on Rockfish Why Conda Setting up Conda on Rockfish Using Conda Running Nextflow with conda Notes on conda versions on Quest vs Rockfish A faster alternative to Conda Why Conda \u00b6 Computational Reproducibility is the ability to reproduce an analysis exactly. In order for computational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub (for help, see the Github page ). Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software, package and environment managers such as Conda and Docker are very useful. Conda works similarly to brew or pyenv that were used in the legacy Andersen-Lab-Env. Note The software environments on Mac and Linux are not exactly identical...but they are very close. Setting up Conda on Rockfish \u00b6 Anaconda is a Python distribution that contains many packages including conda. Miniconda is a more compact version of Anaconda that also includes conda. So in order to install conda, we usually either install Miniconda or Anaconda. On Rockfish, Anaconda is already installed. However there are many versions of Anaconda, each can have a different version of Python and Conda. The current lab environments mainly used module load anaconda3/2022.05 (See Notes below for more info). In your home directory ~/ , create a file called .condarc and put the following lines into it. It sets the channel priority when conda searches for packages. If possible, in one environment it is good to use packages from the same channel. channels: - conda-forge - bioconda - anaconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /home/<jheid>/data-eande106/software/conda_envs/ Using Conda \u00b6 Conda Documentation Conda Cheatsheet After loading the Anaconda module, one can create an environment and install packages into that environment: # Create conda environment named \"name_of_env\" conda create --name name_of_env # Create conda environment in a specific folder conda create -p /home/<jheid>/data_eande106/software/conda_envs/test # activate conda environment # for some `conda activate` works and for others `source activate` conda activate test source activate test # install package into conda environment conda install bcftools When looking to install a package, one resource to check out is anaconda.org , search for your package and run the install command listed on the page. Note Remember to keep in mind the different versions of software/packages. You could have bcftools-v1.10 or bcftools-v.1.12, so make sure you install the correct one! Important You can also install R packages with conda, however conda and R don't always work well together. Check out our quick fix here Running Nextflow with conda \u00b6 When running Nextflow, conda environments can be specified as part of a process or in the nextflow.config file to apply to the entire pipeline (check out the documentation ): Conda within a process: process foo { conda '/home/<jheid>/data_eande106/software/conda_envs/cegwas2-nf_env' ''' your_command --here ''' } Conda for the entire pipeline: // in the nextflow.config file: conda { conda.enabled = true conda.cacheDir = \".env\" } process { conda = \"/home/<jheid>/data_eande106/software/conda_envs/cegwas2-nf_env\" } Notes on conda versions on Quest vs Rockfish \u00b6 As of the end of 2020, existing conda environments for the lab were mostly created by module load python/anaconda from our previous file system, QUEST (which got automatically loaded with module git by accident). It loads Python version 2.7.18 and conda 4.5.2. The other environments were created with module load python/anaconda3.6 (also from QUEST) which loads Python 3.6.0 and conda 4.3.30. To see versions, use conda info or conda -V . As of 2023, conda environments will be generated with module load anaconda3/2022.05 from Rockfish, which loads Python version 3.9.12 and conda 4.12.0. From our limited testing so far, all environments generated in QUEST (now under ~/data-eande106/software/conda_envs/ ) seem to be compatible with the version active in Rockfish Once you activate an environment with conda activate env_name or source activate env_name , the default conda usually get re-directed to the conda that were originally used to create the environment. This is good because it helps ensure that all packages in the same environment uses the same version of conda. One can go to cd ~/.conda/env_name/bin and readlink -f conda or readlink -f activate to see which version of conda is used by this environment. This is exactly how Nextflow determines which conda to use when using an existing conda environment. A faster alternative to Conda \u00b6 In recent years, a 'drop-in' replacement for Conda, called Mamba, was released. Mamba is faster at resolving dependencies, which improves wait times for creating and activating environments. Conda and Mamba seem to be forward and backwards compatible, and they share the same command structure. For example, conda create and mamba create do the same task and share the same parameters. If you prefer to use Mamba, you can install it in your home directory by running these commands: cd ~ wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh chmod +x ./Mambaforge-Linux-x86_64.sh ./Mambaforge-Linux-x86_64.sh After completing the installation, the mamba binary should be added to $PATH , and you should be able to create, configure, and activate environments using the same commands as conda (FYI the conda binary will still be available to be called from $PATH by default). At the time this is being written, this mamba installation will install Python version 3.10.13, with conda version 23.11.0 and mamba version 1.5.5. You will immediately notice a difference in speed when using mamba. Test it by loading the nf23_env environment: mamba activate /home/<jheid>/data-eande106/software/conda_envs/nf23_env/","title":"Conda"},{"location":"rockfish/rf-conda/#using_conda_on_rockfish","text":"Using conda on Rockfish Why Conda Setting up Conda on Rockfish Using Conda Running Nextflow with conda Notes on conda versions on Quest vs Rockfish A faster alternative to Conda","title":"Using conda on Rockfish"},{"location":"rockfish/rf-conda/#why_conda","text":"Computational Reproducibility is the ability to reproduce an analysis exactly. In order for computational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub (for help, see the Github page ). Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software, package and environment managers such as Conda and Docker are very useful. Conda works similarly to brew or pyenv that were used in the legacy Andersen-Lab-Env. Note The software environments on Mac and Linux are not exactly identical...but they are very close.","title":"Why Conda"},{"location":"rockfish/rf-conda/#setting_up_conda_on_rockfish","text":"Anaconda is a Python distribution that contains many packages including conda. Miniconda is a more compact version of Anaconda that also includes conda. So in order to install conda, we usually either install Miniconda or Anaconda. On Rockfish, Anaconda is already installed. However there are many versions of Anaconda, each can have a different version of Python and Conda. The current lab environments mainly used module load anaconda3/2022.05 (See Notes below for more info). In your home directory ~/ , create a file called .condarc and put the following lines into it. It sets the channel priority when conda searches for packages. If possible, in one environment it is good to use packages from the same channel. channels: - conda-forge - bioconda - anaconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /home/<jheid>/data-eande106/software/conda_envs/","title":"Setting up Conda on Rockfish"},{"location":"rockfish/rf-conda/#using_conda","text":"Conda Documentation Conda Cheatsheet After loading the Anaconda module, one can create an environment and install packages into that environment: # Create conda environment named \"name_of_env\" conda create --name name_of_env # Create conda environment in a specific folder conda create -p /home/<jheid>/data_eande106/software/conda_envs/test # activate conda environment # for some `conda activate` works and for others `source activate` conda activate test source activate test # install package into conda environment conda install bcftools When looking to install a package, one resource to check out is anaconda.org , search for your package and run the install command listed on the page. Note Remember to keep in mind the different versions of software/packages. You could have bcftools-v1.10 or bcftools-v.1.12, so make sure you install the correct one! Important You can also install R packages with conda, however conda and R don't always work well together. Check out our quick fix here","title":"Using Conda"},{"location":"rockfish/rf-conda/#running_nextflow_with_conda","text":"When running Nextflow, conda environments can be specified as part of a process or in the nextflow.config file to apply to the entire pipeline (check out the documentation ): Conda within a process: process foo { conda '/home/<jheid>/data_eande106/software/conda_envs/cegwas2-nf_env' ''' your_command --here ''' } Conda for the entire pipeline: // in the nextflow.config file: conda { conda.enabled = true conda.cacheDir = \".env\" } process { conda = \"/home/<jheid>/data_eande106/software/conda_envs/cegwas2-nf_env\" }","title":"Running Nextflow with conda"},{"location":"rockfish/rf-conda/#notes_on_conda_versions_on_quest_vs_rockfish","text":"As of the end of 2020, existing conda environments for the lab were mostly created by module load python/anaconda from our previous file system, QUEST (which got automatically loaded with module git by accident). It loads Python version 2.7.18 and conda 4.5.2. The other environments were created with module load python/anaconda3.6 (also from QUEST) which loads Python 3.6.0 and conda 4.3.30. To see versions, use conda info or conda -V . As of 2023, conda environments will be generated with module load anaconda3/2022.05 from Rockfish, which loads Python version 3.9.12 and conda 4.12.0. From our limited testing so far, all environments generated in QUEST (now under ~/data-eande106/software/conda_envs/ ) seem to be compatible with the version active in Rockfish Once you activate an environment with conda activate env_name or source activate env_name , the default conda usually get re-directed to the conda that were originally used to create the environment. This is good because it helps ensure that all packages in the same environment uses the same version of conda. One can go to cd ~/.conda/env_name/bin and readlink -f conda or readlink -f activate to see which version of conda is used by this environment. This is exactly how Nextflow determines which conda to use when using an existing conda environment.","title":"Notes on conda versions on Quest vs Rockfish"},{"location":"rockfish/rf-conda/#a_faster_alternative_to_conda","text":"In recent years, a 'drop-in' replacement for Conda, called Mamba, was released. Mamba is faster at resolving dependencies, which improves wait times for creating and activating environments. Conda and Mamba seem to be forward and backwards compatible, and they share the same command structure. For example, conda create and mamba create do the same task and share the same parameters. If you prefer to use Mamba, you can install it in your home directory by running these commands: cd ~ wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh chmod +x ./Mambaforge-Linux-x86_64.sh ./Mambaforge-Linux-x86_64.sh After completing the installation, the mamba binary should be added to $PATH , and you should be able to create, configure, and activate environments using the same commands as conda (FYI the conda binary will still be available to be called from $PATH by default). At the time this is being written, this mamba installation will install Python version 3.10.13, with conda version 23.11.0 and mamba version 1.5.5. You will immediately notice a difference in speed when using mamba. Test it by loading the nf23_env environment: mamba activate /home/<jheid>/data-eande106/software/conda_envs/nf23_env/","title":"A faster alternative to Conda"},{"location":"rockfish/rf-intro/","text":"Rockfish \u00b6 Rockfish Introduction New Users Signing into Rockfish Login Nodes Home Directory Partitions/Queues Request 'interact' sessions on Rockfish Using screen, tmux, or nohup to keep jobs from timing out Using packages already installed on Rockfish Submitting jobs to Rockfish Monitoring SLURM Jobs on Rockfish Introduction \u00b6 The Andersen Lab makes use of Rockfish, the computer cluster Johns Hopkins, a part of MARCC. Take some time to read over the overview of what Rockfish is, what it does, how to use it, and how to sign up: Rockfish Documentation Note New to the command line? Check out the quick and dirty bash intro or this introduction first! New Users \u00b6 To gain access to Rockfish: Register new user by requesting allocation and security group access to eande106. Signing into Rockfish \u00b6 After you gain access to the cluster you can login using: ssh <jheid>@login.rockfish.jhu.edu To avoid typing in the password everytime, one can set up a ssh key . I recommend setting an alias in your ~/.bash_profile to make logging in quicker: alias rf=\"ssh <jheid>@login.rockfish.jhu.edu\" The above line makes it so you simply type rf and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below). Login Nodes \u00b6 There are three login nodes we use: login01-03. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired ( e.g. ssh login03 ). Warning When using screen or tmux to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those sessions. Home Directory \u00b6 Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 50 Gb. You can quickly check the storage of your home directory (and any other storage attached to your user) using the following script: quotas.py You can also check your storgae space using the following command: du -hs * You should also add the group's Singularity cache to your Rockfish ~/.bash_profile : export singularity_cachedir=/vast/eande106/singularity More information is provided below to help install and use software. Partitions/Queues \u00b6 Rockfish has several partitions/queues where you can submit jobs. They have different computational resources, and are designed to run specific tasks that vary in complexity and runtime: express queue : This partition is designed to run quick jobs like tests, debugging, or interactive computing (Jupyter notebooks, R-Studio, etc). The limits are up to 4 cores, 8 hours, up to 8GB of memory per job. It is shared meaning many jobs are allowed to run on the same node. shared queue : This partition is designed to run a mix of jobs, from sequential jobs needing just one core to small parallel jobs (32 cores or less). Nodes on this queue may share resources with many other jobs. The memory per core is set by default to 4 GB. If the application needs more than 4GB memory per process users can request 2 cores and this will give 8GB of memory to the job. Each core will add 4 GB or memory parallel queue ; This partition is designed to run ONLY parallel jobs that require 48 cores or more. Jobs can run on a single node or multiple nodes. Nodes in the \u201cparallel\u201d queue are dedicated to the job. These are not shared with any other jobs. Users should make sure all cores will be used. The timelimit is 72 hours. A research group can run jobs up to 3600 cores or 75 nodes. a100 queue : This partition/queue is designed to run jobs that require the use of GPU nodes. Each node has 4 Nvidia A100 gpus with 40GB of memory per gpu. This allocation is defined by the account \u201cPI-userid_gpu\u201c. This account should be included in the SLURM script as bigmem queue : This queue contains nodes with 1500GB of memory and it is designed to run jobs that require more than 192GB of memory. Use of this queue requires a separate allocation of the type \u201cPI-userid_bigmem\u201c. This account should be included in the SLURM script as ica100 queue . This partition is similar to the \u201ca100\u201d queue but each GPU has 80GB of memory. Note The Allocation defines the user group access to these partitions. Currently the Andersen lab allocation (eande106) has access to express , shared , and parallel . We are working on expanding our allocation to the GPU and bigmem partitions. For most jobs, the currently available paritions ar Note Anyone who uses Rockfish should build your own project folder under /home/<jheid>/vast-eande106/projects with your name. You should only write and revise files under your project folder. You can read/copy data from VAST but don't write any data out of your project folder. See the Storage section for more information. Important It is important that we keep the 120 Tb of storage space on VAST from filling up with extraneous or intermediate files. It is good practice to clean up old data/files and backup important data at least every few months. You can check the percent of space remaining with quotas.py Request 'interact' sessions on Rockfish \u00b6 If you are running a few simple commands or want to experiment with files directly you can start an interactive session on Rockfish. The command below (based on the srun command) will give you access to a node where you can run your command: interact -n 1 -c 1 -a eande106 -m 64G -p queue-name -t \u201ctime in minutes\u201d Where -n is the number of nodes, -c is the number of cores, -a is the allocation ID, -m is the allocated memory, -p is the partition/queue, and -t is the walltime for the session. Important Do not run commands for big data directly on login01-03 . These are login nodes and are not meant for running heavy-load workflows. Either open an interact session, or submit a job. Using screen , tmux , or nohup to keep jobs from timing out \u00b6 If you have ever tried to run a pipeline or script that takes a long time (think NemaScan ), you know that if you close down your terminal or if your Rockfish session logs out, it will cause your jobs to end prematurely. There are several ways to avoid this: screen Perhaps the most common way to deal with scripts that run for a long time is screen . For the most simple case use, type screen to open a new screen session and then run your script like normal. Below are some more intermediate commands for taking full advantage of screen : screen -S <some_descriptive_name> : Use this command to name your screen session. Especially useful if you have several screen sessions running and/or want to get back to this particular one later. Ctrl+a followed by d to detach from the current screen session (NOT Ctrl+a+d !) exit to end the current screen session screen -ls lists the IDs of all screen sessions currently running screen -r <screen_id> : Use this command to resume a particular screen session. If you only have one session running you can simply use screen -r Important When using screen on Rockfish, take note that screen sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh login02 ). tmux tmux is another way to deal with scripts that run for a long time. For the most simple case use, type tmux to open a new tmux session and then run your script like normal. Below are some more intermediate commands for taking full advantage of tmux : tmux new -s <some_descriptive_name> : Use this command to name your tmux session. Especially useful if you have several tmux sessions running and/or want to get back to this particular one later. Ctrl+b followed by d to detach from the current tmux session (NOT Ctrl+b+d !) exit to end the current tmux session tmux ls lists the IDs of all tmux sessions currently running tmux attach -t <session_id> : Use this command to resume a particular tmux session. If you only have one session running you can simply use tmux a Important When using tmux on Rockfish, take note that tmux sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh login02 ). nohup Another way to avoid SIGHUP errors is to use nohup . nohup is very simple to use, simply type nohup before your normal command and that's it! nohup nextflow run main.nf Running a command with nohup will look a little different than usual because it will not print anything to the console. Instead, it prints all console outputs into a file in the current directory called nohup.out . You can redirect the output to another filename using: nohup nextflow run main.nf > cegwas_{date}_output.txt When you exit this window and open a new session, you can always look at the contents of the output file using cat nohup.out to see the progress. Note You can also run nohup in the background to continue using the same window for other processes by running nohup nextflow run main.nf & . Using packages already installed on Rockfish \u00b6 Rockfish has a collection of packages installed. You can run module avail to see what packages are currently available on Rockfish. You can also use module spider <search term> to search for tools containing a particular term in their names. You can use module load bcftools or module load bcftools/1.15.1 to load packages with the default or a specific version ( module add does the same thing). If you do echo $PATH before and after loading modules, you can see what module does is simply appending paths to the packages into your $PATH so the packages can be found in your environment. module purge will remove all loaded packages and can be helpful to try when troubleshooting. Note You can also load/install software packages into conda environments for use with a specific project/pipeline. Check out the conda tutorial here . Submitting jobs to Rockfish \u00b6 Jobs on Rockfish are managed by SLURM . While most of our pipelines are managed by Nextflow, it is useful to know how to submit jobs directly to SLURM. Below is a template to submit an array job to SLURM that will run 10 parallel jobs. One can run it with sbatch script.sh . If you dig into the Nextflow working directory, you can see Nextflow actually generates such scripts and submits them to SLURM on your behalf. Thanks, Nextflow! #!/bin/bash #SBATCH -J name # Name of job #SBATCH -A eande106 # Allocation #SBATCH -p parallel # Parition/Queue #SBATCH -t 24:00:00 # Walltime/duration of the job (hh:mm:ss) #SBATCH --cpus-per-task=1 # Number of cores (= processors = cpus) for each task #SBATCH --mem-per-cpu=3G # Memory per core needed for a job #SBATCH --array=0-9 # number of parallel jobs to run counting from 0. Make sure to change this to match total number of files. # make a list of all the files you want to process # this script will run a parallel process for each file in this list name_list=(*.bam) # take the nth ($SGE_TASK_ID-th) file in name_list # don't change this line Input=${name_list[$SLURM_ARRAY_TASK_ID]} # then do your operation on this file Output=`echo $Input | sed 's/.bam/.sam/'` Monitoring SLURM Jobs on Rockfish \u00b6 Here are some commmon commands used to interact with SLURM and check on job status. For more, check out this page . squeue -u <jheid> to check all jobs of a user. scancel <jobid> to cancel a job. scancel {30000..32000} to cancel a range of jobs (job id between 30000 and 32000).","title":"Introduction"},{"location":"rockfish/rf-intro/#rockfish","text":"Rockfish Introduction New Users Signing into Rockfish Login Nodes Home Directory Partitions/Queues Request 'interact' sessions on Rockfish Using screen, tmux, or nohup to keep jobs from timing out Using packages already installed on Rockfish Submitting jobs to Rockfish Monitoring SLURM Jobs on Rockfish","title":"Rockfish"},{"location":"rockfish/rf-intro/#introduction","text":"The Andersen Lab makes use of Rockfish, the computer cluster Johns Hopkins, a part of MARCC. Take some time to read over the overview of what Rockfish is, what it does, how to use it, and how to sign up: Rockfish Documentation Note New to the command line? Check out the quick and dirty bash intro or this introduction first!","title":"Introduction"},{"location":"rockfish/rf-intro/#new_users","text":"To gain access to Rockfish: Register new user by requesting allocation and security group access to eande106.","title":"New Users"},{"location":"rockfish/rf-intro/#signing_into_rockfish","text":"After you gain access to the cluster you can login using: ssh <jheid>@login.rockfish.jhu.edu To avoid typing in the password everytime, one can set up a ssh key . I recommend setting an alias in your ~/.bash_profile to make logging in quicker: alias rf=\"ssh <jheid>@login.rockfish.jhu.edu\" The above line makes it so you simply type rf and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below).","title":"Signing into Rockfish"},{"location":"rockfish/rf-intro/#login_nodes","text":"There are three login nodes we use: login01-03. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired ( e.g. ssh login03 ). Warning When using screen or tmux to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those sessions.","title":"Login Nodes"},{"location":"rockfish/rf-intro/#home_directory","text":"Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 50 Gb. You can quickly check the storage of your home directory (and any other storage attached to your user) using the following script: quotas.py You can also check your storgae space using the following command: du -hs * You should also add the group's Singularity cache to your Rockfish ~/.bash_profile : export singularity_cachedir=/vast/eande106/singularity More information is provided below to help install and use software.","title":"Home Directory"},{"location":"rockfish/rf-intro/#partitionsqueues","text":"Rockfish has several partitions/queues where you can submit jobs. They have different computational resources, and are designed to run specific tasks that vary in complexity and runtime: express queue : This partition is designed to run quick jobs like tests, debugging, or interactive computing (Jupyter notebooks, R-Studio, etc). The limits are up to 4 cores, 8 hours, up to 8GB of memory per job. It is shared meaning many jobs are allowed to run on the same node. shared queue : This partition is designed to run a mix of jobs, from sequential jobs needing just one core to small parallel jobs (32 cores or less). Nodes on this queue may share resources with many other jobs. The memory per core is set by default to 4 GB. If the application needs more than 4GB memory per process users can request 2 cores and this will give 8GB of memory to the job. Each core will add 4 GB or memory parallel queue ; This partition is designed to run ONLY parallel jobs that require 48 cores or more. Jobs can run on a single node or multiple nodes. Nodes in the \u201cparallel\u201d queue are dedicated to the job. These are not shared with any other jobs. Users should make sure all cores will be used. The timelimit is 72 hours. A research group can run jobs up to 3600 cores or 75 nodes. a100 queue : This partition/queue is designed to run jobs that require the use of GPU nodes. Each node has 4 Nvidia A100 gpus with 40GB of memory per gpu. This allocation is defined by the account \u201cPI-userid_gpu\u201c. This account should be included in the SLURM script as bigmem queue : This queue contains nodes with 1500GB of memory and it is designed to run jobs that require more than 192GB of memory. Use of this queue requires a separate allocation of the type \u201cPI-userid_bigmem\u201c. This account should be included in the SLURM script as ica100 queue . This partition is similar to the \u201ca100\u201d queue but each GPU has 80GB of memory. Note The Allocation defines the user group access to these partitions. Currently the Andersen lab allocation (eande106) has access to express , shared , and parallel . We are working on expanding our allocation to the GPU and bigmem partitions. For most jobs, the currently available paritions ar Note Anyone who uses Rockfish should build your own project folder under /home/<jheid>/vast-eande106/projects with your name. You should only write and revise files under your project folder. You can read/copy data from VAST but don't write any data out of your project folder. See the Storage section for more information. Important It is important that we keep the 120 Tb of storage space on VAST from filling up with extraneous or intermediate files. It is good practice to clean up old data/files and backup important data at least every few months. You can check the percent of space remaining with quotas.py","title":"Partitions/Queues"},{"location":"rockfish/rf-intro/#request_interact_sessions_on_rockfish","text":"If you are running a few simple commands or want to experiment with files directly you can start an interactive session on Rockfish. The command below (based on the srun command) will give you access to a node where you can run your command: interact -n 1 -c 1 -a eande106 -m 64G -p queue-name -t \u201ctime in minutes\u201d Where -n is the number of nodes, -c is the number of cores, -a is the allocation ID, -m is the allocated memory, -p is the partition/queue, and -t is the walltime for the session. Important Do not run commands for big data directly on login01-03 . These are login nodes and are not meant for running heavy-load workflows. Either open an interact session, or submit a job.","title":"Request 'interact' sessions on Rockfish"},{"location":"rockfish/rf-intro/#using_screen_tmux_or_nohup_to_keep_jobs_from_timing_out","text":"If you have ever tried to run a pipeline or script that takes a long time (think NemaScan ), you know that if you close down your terminal or if your Rockfish session logs out, it will cause your jobs to end prematurely. There are several ways to avoid this: screen Perhaps the most common way to deal with scripts that run for a long time is screen . For the most simple case use, type screen to open a new screen session and then run your script like normal. Below are some more intermediate commands for taking full advantage of screen : screen -S <some_descriptive_name> : Use this command to name your screen session. Especially useful if you have several screen sessions running and/or want to get back to this particular one later. Ctrl+a followed by d to detach from the current screen session (NOT Ctrl+a+d !) exit to end the current screen session screen -ls lists the IDs of all screen sessions currently running screen -r <screen_id> : Use this command to resume a particular screen session. If you only have one session running you can simply use screen -r Important When using screen on Rockfish, take note that screen sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh login02 ). tmux tmux is another way to deal with scripts that run for a long time. For the most simple case use, type tmux to open a new tmux session and then run your script like normal. Below are some more intermediate commands for taking full advantage of tmux : tmux new -s <some_descriptive_name> : Use this command to name your tmux session. Especially useful if you have several tmux sessions running and/or want to get back to this particular one later. Ctrl+b followed by d to detach from the current tmux session (NOT Ctrl+b+d !) exit to end the current tmux session tmux ls lists the IDs of all tmux sessions currently running tmux attach -t <session_id> : Use this command to resume a particular tmux session. If you only have one session running you can simply use tmux a Important When using tmux on Rockfish, take note that tmux sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh login02 ). nohup Another way to avoid SIGHUP errors is to use nohup . nohup is very simple to use, simply type nohup before your normal command and that's it! nohup nextflow run main.nf Running a command with nohup will look a little different than usual because it will not print anything to the console. Instead, it prints all console outputs into a file in the current directory called nohup.out . You can redirect the output to another filename using: nohup nextflow run main.nf > cegwas_{date}_output.txt When you exit this window and open a new session, you can always look at the contents of the output file using cat nohup.out to see the progress. Note You can also run nohup in the background to continue using the same window for other processes by running nohup nextflow run main.nf & .","title":"Using screen, tmux, or nohup to keep jobs from timing out"},{"location":"rockfish/rf-intro/#using_packages_already_installed_on_rockfish","text":"Rockfish has a collection of packages installed. You can run module avail to see what packages are currently available on Rockfish. You can also use module spider <search term> to search for tools containing a particular term in their names. You can use module load bcftools or module load bcftools/1.15.1 to load packages with the default or a specific version ( module add does the same thing). If you do echo $PATH before and after loading modules, you can see what module does is simply appending paths to the packages into your $PATH so the packages can be found in your environment. module purge will remove all loaded packages and can be helpful to try when troubleshooting. Note You can also load/install software packages into conda environments for use with a specific project/pipeline. Check out the conda tutorial here .","title":"Using packages already installed on Rockfish"},{"location":"rockfish/rf-intro/#submitting_jobs_to_rockfish","text":"Jobs on Rockfish are managed by SLURM . While most of our pipelines are managed by Nextflow, it is useful to know how to submit jobs directly to SLURM. Below is a template to submit an array job to SLURM that will run 10 parallel jobs. One can run it with sbatch script.sh . If you dig into the Nextflow working directory, you can see Nextflow actually generates such scripts and submits them to SLURM on your behalf. Thanks, Nextflow! #!/bin/bash #SBATCH -J name # Name of job #SBATCH -A eande106 # Allocation #SBATCH -p parallel # Parition/Queue #SBATCH -t 24:00:00 # Walltime/duration of the job (hh:mm:ss) #SBATCH --cpus-per-task=1 # Number of cores (= processors = cpus) for each task #SBATCH --mem-per-cpu=3G # Memory per core needed for a job #SBATCH --array=0-9 # number of parallel jobs to run counting from 0. Make sure to change this to match total number of files. # make a list of all the files you want to process # this script will run a parallel process for each file in this list name_list=(*.bam) # take the nth ($SGE_TASK_ID-th) file in name_list # don't change this line Input=${name_list[$SLURM_ARRAY_TASK_ID]} # then do your operation on this file Output=`echo $Input | sed 's/.bam/.sam/'`","title":"Submitting jobs to Rockfish"},{"location":"rockfish/rf-intro/#monitoring_slurm_jobs_on_rockfish","text":"Here are some commmon commands used to interact with SLURM and check on job status. For more, check out this page . squeue -u <jheid> to check all jobs of a user. scancel <jobid> to cancel a job. scancel {30000..32000} to cancel a range of jobs (job id between 30000 and 32000).","title":"Monitoring SLURM Jobs on Rockfish"},{"location":"rockfish/rf-nextflow/","text":"Nextflow \u00b6 Nextflow Installation Configuring Nextflow Running Nextflow Running Nextflow with a remote pipeline Resume Running a custom Nextflow pipeline version Getting an email or text when complete Troubleshooting Writing Nextflow Pipelines Nextflow is an awesome program that allows you to write a computational pipeline by making it simpler to put together many different tasks, maybe even using different programming languages. Nextflow makes parallelism easy and works with SLURM to schedule jobs so you don't have to! Nextflow also supports docker containers and conda enviornments to streamline reproducible pipelines and can be easily adapted to run on different systems - including Google Cloud! Not convinced? Check out this intro . Installation \u00b6 On Rockfish, you do not need to install Nextflow. Instead, you will be using a preinstalled version in a conda environment. To activate this environment, you will first load the conda module, followed by activating the Nextflow environment: module load anaconda conda activate /data/eande106/software/conda_envs/nf23_env This will make Nextflow v23.10.1 available. To make it easier to load, you can create an alias for loading it by placing the following code in ~/.bash_profile : alias nf=\"ml anaconda && conda activate /data/eande106/software/conda_envs/nf23_env\" To exit the Nextflow environment, simply use the command conda deactivate . Important Because this is a shared environment, you should not make changes to it by installing new software, updating Nextflow, etc. Configuring Nextflow \u00b6 There are a few environmental variables that will need to be set to insure proper function of Nextflow on Rockfish. You can set these by adding them to your ~/.bash_profile so that you only need to configure Nextflow once. export NXF_CACHE_DIR=$HOME export NXF_SINGULARITY_CACHEDIR=/vast/eande106/singularity export NXF_SINGULARITY_LIBRARYDIR=/vast/eande106/singularity export NXF_WORK=/scratch4/eande106/ Note Once you have saved these changes to your profile, you will either need to close the terminal window and open a new one, or run the command source ~/.bash_profile in order for them to take effect. Running Nextflow \u00b6 Theoretically, running a Nextflow pipeline should be very straightforward (although with any developing pipelines there are always bugs to work out). Running Nextflow with a remote pipeline \u00b6 In order to run Andersen Lab pipelines, you should be retrieving them from their github repos directly through Nextflow to ensure that you are getting the newest (or specific) version. This can be done by specifying the repo instead of a .nf file. Nextflow will then include information about which version and repo the workflow came from in the logs and final report, allowing for complete reproducibility. Note Nextflow caches repos when you execute them. This means that if you run have previously run a workflow and go to run it again after changes have been made, Nextflow will reuse the cached version unless you specify to download the latest version with the argument -latest . If you need to run a specific branch, commit, or tag of a pipeline repo, you can do this with the argument -r <branch/commit/tag> . # example command to run the latest version of NemaScan nextflow run andersenlab/nemascan \\ -latest \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 # Note: you can also write in one line, the \\ were used to make the code more readable: nextflow run -latest andersenlab/nemascan --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv --vcf 20210121 Note Parameters to Nextflow scripts are designated with the -- . In the case above, there is a parameter called \"traitfile\" which we are setting to be the input_data/c_elegans/phenotypes/test_pheno.tsv file. Arguments to Nextflow are designated with the - . These tell Nextflow how to behave when running a pipeline and are specific to Nextflow, not any given pipeline. When Nextflow is running, it will print to the console the name of each process in the pipeline as well as update on the progress of the script: For example, in the above screenshot from a NemaScan run, there are 14 different processes in the pipeline. Notice that the fix_strain_names_bulk process has already completed! Meanwhile, the vcf_t_geno_matrix process is still running. You can also see that the prepare_gcta_files is actually run 4 times (in this case, because it is run once per 4 traits in my dataset). Another important piece of information from this Nextflow run is the hash that designates the working directory of each process. the [ad/eea615] next to the fix_strain_names_bulk process indicates that the working directory is located at /scratch4/eande106/ad/eea615... . This can be helpful if you want to go into that directory to see what is actually happening or trouble shoot errors. Note I highly recommend adding this function to your ~/.bash_profile to easily access the Nexflow working directory: gw() {cd /scratch4/eande106/$1*} so that when you type gw 3f/6a21a5 (the hash Nextflow shows that indicates the specific working directory for a process) you will go to that folder automatically. Resume \u00b6 Another great thing about Nextflow is that it caches all the processes that completed successfully, so if you have an error at the middle or end of your pipeline, you can re-run the same Nextflow command with -resume and it will start where it finished last time! nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -resume There is usually no downside to adding -resume , so you can get into the habit of always adding it if you want. Important There is some confusion with how the -resume works and sometimes it doesn't work as expected. Check out this and this other guide for more help. One thing I've learned is there is a difference between process.cache = 'deep' vs. 'lenient' . Running a custom Nextflow pipeline version \u00b6 If you need to run a custom version of a pipeline, there are two approached you can use. Clone the repo onto Rockfish and make local changes. Then run Nextflow from the local pipeline. Clone the repo onto your computer, create a new branch, and make changes to your new branch. Then push those changes and run Nextflow on your branch of the pipeline. You should NOT do the first, as this means that your analysis will not be reproducible the moment you delete that local repo, nor will it be available to other people. By creating your own branch, you are able to track changes and allow others to run the same code as you with only a couple simple extra steps. On your machine (or technically Rockfish if you want, but I recommend locally) where is whatever branch name you want: git clone https://github.com/AndersenLab/NemaScan.git cd NemaScan git checkout -b <your-branch> ## Make your local changes ## git commit -a -m \"This is my branch!\" # or some more meaningful message git push --set-upstream origin <your-branch> To run your custom pipeline on Rockfish: nextflow run andersenlab/nemascan -r <your-branch> ... # plus whatever arguments and parameters are needed for this pipeline Getting an email or text when complete \u00b6 If you would like to receive an email or text from Nextflow when your pipeline finishes (either successfully or with error), all you need to do is add -N <email> to your code. Most phone companies have a way to \"email\" your phone as an SMS, so you can use this email address to get a text alert. For example: # send email nextflow run andersenlab/nemascan --debug -N kathryn.evans@northwestern.edu # send text to 801-867-5309 with verizon nextflow run andersenlab/nemascan --debug -N 8018675309@vtext.com Troubleshooting \u00b6 Sometimes, Nextflow may throw an unexpected error that is unrelated to a problem with the data or commands inside the pipeline. If it can't be troubleshooted, one option is to clear the Nextflow cache. This will cause you to lose any progress and require Nextflow to start at the beginning of the pipeline. rm -rf /home/<jheid>/.nextflow Warning You should always be careful when using rm , especially rm -rf . There is no going back. Make certain you want to delete the folder and everything inside it. In this case, it will be fine because nextflow will just clone it again fresh. Writing Nextflow Pipelines \u00b6 See this page for tips on how to get started with your own Nextflow pipeline. Also check out the Nextflow documentation for help getting started!","title":"Nextflow"},{"location":"rockfish/rf-nextflow/#nextflow","text":"Nextflow Installation Configuring Nextflow Running Nextflow Running Nextflow with a remote pipeline Resume Running a custom Nextflow pipeline version Getting an email or text when complete Troubleshooting Writing Nextflow Pipelines Nextflow is an awesome program that allows you to write a computational pipeline by making it simpler to put together many different tasks, maybe even using different programming languages. Nextflow makes parallelism easy and works with SLURM to schedule jobs so you don't have to! Nextflow also supports docker containers and conda enviornments to streamline reproducible pipelines and can be easily adapted to run on different systems - including Google Cloud! Not convinced? Check out this intro .","title":"Nextflow"},{"location":"rockfish/rf-nextflow/#installation","text":"On Rockfish, you do not need to install Nextflow. Instead, you will be using a preinstalled version in a conda environment. To activate this environment, you will first load the conda module, followed by activating the Nextflow environment: module load anaconda conda activate /data/eande106/software/conda_envs/nf23_env This will make Nextflow v23.10.1 available. To make it easier to load, you can create an alias for loading it by placing the following code in ~/.bash_profile : alias nf=\"ml anaconda && conda activate /data/eande106/software/conda_envs/nf23_env\" To exit the Nextflow environment, simply use the command conda deactivate . Important Because this is a shared environment, you should not make changes to it by installing new software, updating Nextflow, etc.","title":"Installation"},{"location":"rockfish/rf-nextflow/#configuring_nextflow","text":"There are a few environmental variables that will need to be set to insure proper function of Nextflow on Rockfish. You can set these by adding them to your ~/.bash_profile so that you only need to configure Nextflow once. export NXF_CACHE_DIR=$HOME export NXF_SINGULARITY_CACHEDIR=/vast/eande106/singularity export NXF_SINGULARITY_LIBRARYDIR=/vast/eande106/singularity export NXF_WORK=/scratch4/eande106/ Note Once you have saved these changes to your profile, you will either need to close the terminal window and open a new one, or run the command source ~/.bash_profile in order for them to take effect.","title":"Configuring Nextflow"},{"location":"rockfish/rf-nextflow/#running_nextflow","text":"Theoretically, running a Nextflow pipeline should be very straightforward (although with any developing pipelines there are always bugs to work out).","title":"Running Nextflow"},{"location":"rockfish/rf-nextflow/#running_nextflow_with_a_remote_pipeline","text":"In order to run Andersen Lab pipelines, you should be retrieving them from their github repos directly through Nextflow to ensure that you are getting the newest (or specific) version. This can be done by specifying the repo instead of a .nf file. Nextflow will then include information about which version and repo the workflow came from in the logs and final report, allowing for complete reproducibility. Note Nextflow caches repos when you execute them. This means that if you run have previously run a workflow and go to run it again after changes have been made, Nextflow will reuse the cached version unless you specify to download the latest version with the argument -latest . If you need to run a specific branch, commit, or tag of a pipeline repo, you can do this with the argument -r <branch/commit/tag> . # example command to run the latest version of NemaScan nextflow run andersenlab/nemascan \\ -latest \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 # Note: you can also write in one line, the \\ were used to make the code more readable: nextflow run -latest andersenlab/nemascan --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv --vcf 20210121 Note Parameters to Nextflow scripts are designated with the -- . In the case above, there is a parameter called \"traitfile\" which we are setting to be the input_data/c_elegans/phenotypes/test_pheno.tsv file. Arguments to Nextflow are designated with the - . These tell Nextflow how to behave when running a pipeline and are specific to Nextflow, not any given pipeline. When Nextflow is running, it will print to the console the name of each process in the pipeline as well as update on the progress of the script: For example, in the above screenshot from a NemaScan run, there are 14 different processes in the pipeline. Notice that the fix_strain_names_bulk process has already completed! Meanwhile, the vcf_t_geno_matrix process is still running. You can also see that the prepare_gcta_files is actually run 4 times (in this case, because it is run once per 4 traits in my dataset). Another important piece of information from this Nextflow run is the hash that designates the working directory of each process. the [ad/eea615] next to the fix_strain_names_bulk process indicates that the working directory is located at /scratch4/eande106/ad/eea615... . This can be helpful if you want to go into that directory to see what is actually happening or trouble shoot errors. Note I highly recommend adding this function to your ~/.bash_profile to easily access the Nexflow working directory: gw() {cd /scratch4/eande106/$1*} so that when you type gw 3f/6a21a5 (the hash Nextflow shows that indicates the specific working directory for a process) you will go to that folder automatically.","title":"Running Nextflow with a remote pipeline"},{"location":"rockfish/rf-nextflow/#resume","text":"Another great thing about Nextflow is that it caches all the processes that completed successfully, so if you have an error at the middle or end of your pipeline, you can re-run the same Nextflow command with -resume and it will start where it finished last time! nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -resume There is usually no downside to adding -resume , so you can get into the habit of always adding it if you want. Important There is some confusion with how the -resume works and sometimes it doesn't work as expected. Check out this and this other guide for more help. One thing I've learned is there is a difference between process.cache = 'deep' vs. 'lenient' .","title":"Resume"},{"location":"rockfish/rf-nextflow/#running_a_custom_nextflow_pipeline_version","text":"If you need to run a custom version of a pipeline, there are two approached you can use. Clone the repo onto Rockfish and make local changes. Then run Nextflow from the local pipeline. Clone the repo onto your computer, create a new branch, and make changes to your new branch. Then push those changes and run Nextflow on your branch of the pipeline. You should NOT do the first, as this means that your analysis will not be reproducible the moment you delete that local repo, nor will it be available to other people. By creating your own branch, you are able to track changes and allow others to run the same code as you with only a couple simple extra steps. On your machine (or technically Rockfish if you want, but I recommend locally) where is whatever branch name you want: git clone https://github.com/AndersenLab/NemaScan.git cd NemaScan git checkout -b <your-branch> ## Make your local changes ## git commit -a -m \"This is my branch!\" # or some more meaningful message git push --set-upstream origin <your-branch> To run your custom pipeline on Rockfish: nextflow run andersenlab/nemascan -r <your-branch> ... # plus whatever arguments and parameters are needed for this pipeline","title":"Running a custom Nextflow pipeline version"},{"location":"rockfish/rf-nextflow/#getting_an_email_or_text_when_complete","text":"If you would like to receive an email or text from Nextflow when your pipeline finishes (either successfully or with error), all you need to do is add -N <email> to your code. Most phone companies have a way to \"email\" your phone as an SMS, so you can use this email address to get a text alert. For example: # send email nextflow run andersenlab/nemascan --debug -N kathryn.evans@northwestern.edu # send text to 801-867-5309 with verizon nextflow run andersenlab/nemascan --debug -N 8018675309@vtext.com","title":"Getting an email or text when complete"},{"location":"rockfish/rf-nextflow/#troubleshooting","text":"Sometimes, Nextflow may throw an unexpected error that is unrelated to a problem with the data or commands inside the pipeline. If it can't be troubleshooted, one option is to clear the Nextflow cache. This will cause you to lose any progress and require Nextflow to start at the beginning of the pipeline. rm -rf /home/<jheid>/.nextflow Warning You should always be careful when using rm , especially rm -rf . There is no going back. Make certain you want to delete the folder and everything inside it. In this case, it will be fine because nextflow will just clone it again fresh.","title":"Troubleshooting"},{"location":"rockfish/rf-nextflow/#writing_nextflow_pipelines","text":"See this page for tips on how to get started with your own Nextflow pipeline. Also check out the Nextflow documentation for help getting started!","title":"Writing Nextflow Pipelines"}]}