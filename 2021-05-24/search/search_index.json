{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Andersen Lab Dry Guide \u00b6 The Dry Guide details the computational infrastructure and tasks used in the Andersen Lab. Looking for more resources? Check out our Andersen Lab code club GitHub repo for more coding tips and tricks!","title":"Home"},{"location":"#welcome_to_the_andersen_lab_dry_guide","text":"The Dry Guide details the computational infrastructure and tasks used in the Andersen Lab. Looking for more resources? Check out our Andersen Lab code club GitHub repo for more coding tips and tricks!","title":"Welcome to the Andersen Lab Dry Guide"},{"location":"adding-seq-data/","text":"Introduction \u00b6 Introduction Basic Commands To Know Prior Your Sequencing Data Step By Step Instructions This section is for adding genomic sequencing data (.tsv files) onto the existing dataset provided and displayed on the NILs browser page on Andersenlab.org . Basic Commands To Know Prior \u00b6 You should freshen up on the following terminal commands. cd - change directories rm - delete files cp - make a copy git Your Sequencing Data \u00b6 In order for your sequencing data to be properly added, it is important to make sure that there are no empty/additional lines located at the bottom of your .tsv file. What you do not want What you do want Once your file has no empty lines at the bottom, save the file and move onto the next instructions below. Step By Step Instructions \u00b6 Add your .tsv file into the pages folder of your Andersenlab github directory. Open terminal and use the cd command to change directories into the pages folder in your Andersenlab github directory. If you did everything correctly, when you type ls into your terminal, it should look something like this. Then run the following commands in your terminal (while still in your pages directory): $ cp yourFileName.tsv copy.tsv $ python addDataTogt_hmm.tsv.py After running the above commands, your sequencing data has now been added to the existing NILs dataset on Andersenlab.org. You can now remove your .tsv from the pages directory by using the rm command in your terminal. Finally, commit your changes and push your code to update the Andersenlab github.","title":"Adding NILs Sequencing Data"},{"location":"adding-seq-data/#introduction","text":"Introduction Basic Commands To Know Prior Your Sequencing Data Step By Step Instructions This section is for adding genomic sequencing data (.tsv files) onto the existing dataset provided and displayed on the NILs browser page on Andersenlab.org .","title":"Introduction"},{"location":"adding-seq-data/#basic_commands_to_know_prior","text":"You should freshen up on the following terminal commands. cd - change directories rm - delete files cp - make a copy git","title":"Basic Commands To Know Prior"},{"location":"adding-seq-data/#your_sequencing_data","text":"In order for your sequencing data to be properly added, it is important to make sure that there are no empty/additional lines located at the bottom of your .tsv file. What you do not want What you do want Once your file has no empty lines at the bottom, save the file and move onto the next instructions below.","title":"Your Sequencing Data"},{"location":"adding-seq-data/#step_by_step_instructions","text":"Add your .tsv file into the pages folder of your Andersenlab github directory. Open terminal and use the cd command to change directories into the pages folder in your Andersenlab github directory. If you did everything correctly, when you type ls into your terminal, it should look something like this. Then run the following commands in your terminal (while still in your pages directory): $ cp yourFileName.tsv copy.tsv $ python addDataTogt_hmm.tsv.py After running the above commands, your sequencing data has now been added to the existing NILs dataset on Andersenlab.org. You can now remove your .tsv from the pages directory by using the rm command in your terminal. Finally, commit your changes and push your code to update the Andersenlab github.","title":"Step By Step Instructions"},{"location":"backup/","text":"Configuring AWS Client \u00b6 The aws commandline utility is installed as a part of the andersen-lab-env You must provide the appropriate s3 credentials in order to backup BAMs. Erik knows where these are. aws configure Backing up BAMS to Amazon S3 \u00b6 BAMS are uploaded to S3. This allows us to back them up there and access them through CeNDR. Currently, the only BAMs that we upload to s3 are wild isolate BAMs. Once you've added new sequence data you can sync the new BAMs by running the following: # First, cd to the location of isotype bams cd /projects/b1059/data/alignments/WI/isotype # Then run the following command aws s3 sync . s3://elegansvariation.org/bam Local Backup \u00b6 In addition to backing up BAMs on S3, we also backup FASTQs locally. There are four hard drives: PortusTotus - FASTQ Backup 1 Hawkeye - FASTQ Backup 2 Armadillo - open Rhino - open Each one is labeled with what is backed up on it. After you have added new sequenced data you should sync FROM the data/fastq folder on b1059 TO the the PortusTotus (FASTQ Backup 1) and Hawkeye (FASTQ Backup 2) hard drives.","title":"Backup"},{"location":"backup/#configuring_aws_client","text":"The aws commandline utility is installed as a part of the andersen-lab-env You must provide the appropriate s3 credentials in order to backup BAMs. Erik knows where these are. aws configure","title":"Configuring AWS Client"},{"location":"backup/#backing_up_bams_to_amazon_s3","text":"BAMS are uploaded to S3. This allows us to back them up there and access them through CeNDR. Currently, the only BAMs that we upload to s3 are wild isolate BAMs. Once you've added new sequence data you can sync the new BAMs by running the following: # First, cd to the location of isotype bams cd /projects/b1059/data/alignments/WI/isotype # Then run the following command aws s3 sync . s3://elegansvariation.org/bam","title":"Backing up BAMS to Amazon S3"},{"location":"backup/#local_backup","text":"In addition to backing up BAMs on S3, we also backup FASTQs locally. There are four hard drives: PortusTotus - FASTQ Backup 1 Hawkeye - FASTQ Backup 2 Armadillo - open Rhino - open Each one is labeled with what is backed up on it. After you have added new sequenced data you should sync FROM the data/fastq folder on b1059 TO the the PortusTotus (FASTQ Backup 1) and Hawkeye (FASTQ Backup 2) hard drives.","title":"Local Backup"},{"location":"bash/","text":"Bash \u00b6 Bash Basic Commands More Advanced Good Guides grep awk Rearranging columns Filtering based on criteria bcftools Screen Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with this introduction to bash . Also check out this cheatsheet Basic Commands \u00b6 You should familiarize yourself with the following commands. alias - create a shortcut for a command cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted. More Advanced \u00b6 You should learn these once you have the basics down. git - version control awk - file manipulation; Filtering; Rearranging columns sed - quick find/replace Good Guides \u00b6 Below are some good guides for various bash utilities. grep \u00b6 using grep with regular expressions another regex grep guide awk \u00b6 awk guide awk by example - hundreds of examples Rearranging columns \u00b6 cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column. Filtering based on criteria \u00b6 Print only lines that start with a comment (#) character cat example.tsv awk '$0 ~ \"^#\" { print }' bcftools \u00b6 bcftools manual Screen \u00b6 Screen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running nextflow because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection. Screen basics","title":"Bash"},{"location":"bash/#bash","text":"Bash Basic Commands More Advanced Good Guides grep awk Rearranging columns Filtering based on criteria bcftools Screen Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with this introduction to bash . Also check out this cheatsheet","title":"Bash"},{"location":"bash/#basic_commands","text":"You should familiarize yourself with the following commands. alias - create a shortcut for a command cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted.","title":"Basic Commands"},{"location":"bash/#more_advanced","text":"You should learn these once you have the basics down. git - version control awk - file manipulation; Filtering; Rearranging columns sed - quick find/replace","title":"More Advanced"},{"location":"bash/#good_guides","text":"Below are some good guides for various bash utilities.","title":"Good Guides"},{"location":"bash/#grep","text":"using grep with regular expressions another regex grep guide","title":"grep"},{"location":"bash/#awk","text":"awk guide awk by example - hundreds of examples","title":"awk"},{"location":"bash/#rearranging_columns","text":"cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column.","title":"Rearranging columns"},{"location":"bash/#filtering_based_on_criteria","text":"Print only lines that start with a comment (#) character cat example.tsv awk '$0 ~ \"^#\" { print }'","title":"Filtering based on criteria"},{"location":"bash/#bcftools","text":"bcftools manual","title":"bcftools"},{"location":"bash/#screen","text":"Screen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running nextflow because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection. Screen basics","title":"Screen"},{"location":"cendr/","text":"CeNDR \u00b6 CeNDR Setting up CeNDR Clone the repo Setup a python environment Download and install the gcloud-sdk Create a cendr gcloud configuration Install direnv Test flask Setup Credentials Load the database Test the site Creating a new release Uploading BAMs Uploading Release Data Bump the CeNDR version number and change-log Adding the release to the CeNDR website Adding isotype images Update the current variant datasets. Updating Publications Setting up CeNDR \u00b6 Clone the repo \u00b6 Clone the repo first. git clone http://www.github.com/andersenlab/cendr Switch to the development branch git checkout --track origin/development Setup a python environment \u00b6 Use miniconda as it will make your life much easier. The conda environment has been specified in the env.yaml file, and can be installed using: conda env create -f env.yaml Download and install the gcloud-sdk \u00b6 Install the gcloud-sdk Create a cendr gcloud configuration \u00b6 gcloud config configurations create cendr Install direnv \u00b6 direnv allows you to load a configuration file when you enter the development directory. Please read about how it works. CeNDR uses a .envrc file within the repo to set up the appropriate environmental variables. Once direnv is installed you can run direnv allow within the CeNDR repo: direnv allow Test flask \u00b6 With direnv enabled, you are nearly able to run the site locally. Run flask , and you should see the following: > flask Usage: flask [OPTIONS] COMMAND [ARGS]... A general utility script for Flask applications. Provides commands from Flask, extensions, and the application. Loads the application defined in the FLASK_APP environment variable, or from a wsgi.py file. Setting the FLASK_ENV environment variable to 'development' will enable debug mode. $ export FLASK_APP=hello.py $ export FLASK_ENV=development $ flask run Options: --version Show the flask version --help Show this message and exit. Commands: decrypt_credentials Decrypt credentials download_db Download the database (used in docker... initdb Initialize the database routes Show the routes for the app. run Run a development server. shell Runs a shell in the app context. update_credentials Update credentials update_strains Updates the strain table of the database If you do not see the full set of commands there - something is broken. Setup Credentials \u00b6 Authenticate with gcloud. Run the following command: mkdir -p env_config flask decrypt_credentials This will create a directory with the site credentials ( env_config ). Keep these secret. Important DO NOT COMMIT THESE CREDENTIALS TO GITHUB !!! Load the database \u00b6 The site uses an SQLite database that can be setup by running: flask download_db This will update the SQLite database used by CeNDR ( base/cendr.db ). The tables are: homologs - A table of homologs+orthologs. strain - Strain info pulled from the google spreadsheet C. elegans WI Strain Info . wormbase_gene - Summarizes gene information; Broken into component parts (e.g. exons, introns etc.). wormbase_gene_summary - Summarizes gene information. One line per gene. metadata - tracks how data was obtained. When. Where. etc. Test the site \u00b6 You can at this point test the site locally by running: flask run Be sure you have direnv. Otherwise you should source the .envrc file prior to running: source .envrc flask run Creating a new release \u00b6 Before a new release is possible, you must have first completed the following tasks: See Add new sequence data for further details . Add new wild isolate sequence data, and process with the trimmomatic-nf pipeline. Identified new isotypes using the concordance-nf Updated the C. elegans WI Strain Info spreadsheet, adding in new isotypes. Update the release column to reflect the release data in the C. elegans WI Strain Info spreadsheet Run and process sequence data with the wi-nf pipeline. Pushing a new release requires a series of steps described below. Uploading BAMs \u00b6 You will need AWS credentials to upload BAMs to Amazon S3. These are available in the secret credentials location. pip install aws-shell aws configure # Use s3 user credentials Once configured, navigate to the BAM location on b1059. cd /projects/b1059/data/alignments/WI/isotype # CD to bams folder... aws s3 sync . s3://elegansvariation.org/bam Run this command in screen to ensure that it completes (it's going to take a while) Uploading Release Data \u00b6 When you run the wi-nf pipeline it will create a folder with the format WI-YYYYMMDD . These data are output in a format that CeNDR can read as a release. You must upload the WI-YYYYMMDD folder to google storage with a command that looks like this: # First cd to the path of the results folder (WI-YYYYMMDD) from the `wi-nf` pipeline. gsutil rsync . gs://elegansvariation.org/releases/YYYYMMDD/ Important Use rsync to copy the files up to google storage. Note that the WI- prefix has been dropped from the YYYYMMDD declaration. Bump the CeNDR version number and change-log \u00b6 Because we are creating a new data release, we need to \"bump\" or move up the CeNDR version. The CeNDR version number is specified in a file at the base of the repo: travis.yml . Modify this line: - export VERSION_NUM=1-2-8 And increase the version number by 1 (e.g. 1-2-9). You should also update the change log and/or add a news item. The change-log is a markdown file located at base/static/content/help/Change-Log.md ; News items are located at base/static/news/ . Look at existing content to get an idea of how to add new items. It is fairly straightforward. You should be able to see changes on the test site. Adding the release to the CeNDR website \u00b6 After the site is loaded, the BAMs and release data are up, and the database is updated, you need to modify the file base/constants.py to add the new release. The date must match the date of the release that was uploaded. Add your release with the appropriate date and the annotation database used (e.g. (\"YYYYMMDD\", \"Annotation Database\") ). RELEASES = [(\"20180413\", \"WS263\"), (\"20170531\", \"WS258\"), (\"20160408\", \"WS245\")] Commit your changes to the development branch of CeNDR and push them to github. Once pushed, travis-ci will build the app and deploy it to a test branch. Use the google app engine interface to identify and test the app. If everything looks good open a pull request bringing the changes on the development branch to the master branch. Again, travis-ci will launch the new site. Important You need to shut down development instances and older versions of the site on the google-app engine interface once you are done testing/deploying new instances to prevent us from incurring charges for those running instances. Adding isotype images \u00b6 Isolation photos are initially prepared on dropbox and are located in the folder here: ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans Each file should be named using the isotype name and the strain name strain name in the following format: <isotype>_<strain>.jpg Then you will use imagemagick (a commandline-based utility) to scale the images down to 1000 pixels (width) and generate a 150px thumbnail. for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; Once you have generated the images you can upload them to google storage. They should be uploaded to the following location: gs://elegansvariation.org/photos/isolation You can drag/drop the photos using the web-based browser or use gsutil : # First cd to the appropriate directory # cd ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation The script for processing files is located in the dropbox folder and is called 'process_images.sh'. It's also here: for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; # Copy using rsync; Skip .DS_Store files. gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation Update the current variant datasets. \u00b6 The current folder located in gs://elegansvariation.org/releases contains the latest variant datasets and is used by WormBase to display natural variation data. Once you've completed a new release, update the files in this folder gs://elegansvariation.org/releases/current folder. Updating Publications \u00b6 The publications page ( /about/publications ) is generated using a google spreadsheet. The spreadsheet can be accessed here . You can request access to edit the spreadsheet by visiting that link. The last row of the spreadsheet contains a function that can fetch publication data from Pubmed using its API. Simply fill in column A with the PMID (Pubmed Identifier), and the publication data will be fetched. Once you have retrieved the latest pubmed data, create a new row and copy/paste the values for any new publications so they are not fetched from the Pubmed API. Alternatively, you can fill in the details for a publication manually. In either case, any details added should be double checked. Changes should be instant, but there may be some dely on the CeNDR website.","title":"CeNDR"},{"location":"cendr/#cendr","text":"CeNDR Setting up CeNDR Clone the repo Setup a python environment Download and install the gcloud-sdk Create a cendr gcloud configuration Install direnv Test flask Setup Credentials Load the database Test the site Creating a new release Uploading BAMs Uploading Release Data Bump the CeNDR version number and change-log Adding the release to the CeNDR website Adding isotype images Update the current variant datasets. Updating Publications","title":"CeNDR"},{"location":"cendr/#setting_up_cendr","text":"","title":"Setting up CeNDR"},{"location":"cendr/#clone_the_repo","text":"Clone the repo first. git clone http://www.github.com/andersenlab/cendr Switch to the development branch git checkout --track origin/development","title":"Clone the repo"},{"location":"cendr/#setup_a_python_environment","text":"Use miniconda as it will make your life much easier. The conda environment has been specified in the env.yaml file, and can be installed using: conda env create -f env.yaml","title":"Setup a python environment"},{"location":"cendr/#download_and_install_the_gcloud-sdk","text":"Install the gcloud-sdk","title":"Download and install the gcloud-sdk"},{"location":"cendr/#create_a_cendr_gcloud_configuration","text":"gcloud config configurations create cendr","title":"Create a cendr gcloud configuration"},{"location":"cendr/#install_direnv","text":"direnv allows you to load a configuration file when you enter the development directory. Please read about how it works. CeNDR uses a .envrc file within the repo to set up the appropriate environmental variables. Once direnv is installed you can run direnv allow within the CeNDR repo: direnv allow","title":"Install direnv"},{"location":"cendr/#test_flask","text":"With direnv enabled, you are nearly able to run the site locally. Run flask , and you should see the following: > flask Usage: flask [OPTIONS] COMMAND [ARGS]... A general utility script for Flask applications. Provides commands from Flask, extensions, and the application. Loads the application defined in the FLASK_APP environment variable, or from a wsgi.py file. Setting the FLASK_ENV environment variable to 'development' will enable debug mode. $ export FLASK_APP=hello.py $ export FLASK_ENV=development $ flask run Options: --version Show the flask version --help Show this message and exit. Commands: decrypt_credentials Decrypt credentials download_db Download the database (used in docker... initdb Initialize the database routes Show the routes for the app. run Run a development server. shell Runs a shell in the app context. update_credentials Update credentials update_strains Updates the strain table of the database If you do not see the full set of commands there - something is broken.","title":"Test flask"},{"location":"cendr/#setup_credentials","text":"Authenticate with gcloud. Run the following command: mkdir -p env_config flask decrypt_credentials This will create a directory with the site credentials ( env_config ). Keep these secret. Important DO NOT COMMIT THESE CREDENTIALS TO GITHUB !!!","title":"Setup Credentials"},{"location":"cendr/#load_the_database","text":"The site uses an SQLite database that can be setup by running: flask download_db This will update the SQLite database used by CeNDR ( base/cendr.db ). The tables are: homologs - A table of homologs+orthologs. strain - Strain info pulled from the google spreadsheet C. elegans WI Strain Info . wormbase_gene - Summarizes gene information; Broken into component parts (e.g. exons, introns etc.). wormbase_gene_summary - Summarizes gene information. One line per gene. metadata - tracks how data was obtained. When. Where. etc.","title":"Load the database"},{"location":"cendr/#test_the_site","text":"You can at this point test the site locally by running: flask run Be sure you have direnv. Otherwise you should source the .envrc file prior to running: source .envrc flask run","title":"Test the site"},{"location":"cendr/#creating_a_new_release","text":"Before a new release is possible, you must have first completed the following tasks: See Add new sequence data for further details . Add new wild isolate sequence data, and process with the trimmomatic-nf pipeline. Identified new isotypes using the concordance-nf Updated the C. elegans WI Strain Info spreadsheet, adding in new isotypes. Update the release column to reflect the release data in the C. elegans WI Strain Info spreadsheet Run and process sequence data with the wi-nf pipeline. Pushing a new release requires a series of steps described below.","title":"Creating a new release"},{"location":"cendr/#uploading_bams","text":"You will need AWS credentials to upload BAMs to Amazon S3. These are available in the secret credentials location. pip install aws-shell aws configure # Use s3 user credentials Once configured, navigate to the BAM location on b1059. cd /projects/b1059/data/alignments/WI/isotype # CD to bams folder... aws s3 sync . s3://elegansvariation.org/bam Run this command in screen to ensure that it completes (it's going to take a while)","title":"Uploading BAMs"},{"location":"cendr/#uploading_release_data","text":"When you run the wi-nf pipeline it will create a folder with the format WI-YYYYMMDD . These data are output in a format that CeNDR can read as a release. You must upload the WI-YYYYMMDD folder to google storage with a command that looks like this: # First cd to the path of the results folder (WI-YYYYMMDD) from the `wi-nf` pipeline. gsutil rsync . gs://elegansvariation.org/releases/YYYYMMDD/ Important Use rsync to copy the files up to google storage. Note that the WI- prefix has been dropped from the YYYYMMDD declaration.","title":"Uploading Release Data"},{"location":"cendr/#bump_the_cendr_version_number_and_change-log","text":"Because we are creating a new data release, we need to \"bump\" or move up the CeNDR version. The CeNDR version number is specified in a file at the base of the repo: travis.yml . Modify this line: - export VERSION_NUM=1-2-8 And increase the version number by 1 (e.g. 1-2-9). You should also update the change log and/or add a news item. The change-log is a markdown file located at base/static/content/help/Change-Log.md ; News items are located at base/static/news/ . Look at existing content to get an idea of how to add new items. It is fairly straightforward. You should be able to see changes on the test site.","title":"Bump the CeNDR version number and change-log"},{"location":"cendr/#adding_the_release_to_the_cendr_website","text":"After the site is loaded, the BAMs and release data are up, and the database is updated, you need to modify the file base/constants.py to add the new release. The date must match the date of the release that was uploaded. Add your release with the appropriate date and the annotation database used (e.g. (\"YYYYMMDD\", \"Annotation Database\") ). RELEASES = [(\"20180413\", \"WS263\"), (\"20170531\", \"WS258\"), (\"20160408\", \"WS245\")] Commit your changes to the development branch of CeNDR and push them to github. Once pushed, travis-ci will build the app and deploy it to a test branch. Use the google app engine interface to identify and test the app. If everything looks good open a pull request bringing the changes on the development branch to the master branch. Again, travis-ci will launch the new site. Important You need to shut down development instances and older versions of the site on the google-app engine interface once you are done testing/deploying new instances to prevent us from incurring charges for those running instances.","title":"Adding the release to the CeNDR website"},{"location":"cendr/#adding_isotype_images","text":"Isolation photos are initially prepared on dropbox and are located in the folder here: ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans Each file should be named using the isotype name and the strain name strain name in the following format: <isotype>_<strain>.jpg Then you will use imagemagick (a commandline-based utility) to scale the images down to 1000 pixels (width) and generate a 150px thumbnail. for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; Once you have generated the images you can upload them to google storage. They should be uploaded to the following location: gs://elegansvariation.org/photos/isolation You can drag/drop the photos using the web-based browser or use gsutil : # First cd to the appropriate directory # cd ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation The script for processing files is located in the dropbox folder and is called 'process_images.sh'. It's also here: for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; # Copy using rsync; Skip .DS_Store files. gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation","title":"Adding isotype images"},{"location":"cendr/#update_the_current_variant_datasets","text":"The current folder located in gs://elegansvariation.org/releases contains the latest variant datasets and is used by WormBase to display natural variation data. Once you've completed a new release, update the files in this folder gs://elegansvariation.org/releases/current folder.","title":"Update the current variant datasets."},{"location":"cendr/#updating_publications","text":"The publications page ( /about/publications ) is generated using a google spreadsheet. The spreadsheet can be accessed here . You can request access to edit the spreadsheet by visiting that link. The last row of the spreadsheet contains a function that can fetch publication data from Pubmed using its API. Simply fill in column A with the PMID (Pubmed Identifier), and the publication data will be fetched. Once you have retrieved the latest pubmed data, create a new row and copy/paste the values for any new publications so they are not fetched from the Pubmed API. Alternatively, you can fill in the details for a publication manually. In either case, any details added should be double checked. Changes should be instant, but there may be some dely on the CeNDR website.","title":"Updating Publications"},{"location":"cloud/","text":"AndersenLab cloud resources \u00b6 AndersenLab cloud resources Google Domains Google Cloud Google Cloud Storage Buckets elegansvariation.org andersenlab.org Other buckets Secret Bucket cegwas (deprecated) Google datastore App engine Error Reporting BigQuery AWS S3 Fargate For full documentation visit mkdocs.org . Google Domains \u00b6 Any domain names the lab uses should be registered with Google Domains. The two ones currently are: andersenlab.org elegansvariation.org Google domains can be used to forward domain-specific email addresses if necessary. For example, example@andersenlab.org could be created and forwarded to an email address. Google Cloud \u00b6 Google cloud is used for a variety of services that the lab uses. Google Cloud Storage \u00b6 Google cloud storage is used to store and distribute files on CeNDR, for cegwas, and for some files on the lab website. Buckets \u00b6 Files are grouped into 'buckets' on google storage. We use the following buckets: elegansvariation.org \u00b6 This bucket contains all the data associated with elegansvariation.org. It is broken down into six primary directories. browser_tracks - for genome-browser tracks that rarely if ever change. db - Storage/access to the SQLite database. photos - sample collection photos. releases - dataset releases. For more detail, see post-gatk-nf . reports - images and data files within reports. static - static assets used by the site. bam - stores all BAM files at the strain level andersenlab.org \u00b6 In some cases the data associated with a publication is too large to put on github. We store those data here, along with a couple other odds and ends. Other buckets \u00b6 Google Cloud creates a bunch of other buckets too. Most of these should be ignored as they are part of the Secret Bucket \u00b6 There is one other secret bucket. Ask Erik about it. cegwas (deprecated) \u00b6 Currently this bucket has a SQLite database used by cegwas. This will be replaced to use the same database used by CeNDR in the db/ folder. Once the new version of cegwas is developed - this section should be removed, and that bucket should be deleted. Google datastore \u00b6 Google datastore used as the database for CeNDR. It stores information on mappings, traits, and more. App engine \u00b6 App engine is the platform CeNDR runs on. Error Reporting \u00b6 Google cloud contains a really nice error reporting interface. Error reports are generated whenever something goes wrong on CeNDR. A github issue can be created for these errors and they can be addressed. BigQuery \u00b6 We have used bigquery in the past for large query jobs. We are not actively using it as of late. AWS \u00b6 S3 \u00b6 In the past we stored BAMs on AWS at elegansvariation.org , however these data have now been migrated to GCP as of the 20210121 CeNDR release. Fargate \u00b6 Amazon Fargate was used to run the mapping pipeline on CeNDR in the past, but this is now moved to GCP as of 2021","title":"Cloud"},{"location":"cloud/#andersenlab_cloud_resources","text":"AndersenLab cloud resources Google Domains Google Cloud Google Cloud Storage Buckets elegansvariation.org andersenlab.org Other buckets Secret Bucket cegwas (deprecated) Google datastore App engine Error Reporting BigQuery AWS S3 Fargate For full documentation visit mkdocs.org .","title":"AndersenLab cloud resources"},{"location":"cloud/#google_domains","text":"Any domain names the lab uses should be registered with Google Domains. The two ones currently are: andersenlab.org elegansvariation.org Google domains can be used to forward domain-specific email addresses if necessary. For example, example@andersenlab.org could be created and forwarded to an email address.","title":"Google Domains"},{"location":"cloud/#google_cloud","text":"Google cloud is used for a variety of services that the lab uses.","title":"Google Cloud"},{"location":"cloud/#google_cloud_storage","text":"Google cloud storage is used to store and distribute files on CeNDR, for cegwas, and for some files on the lab website.","title":"Google Cloud Storage"},{"location":"cloud/#buckets","text":"Files are grouped into 'buckets' on google storage. We use the following buckets:","title":"Buckets"},{"location":"cloud/#elegansvariationorg","text":"This bucket contains all the data associated with elegansvariation.org. It is broken down into six primary directories. browser_tracks - for genome-browser tracks that rarely if ever change. db - Storage/access to the SQLite database. photos - sample collection photos. releases - dataset releases. For more detail, see post-gatk-nf . reports - images and data files within reports. static - static assets used by the site. bam - stores all BAM files at the strain level","title":"elegansvariation.org"},{"location":"cloud/#andersenlaborg","text":"In some cases the data associated with a publication is too large to put on github. We store those data here, along with a couple other odds and ends.","title":"andersenlab.org"},{"location":"cloud/#other_buckets","text":"Google Cloud creates a bunch of other buckets too. Most of these should be ignored as they are part of the","title":"Other buckets"},{"location":"cloud/#secret_bucket","text":"There is one other secret bucket. Ask Erik about it.","title":"Secret Bucket"},{"location":"cloud/#cegwas_deprecated","text":"Currently this bucket has a SQLite database used by cegwas. This will be replaced to use the same database used by CeNDR in the db/ folder. Once the new version of cegwas is developed - this section should be removed, and that bucket should be deleted.","title":"cegwas (deprecated)"},{"location":"cloud/#google_datastore","text":"Google datastore used as the database for CeNDR. It stores information on mappings, traits, and more.","title":"Google datastore"},{"location":"cloud/#app_engine","text":"App engine is the platform CeNDR runs on.","title":"App engine"},{"location":"cloud/#error_reporting","text":"Google cloud contains a really nice error reporting interface. Error reports are generated whenever something goes wrong on CeNDR. A github issue can be created for these errors and they can be addressed.","title":"Error Reporting"},{"location":"cloud/#bigquery","text":"We have used bigquery in the past for large query jobs. We are not actively using it as of late.","title":"BigQuery"},{"location":"cloud/#aws","text":"","title":"AWS"},{"location":"cloud/#s3","text":"In the past we stored BAMs on AWS at elegansvariation.org , however these data have now been migrated to GCP as of the 20210121 CeNDR release.","title":"S3"},{"location":"cloud/#fargate","text":"Amazon Fargate was used to run the mapping pipeline on CeNDR in the past, but this is now moved to GCP as of 2021","title":"Fargate"},{"location":"github/","text":"Git and Github \u00b6 Git and Github Introduction Using Git and Github The GitHub Flow Command Line git commands GitHub Flow Best Practices Resources Introduction \u00b6 Git is a version control software package similar to traccked changes and saving documents as \"final1.pdf\" and \"final2.pdf\" Github is a 3rd party web-based graphical interface that has a copy of the project that you and/or other people can push and pull from to work on the same code simultaneously. Keep in mind, its not like google docs, it doesn\u2019t update automatically, requires you to push changes and pull changes to the new computer. Note You must install git to use and create an account on Github. Check out this intro guide here Using Git and Github \u00b6 The Andersen Lab Github can be found here . As of the writing of this page, we have 165 \"repositories\" (or projects). Notice some projects are code like NemaScan and others are personal projects ( abamectin ) or manuscripts ( mol_eco_manuscript ). Anything can be a repo! There are two main ways to use Git: (1) on the command line (aka Terminal on Macs) or with a GUI (graphical user interface). Both are good and neither are \"wrong\". For new users, it is usually easier to start with a GUI like Github Desktop . However, only a few basic commands are really necessary to get started using git on the command line, so don't be nervous! Important Git GUI like Github Desktop cannot be used with repositories on QUEST. If you are building a pipeline on QUEST, it is essential to get comfortable with using Git on the command line The GitHub Flow \u00b6 There are several different Git branching strategies, but the most popular for our lab is the \"GitHub Flow\". This 8 step process can help keep our pipelines flowing, functional, and organized. New to the GitHub flow? I highly recommend you try out this amazing tutorial to practice all the steps from beginning to end. Then start putting it in use for your own pipelines! The following 8 steps can be done on the command line or with a GUI. Below I will show the basic git commands for managing a repo on the command line, for more help you can find the slides from Code club at ~/Dropbox/AndersenLab/LabMeetings/CodeClub/20210326_KSE/20210326_slides.key Note When you are maintaining a project repo that only you are updating, it is less important to follow the GitHub Flow with creating short-lived branches. However, if you are developing/maintaining code that other people will use and/or working collaboratively this is an essential skill to master. Clone/pull # Cloning - new repo cd < directory you want repo stored > git clone https://github.com/AndersenLab/code_club.git cd code_club # pulling - already cloned repo you want to get newest version of cd < directory of repo > git pull Branch # create a new branch AND move to it git checkout -b <branch_name> # list all available branches git branch # move to a branch git checkout < name of branch > Edit No code here... make any edits to the repo. Commit # first step - add changed files to staging area git add <changed file> # OR add ALL files to staging area git add . # commit files in staging area git commit -m \"<some message about what changes you made>\" Push # push changes to remote git push # when it is your first time pushing a new branch, it might prompt you to set an upstream branch: git push --set-upstream origin new_branch Pull request I generally like to do this step online at github.com because I think it is useful to visually see the changes I made go to the repo site click the green \"compare and pull request\" button check the branches are right at the top: which branch is merging into which branch optional: assing reviewer and/or assignees on the right hand side. This is often useful when coding collaboratively update the title/comment for the pull request to let yourself and others know what changes were made and why Inspect I generally like to do this step online at github.com because I think it is useful to visually see the changes I made. If you scroll down you should be able to see which files were changed and what exact changes were made. If there are merge conflicts, github will walk you through fixing them. Merge When you are satisfied with your merge, click the green \"merge pull request\" button. Also make sure the delete the old branch when you are done as part of keeping the repo clean and clutter-free Note Good practice is to make a new branch to implement a new feature, then delete the branch once it has been merged. To start a new feature, open a NEW branch. Not as important on self-projects, but very important for collaboration Command Line git commands \u00b6 Basic git clone - clone remote repository git pull - pull most recent version from remote git add - add local files to be staged for remote git commit - stage/commit local changes git push - push local commits to remote Intermediate git branch - list all available branches git checkout - move to new branch git status - checks which branch you are on and if you have any unsaved changes git log - shows log of previous commits on current branch git diff - shows details of changes made For more, check out this tutorial, and others. GitHub Flow Best Practices \u00b6 Any code in the main branch should be deployable Create new descriptively-named branches off the main branch for new work such as feature/add-new-plot Commit new work to your local branches and regularly push work to the remote To request feedback or help, or when you think your work is ready to merge into the main branch, open a pull request After your work or feature has been reviewed and approved, it can be merged into the main branch Delete stale branches! Once your work has been merged into the main branch, it should be deployed immediately Note GitHub Flow is not the only branching strategy out there! This was a great article about the three most common strategies with pros and cons for why you might use each one. I challenge you to think aobut which strategy might be best for our lab moving forward and let's start a discussion about it! Resources \u00b6 This blog on the differences between git and github \"Git started\" using Git on the command line here Overview of top Git GUI from 2021 here Great intro video to the GitHub Flow HIGHLY RECOMMENDED introduction tutorial to GitHub Flow Amazing article on different git branch strategies here","title":"Git and Github"},{"location":"github/#git_and_github","text":"Git and Github Introduction Using Git and Github The GitHub Flow Command Line git commands GitHub Flow Best Practices Resources","title":"Git and Github"},{"location":"github/#introduction","text":"Git is a version control software package similar to traccked changes and saving documents as \"final1.pdf\" and \"final2.pdf\" Github is a 3rd party web-based graphical interface that has a copy of the project that you and/or other people can push and pull from to work on the same code simultaneously. Keep in mind, its not like google docs, it doesn\u2019t update automatically, requires you to push changes and pull changes to the new computer. Note You must install git to use and create an account on Github. Check out this intro guide here","title":"Introduction"},{"location":"github/#using_git_and_github","text":"The Andersen Lab Github can be found here . As of the writing of this page, we have 165 \"repositories\" (or projects). Notice some projects are code like NemaScan and others are personal projects ( abamectin ) or manuscripts ( mol_eco_manuscript ). Anything can be a repo! There are two main ways to use Git: (1) on the command line (aka Terminal on Macs) or with a GUI (graphical user interface). Both are good and neither are \"wrong\". For new users, it is usually easier to start with a GUI like Github Desktop . However, only a few basic commands are really necessary to get started using git on the command line, so don't be nervous! Important Git GUI like Github Desktop cannot be used with repositories on QUEST. If you are building a pipeline on QUEST, it is essential to get comfortable with using Git on the command line","title":"Using Git and Github"},{"location":"github/#the_github_flow","text":"There are several different Git branching strategies, but the most popular for our lab is the \"GitHub Flow\". This 8 step process can help keep our pipelines flowing, functional, and organized. New to the GitHub flow? I highly recommend you try out this amazing tutorial to practice all the steps from beginning to end. Then start putting it in use for your own pipelines! The following 8 steps can be done on the command line or with a GUI. Below I will show the basic git commands for managing a repo on the command line, for more help you can find the slides from Code club at ~/Dropbox/AndersenLab/LabMeetings/CodeClub/20210326_KSE/20210326_slides.key Note When you are maintaining a project repo that only you are updating, it is less important to follow the GitHub Flow with creating short-lived branches. However, if you are developing/maintaining code that other people will use and/or working collaboratively this is an essential skill to master. Clone/pull # Cloning - new repo cd < directory you want repo stored > git clone https://github.com/AndersenLab/code_club.git cd code_club # pulling - already cloned repo you want to get newest version of cd < directory of repo > git pull Branch # create a new branch AND move to it git checkout -b <branch_name> # list all available branches git branch # move to a branch git checkout < name of branch > Edit No code here... make any edits to the repo. Commit # first step - add changed files to staging area git add <changed file> # OR add ALL files to staging area git add . # commit files in staging area git commit -m \"<some message about what changes you made>\" Push # push changes to remote git push # when it is your first time pushing a new branch, it might prompt you to set an upstream branch: git push --set-upstream origin new_branch Pull request I generally like to do this step online at github.com because I think it is useful to visually see the changes I made go to the repo site click the green \"compare and pull request\" button check the branches are right at the top: which branch is merging into which branch optional: assing reviewer and/or assignees on the right hand side. This is often useful when coding collaboratively update the title/comment for the pull request to let yourself and others know what changes were made and why Inspect I generally like to do this step online at github.com because I think it is useful to visually see the changes I made. If you scroll down you should be able to see which files were changed and what exact changes were made. If there are merge conflicts, github will walk you through fixing them. Merge When you are satisfied with your merge, click the green \"merge pull request\" button. Also make sure the delete the old branch when you are done as part of keeping the repo clean and clutter-free Note Good practice is to make a new branch to implement a new feature, then delete the branch once it has been merged. To start a new feature, open a NEW branch. Not as important on self-projects, but very important for collaboration","title":"The GitHub Flow"},{"location":"github/#command_line_git_commands","text":"Basic git clone - clone remote repository git pull - pull most recent version from remote git add - add local files to be staged for remote git commit - stage/commit local changes git push - push local commits to remote Intermediate git branch - list all available branches git checkout - move to new branch git status - checks which branch you are on and if you have any unsaved changes git log - shows log of previous commits on current branch git diff - shows details of changes made For more, check out this tutorial, and others.","title":"Command Line git commands"},{"location":"github/#github_flow_best_practices","text":"Any code in the main branch should be deployable Create new descriptively-named branches off the main branch for new work such as feature/add-new-plot Commit new work to your local branches and regularly push work to the remote To request feedback or help, or when you think your work is ready to merge into the main branch, open a pull request After your work or feature has been reviewed and approved, it can be merged into the main branch Delete stale branches! Once your work has been merged into the main branch, it should be deployed immediately Note GitHub Flow is not the only branching strategy out there! This was a great article about the three most common strategies with pros and cons for why you might use each one. I challenge you to think aobut which strategy might be best for our lab moving forward and let's start a discussion about it!","title":"GitHub Flow Best Practices"},{"location":"github/#resources","text":"This blog on the differences between git and github \"Git started\" using Git on the command line here Overview of top Git GUI from 2021 here Great intro video to the GitHub Flow HIGHLY RECOMMENDED introduction tutorial to GitHub Flow Amazing article on different git branch strategies here","title":"Resources"},{"location":"labsite/","text":"Andersenlab.org \u00b6 Andersenlab.org Getting Started Software-Dependencies Cloning the repo Updating the site andersenlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software Getting Started \u00b6 The Andersen Lab website was built using jekyll and runs using the Github Pages service. Software-Dependencies \u00b6 Several software packages are required for editing/maintaining the Andersen Lab site. They can be installed using Homebrew : brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml . Cloning the repo \u00b6 To get started editing, clone the repo: git clone https://github.com/andersenlab/andersenlab.github.io This repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url. Updating the site \u00b6 In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git. andersenlab.github.io \u00b6 The structure of the Andersen Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with Announcements \u00b6 Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication. General Announcements \u00b6 To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more. Publication Post \u00b6 New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Katie's paper accepted at <em>G3</em>!\" subtitle: \"Correlations of geneotype with climate parameters suggest <em>Caenorhabditis elegans</em> niche adaptations\" layout: post tags: news published: true PMID: 27866149 --- Congratulations to Katie for her paper accepted at G3! Lab members \u00b6 Adding new lab members: \u00b6 (1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page. Set Status to Former \u00b6 Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Mostafa pub_names: - Zamanian M last_name: Zamanian description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2015-2017 photo: Mostafa2014.jpg former: true github: mzamanian email: zamanian@northwestern.edu current_status: Assistant Professor at UW Madison -- <a href='http://www.zamanianlab.org/'>Zamanian Lab Website</a> Remove lab members \u00b6 Remove the persons information from _data/people.yaml ; Optionally delete their photo. Funding \u00b6 Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: nigms: https://www.nigms.nih.gov/Pages/default.aspx acs: http://www.cancer.org/ pew: http://www.pewtrusts.org/en niaid: https://www.niaid.nih.gov/ aws: https://aws.amazon.com/ weinberg: http://www.weinberg.northwestern.edu/ mod: http://www.marchofdimes.org/ cbc: http://www.chicagobiomedicalconsortium.org/ Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here. Protocols \u00b6 Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>C. elegans</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Brood Assay file: SemiQuantitativeBroodAssay.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Pseudomonas aeruginosa</em> Fast-killing assay</a> file: FKAprotocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Staphylococcus aureus</em> killing assay</a> file: Staphaureus_Protocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Bacillus thuringiensis</em> toxin assay on plates</a> file: Bacillus-thuringiensis-toxin-plate-assay.pdf group: <em>C. elegans</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines. Research \u00b6 The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: High-throughput approaches to understand conserved drug responses image: worms_drugs2.jpg order: 1 --- Because of the efforts of a number of highly dedicated scientists and citizen volunteers... To this end, we deep sequenced all of these strains... The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000). Publications \u00b6 Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the Caenorhabditis elegans species You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Caenorhabditis elegans</em> Final result: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Date_Published: 2017 May 09 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the <em>Caenorhabditis elegans</em> species PDF: 2017Laricchia (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2017Laricchia has an associated folder in publications/ where supplemental data and figures are stored: publications/2017Laricchia/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2017Laricchia : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page. Photo Albums \u00b6 Photo albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2017-08-05-Hawaii Trip . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the andersenlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push Software \u00b6 If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Andersen Labsite"},{"location":"labsite/#andersenlaborg","text":"Andersenlab.org Getting Started Software-Dependencies Cloning the repo Updating the site andersenlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software","title":"Andersenlab.org"},{"location":"labsite/#getting_started","text":"The Andersen Lab website was built using jekyll and runs using the Github Pages service.","title":"Getting Started"},{"location":"labsite/#software-dependencies","text":"Several software packages are required for editing/maintaining the Andersen Lab site. They can be installed using Homebrew : brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml .","title":"Software-Dependencies"},{"location":"labsite/#cloning_the_repo","text":"To get started editing, clone the repo: git clone https://github.com/andersenlab/andersenlab.github.io This repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url.","title":"Cloning the repo"},{"location":"labsite/#updating_the_site","text":"In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git.","title":"Updating the site"},{"location":"labsite/#andersenlabgithubio","text":"The structure of the Andersen Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with","title":"andersenlab.github.io"},{"location":"labsite/#announcements","text":"Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication.","title":"Announcements"},{"location":"labsite/#general_announcements","text":"To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more.","title":"General Announcements"},{"location":"labsite/#publication_post","text":"New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Katie's paper accepted at <em>G3</em>!\" subtitle: \"Correlations of geneotype with climate parameters suggest <em>Caenorhabditis elegans</em> niche adaptations\" layout: post tags: news published: true PMID: 27866149 --- Congratulations to Katie for her paper accepted at G3!","title":"Publication Post"},{"location":"labsite/#lab_members","text":"","title":"Lab members"},{"location":"labsite/#adding_new_lab_members","text":"(1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page.","title":"Adding new lab members:"},{"location":"labsite/#set_status_to_former","text":"Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Mostafa pub_names: - Zamanian M last_name: Zamanian description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2015-2017 photo: Mostafa2014.jpg former: true github: mzamanian email: zamanian@northwestern.edu current_status: Assistant Professor at UW Madison -- <a href='http://www.zamanianlab.org/'>Zamanian Lab Website</a>","title":"Set Status to Former"},{"location":"labsite/#remove_lab_members","text":"Remove the persons information from _data/people.yaml ; Optionally delete their photo.","title":"Remove lab members"},{"location":"labsite/#funding","text":"Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: nigms: https://www.nigms.nih.gov/Pages/default.aspx acs: http://www.cancer.org/ pew: http://www.pewtrusts.org/en niaid: https://www.niaid.nih.gov/ aws: https://aws.amazon.com/ weinberg: http://www.weinberg.northwestern.edu/ mod: http://www.marchofdimes.org/ cbc: http://www.chicagobiomedicalconsortium.org/ Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here.","title":"Funding"},{"location":"labsite/#protocols","text":"Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>C. elegans</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Brood Assay file: SemiQuantitativeBroodAssay.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Pseudomonas aeruginosa</em> Fast-killing assay</a> file: FKAprotocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Staphylococcus aureus</em> killing assay</a> file: Staphaureus_Protocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Bacillus thuringiensis</em> toxin assay on plates</a> file: Bacillus-thuringiensis-toxin-plate-assay.pdf group: <em>C. elegans</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines.","title":"Protocols"},{"location":"labsite/#research","text":"The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: High-throughput approaches to understand conserved drug responses image: worms_drugs2.jpg order: 1 --- Because of the efforts of a number of highly dedicated scientists and citizen volunteers... To this end, we deep sequenced all of these strains... The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000).","title":"Research"},{"location":"labsite/#publications","text":"Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the Caenorhabditis elegans species You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Caenorhabditis elegans</em> Final result: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Date_Published: 2017 May 09 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the <em>Caenorhabditis elegans</em> species PDF: 2017Laricchia (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2017Laricchia has an associated folder in publications/ where supplemental data and figures are stored: publications/2017Laricchia/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2017Laricchia : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page.","title":"Publications"},{"location":"labsite/#photo_albums","text":"Photo albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2017-08-05-Hawaii Trip . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the andersenlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push","title":"Photo Albums"},{"location":"labsite/#software","text":"If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Software"},{"location":"pipeline-GCPconfig/","text":"Running Nextflow pipeline on GCP \u00b6 Running Nextflow pipeline on GCP Enable API Create service account Generate credential for the service account Nextflow version and mode Configure Nextflow for GCP Google genomic API allows auto scale for computational resources by creating and closing VMs automatically. We have a dedicated google project caendr which using Google genomic APT for all the nextflow pipelines in our lab. To access it, you should provide your gmail accout to Erik and ask Erik give you a project owner role for caendr . I already preset the project to enable running Nextflow pipelines using Google genomic API . See below for more details. Enable API \u00b6 Go the the main page of google cloud platform . In the Produck & Services menu, click APIs & Services , and then click Enable APIs and Services . The following APIs should be enabled to run nextflow pipeline on GCP. Genomics API Cloud Life Sciences API Compute Engine API Google Container Registry API Create service account \u00b6 Go to the IAM & admin , find the Service accounts . Click CREATE SERVICE ACCOUNT to create a new service accounts. Note this service accounts must have a project owner role to run Nextflow pipelines. The service account I created here is called nextflowRUN . You don't need to do the above processes when you use GCP. But you have to do all the following processes to make sure you have the right permissions to caendr . Generate credential for the service account \u00b6 After you get the access to caendr , go to the API & Services . Click the Create credentials button, select Service account key . And choose nextflowRUN to generate a JSON file, which is a privite key file for using nextflowRUN . Download the file and save it in a safe place. Finally, define the GOOGLE_APPLICATION_CREDENTIALS variable in .bash_profile with the directory of the JSON file. Which should looks like the example. export GOOGLE_APPLICATION_CREDENTIALS=$HOME/google_creds/caendr-2cae6210c8d1.json Nextflow version and mode \u00b6 Only the version of 19.07.0 or higher of Nextflow are compatible with GCP. And also, the Nextflow should have a google mode. You can define the version and mode in .bash_profile . export NXF_VER=19.07.0 export NXF_MODE=google Then, run the following code to update or install Nextflow. curl https://get.nextflow.io | bash Configure Nextflow for GCP \u00b6 To run Nextflow pipelines on GCP, you need to build docker images for them. Check the docker file repo of our lab for more information. The google genomic API has its own executor called google-pipelines , you need to define the executor variable with google-pipelines in the nextflow.config file. Here is the example for concordance-nf . docker { enabled = true } process { executor = 'google-pipelines' withLabel: bam_coverage { container = 'faithman/bam_toolbox:latest' } container = 'faithman/concordance:latest' machineType = 'n1-standard-4' } google { project = 'caendr' zone = 'us-central1-a' } cloud { preemptible = true } executor { queueSize = 500 } Important The file system of google buckets is not like S3 that can read/write directly by most softwares. You have to use gsutil tool to interact with google buckets to read/write files in most situations. Nextflow has built-in functions to interact with google buckets, but you still can not read/write files directly in your script. All the files have to be read and write via channels in Nextflow!","title":"GCP"},{"location":"pipeline-GCPconfig/#running_nextflow_pipeline_on_gcp","text":"Running Nextflow pipeline on GCP Enable API Create service account Generate credential for the service account Nextflow version and mode Configure Nextflow for GCP Google genomic API allows auto scale for computational resources by creating and closing VMs automatically. We have a dedicated google project caendr which using Google genomic APT for all the nextflow pipelines in our lab. To access it, you should provide your gmail accout to Erik and ask Erik give you a project owner role for caendr . I already preset the project to enable running Nextflow pipelines using Google genomic API . See below for more details.","title":"Running Nextflow pipeline on GCP"},{"location":"pipeline-GCPconfig/#enable_api","text":"Go the the main page of google cloud platform . In the Produck & Services menu, click APIs & Services , and then click Enable APIs and Services . The following APIs should be enabled to run nextflow pipeline on GCP. Genomics API Cloud Life Sciences API Compute Engine API Google Container Registry API","title":"Enable API"},{"location":"pipeline-GCPconfig/#create_service_account","text":"Go to the IAM & admin , find the Service accounts . Click CREATE SERVICE ACCOUNT to create a new service accounts. Note this service accounts must have a project owner role to run Nextflow pipelines. The service account I created here is called nextflowRUN . You don't need to do the above processes when you use GCP. But you have to do all the following processes to make sure you have the right permissions to caendr .","title":"Create service account"},{"location":"pipeline-GCPconfig/#generate_credential_for_the_service_account","text":"After you get the access to caendr , go to the API & Services . Click the Create credentials button, select Service account key . And choose nextflowRUN to generate a JSON file, which is a privite key file for using nextflowRUN . Download the file and save it in a safe place. Finally, define the GOOGLE_APPLICATION_CREDENTIALS variable in .bash_profile with the directory of the JSON file. Which should looks like the example. export GOOGLE_APPLICATION_CREDENTIALS=$HOME/google_creds/caendr-2cae6210c8d1.json","title":"Generate credential for the service account"},{"location":"pipeline-GCPconfig/#nextflow_version_and_mode","text":"Only the version of 19.07.0 or higher of Nextflow are compatible with GCP. And also, the Nextflow should have a google mode. You can define the version and mode in .bash_profile . export NXF_VER=19.07.0 export NXF_MODE=google Then, run the following code to update or install Nextflow. curl https://get.nextflow.io | bash","title":"Nextflow version and mode"},{"location":"pipeline-GCPconfig/#configure_nextflow_for_gcp","text":"To run Nextflow pipelines on GCP, you need to build docker images for them. Check the docker file repo of our lab for more information. The google genomic API has its own executor called google-pipelines , you need to define the executor variable with google-pipelines in the nextflow.config file. Here is the example for concordance-nf . docker { enabled = true } process { executor = 'google-pipelines' withLabel: bam_coverage { container = 'faithman/bam_toolbox:latest' } container = 'faithman/concordance:latest' machineType = 'n1-standard-4' } google { project = 'caendr' zone = 'us-central1-a' } cloud { preemptible = true } executor { queueSize = 500 } Important The file system of google buckets is not like S3 that can read/write directly by most softwares. You have to use gsutil tool to interact with google buckets to read/write files in most situations. Nextflow has built-in functions to interact with google buckets, but you still can not read/write files directly in your script. All the files have to be read and write via channels in Nextflow!","title":"Configure Nextflow for GCP"},{"location":"pipeline-alignment/","text":"alignment-nf \u00b6 alignment-nf Pipeline overview Software requirements Usage Testing on Quest Running on Quest Parameters -profile --sample_sheet --debug (optional) --fq_prefix (optional) --kmers (optional) --reference (optional) --output (optional) Output Data storage Cleanup Archive construct_sample_sheet.sh Adding new sequencing datasets The alignment-nf pipeline performs alignment for wild isolate sequence data at the strain level , and outputs BAMs and related information. Those BAMs can be used for downstream analysis including variant calling, concordance analysis , wi-gatk-nf (variant calling) and other analyses. This page details how to run the pipeline and how to add new wild isolate sequencing data. Note Historically, sequence processing was performed at the isotype level. We are still interested in filtering strains used in analysis at the isotype level, but alignment and variant calling are now performed at the strain level rather than at the isotype level. Pipeline overview \u00b6 \u2597\u2596 \u259d\u259c \u259d \u2597 \u2597\u2596 \u2596\u2597\u2584\u2584\u2596 \u2590\u258c \u2590 \u2597\u2584 \u2584\u2584 \u2597\u2597\u2596 \u2597\u2584\u2584 \u2584\u2596 \u2597\u2597\u2596 \u2597\u259f\u2584 \u2590\u259a \u258c\u2590 \u258c\u2590 \u2590 \u2590 \u2590\u2598\u259c \u2590\u2598\u2590 \u2590\u2590\u2590 \u2590\u2598\u2590 \u2590\u2598\u2590 \u2590 \u2590\u2590\u2596\u258c\u2590\u2584\u2584\u2596 \u2599\u259f \u2590 \u2590 \u2590 \u2590 \u2590 \u2590 \u2590\u2590\u2590 \u2590\u2580\u2580 \u2590 \u2590 \u2590 \u2580\u2598 \u2590 \u258c\u258c\u2590 \u2590 \u258c \u259d\u2584 \u2597\u259f\u2584 \u259d\u2599\u259c \u2590 \u2590 \u2590\u2590\u2590 \u259d\u2599\u259e \u2590 \u2590 \u259d\u2584 \u2590 \u2590\u258c\u2590 \u2596\u2590 \u259d\u2598 parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --sample_sheet See test_data/sample_sheet for example null --species Species to map: 'ce', 'cb' or 'ct' null --fq_prefix Path to fastq if not in sample_sheet /projects/b1059/data/{species}/WI/fastq/dna/ --kmers Whether to count kmers false --reference genome.fasta.gz to use in place of default defaults for c.e, c.b, and c.t --output Output folder name. alignment-{date} HELP: http://andersenlab.org/dry-guide/pipeline-alignment/ The logo above looks better in your terminal! Software requirements \u00b6 Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Currently only runs on Quest with conda environments installed at /projects/b1059/software/conda_envs/ Note mosdepth is used to calculate coverage. mosdepth is available on Linux machines, but not on Mac OSX. That is why the conda environment for the coverage process is specified as conda { System.properties['os.name'] != \"Mac OS X\" ? 'bioconda::mosdepth=0.2.6' : \"\" } . This snippet allows mosdepth to run off the executable present in the bin folder locally on Mac OSX, or use the conda-based installation when on Linux. Usage \u00b6 Testing on Quest \u00b6 This command uses a test dataset nextflow run main.nf --debug -profile quest Running on Quest \u00b6 You should run this in a screen session. Nnextflow run main.nf --sample_sheet <path_to_sample_sheet> --species c_elegans -profile quest -resume Parameters \u00b6 -profile \u00b6 There are three configuration profiles for this pipeline. local - Used for local development. quest - Used for running on Quest. gcp - For running on Google Cloud. --sample_sheet \u00b6 The sample sheet for alignment is the output from the trim-fq-nf pipeline. the sample sheet has the following columns: strain - the name of the strain. Multiple sequencing runs of the same strain are merged together. id - A unique ID for each sequencing run. This must be unique for every single pair of FASTQs. lb - A library ID. This should uniquely identify a DNA sequencing library. fq1 - The path to FASTQ1 fq2 - The path to FASTQ2 strain id lb fq1 fq2 seq_folder AB1 BGI1-RET2-AB1 RET2 BGI1-RET2-AB1-trim-1P.fq.gz BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set ECA243 BGI3-RET3b-ECA243 RET3b BGI3-RET3b-ECA243-trim-1P.fq.gz BGI3-RET3b-ECA243-trim-2P.fq.gz original_wi_set ECA718 ECA718_RET-S16_S26_L001 RET-S16 ECA718_RET-S16_S26_L001_1P.fq.gz ECA718_RET-S16_S26_L001_2P.fq.gz 20180306_Duke_NovaSeq_6000 Note Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . The library column is a useful tool for identifying errors by variant callers. For example, if the same library is sequenced twice, and a variant is only observed in one sequencing run then that variant may be excluded as a technical / PCR artifact depending on the variant caller being used. Important The alignment pipeline will merge multiple sequencing runs of the same strain into a single bam. However, summary output is provided at both the strain and id level. In this way, if there is a poor sequencing run it can be identified and removed from a collection of sequencing runs belonging to a strain. Note The sample sheet is a critical tool. It allows us to associated metadata with each sequencing run (e.g. isotype, reference strain, id, library). It also allows us to quickly verify that all results have been output. It is much easier than working with a list of files! --debug (optional) \u00b6 You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder) using your specified configuration profile (e.g. local / quest / gcp). For example: nextflow run main.nf -profile quest --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv --fq_prefix (optional) \u00b6 Within a sample sheet you may specify the locations of FASTQs using an absolute directory or a relative directory. If you want to use a relative directory, you should use the --fq_prefix to set the path that should be prefixed to each FASTQ. Note Previously, this option was --fqs_file_prefix --kmers (optional) \u00b6 default = false Toggles kmer-analysis --reference (optional) \u00b6 A fasta reference indexed with BWA. WS245 is packaged with the pipeline for convenience when testing or running locally. On Quest, the default references are here: c_elegans: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz c_briggsae: /projects/b1059/projects/Lewis/c_briggsae/variant_calling/alignment/reference/caenorhabditis_briggsae_QX1410.v0.9.scaffolds.fa.gz c_tropicalis: /projects/b1059/projects/Lewis/c_tropicalis/variant_calling/alignment/reference/caenorhabditis_tropicalis_NIC58.v1.scaffolds.fa.gz --output (optional) \u00b6 Default - alignment-YYYYMMDD A directory in which to output results. If you have set --debug true , the default output directory will be alignment-YYYYMMDD-debug . Output \u00b6 \u251c\u2500\u2500 _aggregate \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 multiqc \u2502 \u251c\u2500\u2500 id_data/ \u2502 \u2502 \u251c\u2500\u2500 ... (same as strain_data/) \u2502 \u251c\u2500\u2500 id_multiqc_report.html \u2502 \u251c\u2500\u2500 strain_data/ \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-dist-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-per-contig_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-plot-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_picard_deduplication_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Normalised_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools_alignment_plot_1.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc.log \u2502 \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u2502 \u251c\u2500\u2500 multiqc_general_stats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_picard_dups.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_qualimap_bamqc_genome_results.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_flagstat.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_sources.txt \u2502 \u2514\u2500\u2500 strain_multiqc_report.html \u251c\u2500\u2500 bam \u2502 \u251c\u2500\u2500 [strain].bam \u2502 \u2514\u2500\u2500 [strain].bam.bai \u251c\u2500\u2500 coverage \u2502 \u251c\u2500\u2500 id \u2502 \u2502 \u251c\u2500\u2500 [id].mosdepth.global.dist.txt \u2502 \u2502 \u251c\u2500\u2500 [id].mosdepth.summary.txt \u2502 \u2502 \u251c\u2500\u2500 [id].per-base.bed.gz \u2502 \u2502 \u2514\u2500\u2500 [id].per-base.bed.gz.csi \u2502 \u2514\u2500\u2500 strain \u2502 \u251c\u2500\u2500 [strain].mosdepth.global.dist.txt \u2502 \u251c\u2500\u2500 [strain].mosdepth.summary.txt \u2502 \u251c\u2500\u2500 [strain].per-base.bed.gz \u2502 \u2514\u2500\u2500 [strain].per-base.bed.gz.csi \u251c\u2500\u2500 software_versions.txt \u2514\u2500\u2500 summary.txt Most files should be obvious. A few are detailed below. software_versions.txt - Outputs the software versions used for every process (step) of the pipeline. summary.txt - Outputs a summary of the parameters used. sample_sheet.tsv - The sample sheet that was used to produce the alignment directory. strain_sheet.tsv - A summary of all strains and bams in the alignment directory. aggregate - Stores data that has been aggregated across all strains or sequencing IDs. coverage - Contains coverage data at the strain or id level, presented in a variety of ways. Data storage \u00b6 Cleanup \u00b6 Once the alignment-nf pipeline has completed successfully and you have removed low coverage strains (see pipeline overview ), all BAM files can be moved to /projects/b1059/data/{species}/WI/alignments/ prior to variant calling. Archive \u00b6 The following sections have been integrated into other code that no longer needs to be run manually, but I am keeping the documentation here in case we need to go back to it. It is important to always check that the sample sheet is generated appropriately. If there are errors in teh sample sheet, one can be constructed manually using the following code: construct_sample_sheet.sh \u00b6 The scripts/construct_sample_sheet.sh script generates the WI_sample_sheet.tsv file. Warning The WI_sample_sheet.tsv file should never be generated and/or edited by hand. It should only be generated using the scripts/construct_sample_sheet.tsv script. The construct_sample_sheet.sh script does a few things. (1) Parses FASTQ Filenames Unfortunately, no two sequencing centers are alike and they use different formats for naming sequencing files. For example: ECA768_RET-S11_S79_L001_2P.fq.gz [strain]_[lib_lib#]_[sample_#]_[lane]_[read].fq.gz XZ1734_S573_L007_2P.fq.gz [strain]_[sample_#]_[lane]_[read].fq.gz In some cases they even changed formats over time! The script parses the FASTQ filenames from different sequencing centers, extracting the strain name, and a unique ID. Note that the library and unique sequencing run ID ( id ) are named somewhat arbitrarily. The most imporant aspect of these columns is that any DNA library that has been sequenced multiple times possess the same library , and that every pair of FASTQs possess a unique sequencing ID. Consider the following (fake) example: strain isotype reference_strain id library AB1 AB1 TRUE BGI2-RET2-AB1 RET2 AB1 AB1 TRUE BGI2-RET3-AB1 RET3 AB4 CB4858 FALSE BGI1-RET2-AB4 RET2 AB4 CB4858 FALSE BGI2-RET2-AB4 RET2 AB1 was sequenced twice, however two different DNA libraries were produced for each sequencing run ( RET2 and RET3 ). AB4 was also sequenced twice, but both sequencing runs were of the same DNA library (called RET2 ). Note that the id column is always unique across all sequencing runs. If you look at the WI_sample_sheet.tsv in more detail you will observe that the id and library columns are not consistantly named. This is not ideal, but it works. The inconsistancy does not affect analysis, and exists because the filenames are not consistant, but unique library and sequencing run IDs must be derived from them. (2) Clean up strain names The second thing the construct_sample_sheet.sh script does is that it replaces shorthand strain names or innapropriately named strains with the 3-letter system. For example, N2Baer is renamed to ECA254 . (3) Integrate metadata The C. elegans WI Strain Info google spreadsheet is a master spreadlist containing every strain, reference_strain, and isotype for C. elegans wild isolates. The script downloads this dataset and uses it to integrate the isotype and reference strain into the sample sheet. Adding new sequencing datasets \u00b6 Sequencing data should be added to QUEST and processed through the trimming pipeline before being added to WI_sample_sheet.tsv . Before proceeding, be sure to read pipeline-trimming To add new sequencing datasets you will need to devise a strategy for extracting the strain name, a unique ID, and sequencing library from the FASTQ filenames. This may be the same as a past dataset, in which case you can append the sequencing run folder name to the list with that format. Alternatively, you may need to create a custom set of bash commands for generating the rows corresponding to each FASTQ pair. Here is an example from the construct_sample_sheet.sh script. #===================================# # BGI-20161012-ECA23 # #===================================# out=`mktemp` seq_folder=BGI-20161012-ECA23 >&2 echo ${seq_folder} prefix=${fastq_dir}/WI/dna/processed/$seq_folder for i in `ls -1 $prefix/*1P.fq.gz`; do bname=`basename ${i}`; barcode=`zcat ${i} | grep '@' | cut -f 10 -d ':' | sed 's/_//g' | head -n 100 | uniq -c | sort -k 1,1n | cut -c 9-100 | tail -n 1` echo -e \"${bname}\\t${i}\\t${barcode}\" >> ${out} done; cat ${out} |\\ awk -v prefix=${prefix} -v seq_folder=${seq_folder} '{ fq1 = $1; fq2 = $1; LB = $3; gsub(\"N\", \"\", LB); gsub(\"1P.fq.gz\", \"2P.fq.gz\", fq2); ID = $1; gsub(\"_1P.fq.gz\", \"\", ID); split(ID, a, \"[-_]\") SM=a[2]; print SM \"\\t\" ID \"\\t\" LB \"\\t\" prefix \"/\" fq1 \"\\t\" prefix \"/\" fq2 \"\\t\" seq_folder; }' >> ${fq_sheet} Notes on this snippet: SM = strain , LB = library , and ID = id in the final output file. The sequencing run is listed in the comment box at the top. Barcodes are extracted from each FASTQ in the first forloop. These are used to define the library . The id is defined using the basename of the file. A final column corresponding to the seq_folder is always added.","title":"alignment-nf"},{"location":"pipeline-alignment/#alignment-nf","text":"alignment-nf Pipeline overview Software requirements Usage Testing on Quest Running on Quest Parameters -profile --sample_sheet --debug (optional) --fq_prefix (optional) --kmers (optional) --reference (optional) --output (optional) Output Data storage Cleanup Archive construct_sample_sheet.sh Adding new sequencing datasets The alignment-nf pipeline performs alignment for wild isolate sequence data at the strain level , and outputs BAMs and related information. Those BAMs can be used for downstream analysis including variant calling, concordance analysis , wi-gatk-nf (variant calling) and other analyses. This page details how to run the pipeline and how to add new wild isolate sequencing data. Note Historically, sequence processing was performed at the isotype level. We are still interested in filtering strains used in analysis at the isotype level, but alignment and variant calling are now performed at the strain level rather than at the isotype level.","title":"alignment-nf"},{"location":"pipeline-alignment/#pipeline_overview","text":"\u2597\u2596 \u259d\u259c \u259d \u2597 \u2597\u2596 \u2596\u2597\u2584\u2584\u2596 \u2590\u258c \u2590 \u2597\u2584 \u2584\u2584 \u2597\u2597\u2596 \u2597\u2584\u2584 \u2584\u2596 \u2597\u2597\u2596 \u2597\u259f\u2584 \u2590\u259a \u258c\u2590 \u258c\u2590 \u2590 \u2590 \u2590\u2598\u259c \u2590\u2598\u2590 \u2590\u2590\u2590 \u2590\u2598\u2590 \u2590\u2598\u2590 \u2590 \u2590\u2590\u2596\u258c\u2590\u2584\u2584\u2596 \u2599\u259f \u2590 \u2590 \u2590 \u2590 \u2590 \u2590 \u2590\u2590\u2590 \u2590\u2580\u2580 \u2590 \u2590 \u2590 \u2580\u2598 \u2590 \u258c\u258c\u2590 \u2590 \u258c \u259d\u2584 \u2597\u259f\u2584 \u259d\u2599\u259c \u2590 \u2590 \u2590\u2590\u2590 \u259d\u2599\u259e \u2590 \u2590 \u259d\u2584 \u2590 \u2590\u258c\u2590 \u2596\u2590 \u259d\u2598 parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --sample_sheet See test_data/sample_sheet for example null --species Species to map: 'ce', 'cb' or 'ct' null --fq_prefix Path to fastq if not in sample_sheet /projects/b1059/data/{species}/WI/fastq/dna/ --kmers Whether to count kmers false --reference genome.fasta.gz to use in place of default defaults for c.e, c.b, and c.t --output Output folder name. alignment-{date} HELP: http://andersenlab.org/dry-guide/pipeline-alignment/ The logo above looks better in your terminal!","title":"Pipeline overview"},{"location":"pipeline-alignment/#software_requirements","text":"Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Currently only runs on Quest with conda environments installed at /projects/b1059/software/conda_envs/ Note mosdepth is used to calculate coverage. mosdepth is available on Linux machines, but not on Mac OSX. That is why the conda environment for the coverage process is specified as conda { System.properties['os.name'] != \"Mac OS X\" ? 'bioconda::mosdepth=0.2.6' : \"\" } . This snippet allows mosdepth to run off the executable present in the bin folder locally on Mac OSX, or use the conda-based installation when on Linux.","title":"Software requirements"},{"location":"pipeline-alignment/#usage","text":"","title":"Usage"},{"location":"pipeline-alignment/#testing_on_quest","text":"This command uses a test dataset nextflow run main.nf --debug -profile quest","title":"Testing on Quest"},{"location":"pipeline-alignment/#running_on_quest","text":"You should run this in a screen session. Nnextflow run main.nf --sample_sheet <path_to_sample_sheet> --species c_elegans -profile quest -resume","title":"Running on Quest"},{"location":"pipeline-alignment/#parameters","text":"","title":"Parameters"},{"location":"pipeline-alignment/#-profile","text":"There are three configuration profiles for this pipeline. local - Used for local development. quest - Used for running on Quest. gcp - For running on Google Cloud.","title":"-profile"},{"location":"pipeline-alignment/#--sample_sheet","text":"The sample sheet for alignment is the output from the trim-fq-nf pipeline. the sample sheet has the following columns: strain - the name of the strain. Multiple sequencing runs of the same strain are merged together. id - A unique ID for each sequencing run. This must be unique for every single pair of FASTQs. lb - A library ID. This should uniquely identify a DNA sequencing library. fq1 - The path to FASTQ1 fq2 - The path to FASTQ2 strain id lb fq1 fq2 seq_folder AB1 BGI1-RET2-AB1 RET2 BGI1-RET2-AB1-trim-1P.fq.gz BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set ECA243 BGI3-RET3b-ECA243 RET3b BGI3-RET3b-ECA243-trim-1P.fq.gz BGI3-RET3b-ECA243-trim-2P.fq.gz original_wi_set ECA718 ECA718_RET-S16_S26_L001 RET-S16 ECA718_RET-S16_S26_L001_1P.fq.gz ECA718_RET-S16_S26_L001_2P.fq.gz 20180306_Duke_NovaSeq_6000 Note Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . The library column is a useful tool for identifying errors by variant callers. For example, if the same library is sequenced twice, and a variant is only observed in one sequencing run then that variant may be excluded as a technical / PCR artifact depending on the variant caller being used. Important The alignment pipeline will merge multiple sequencing runs of the same strain into a single bam. However, summary output is provided at both the strain and id level. In this way, if there is a poor sequencing run it can be identified and removed from a collection of sequencing runs belonging to a strain. Note The sample sheet is a critical tool. It allows us to associated metadata with each sequencing run (e.g. isotype, reference strain, id, library). It also allows us to quickly verify that all results have been output. It is much easier than working with a list of files!","title":"--sample_sheet"},{"location":"pipeline-alignment/#--debug_optional","text":"You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder) using your specified configuration profile (e.g. local / quest / gcp). For example: nextflow run main.nf -profile quest --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv","title":"--debug (optional)"},{"location":"pipeline-alignment/#--fq_prefix_optional","text":"Within a sample sheet you may specify the locations of FASTQs using an absolute directory or a relative directory. If you want to use a relative directory, you should use the --fq_prefix to set the path that should be prefixed to each FASTQ. Note Previously, this option was --fqs_file_prefix","title":"--fq_prefix (optional)"},{"location":"pipeline-alignment/#--kmers_optional","text":"default = false Toggles kmer-analysis","title":"--kmers (optional)"},{"location":"pipeline-alignment/#--reference_optional","text":"A fasta reference indexed with BWA. WS245 is packaged with the pipeline for convenience when testing or running locally. On Quest, the default references are here: c_elegans: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz c_briggsae: /projects/b1059/projects/Lewis/c_briggsae/variant_calling/alignment/reference/caenorhabditis_briggsae_QX1410.v0.9.scaffolds.fa.gz c_tropicalis: /projects/b1059/projects/Lewis/c_tropicalis/variant_calling/alignment/reference/caenorhabditis_tropicalis_NIC58.v1.scaffolds.fa.gz","title":"--reference (optional)"},{"location":"pipeline-alignment/#--output_optional","text":"Default - alignment-YYYYMMDD A directory in which to output results. If you have set --debug true , the default output directory will be alignment-YYYYMMDD-debug .","title":"--output (optional)"},{"location":"pipeline-alignment/#output","text":"\u251c\u2500\u2500 _aggregate \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 multiqc \u2502 \u251c\u2500\u2500 id_data/ \u2502 \u2502 \u251c\u2500\u2500 ... (same as strain_data/) \u2502 \u251c\u2500\u2500 id_multiqc_report.html \u2502 \u251c\u2500\u2500 strain_data/ \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-dist-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-per-contig_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-plot-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_picard_deduplication_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Normalised_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools_alignment_plot_1.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc.log \u2502 \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u2502 \u251c\u2500\u2500 multiqc_general_stats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_picard_dups.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_qualimap_bamqc_genome_results.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_flagstat.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_sources.txt \u2502 \u2514\u2500\u2500 strain_multiqc_report.html \u251c\u2500\u2500 bam \u2502 \u251c\u2500\u2500 [strain].bam \u2502 \u2514\u2500\u2500 [strain].bam.bai \u251c\u2500\u2500 coverage \u2502 \u251c\u2500\u2500 id \u2502 \u2502 \u251c\u2500\u2500 [id].mosdepth.global.dist.txt \u2502 \u2502 \u251c\u2500\u2500 [id].mosdepth.summary.txt \u2502 \u2502 \u251c\u2500\u2500 [id].per-base.bed.gz \u2502 \u2502 \u2514\u2500\u2500 [id].per-base.bed.gz.csi \u2502 \u2514\u2500\u2500 strain \u2502 \u251c\u2500\u2500 [strain].mosdepth.global.dist.txt \u2502 \u251c\u2500\u2500 [strain].mosdepth.summary.txt \u2502 \u251c\u2500\u2500 [strain].per-base.bed.gz \u2502 \u2514\u2500\u2500 [strain].per-base.bed.gz.csi \u251c\u2500\u2500 software_versions.txt \u2514\u2500\u2500 summary.txt Most files should be obvious. A few are detailed below. software_versions.txt - Outputs the software versions used for every process (step) of the pipeline. summary.txt - Outputs a summary of the parameters used. sample_sheet.tsv - The sample sheet that was used to produce the alignment directory. strain_sheet.tsv - A summary of all strains and bams in the alignment directory. aggregate - Stores data that has been aggregated across all strains or sequencing IDs. coverage - Contains coverage data at the strain or id level, presented in a variety of ways.","title":"Output"},{"location":"pipeline-alignment/#data_storage","text":"","title":"Data storage"},{"location":"pipeline-alignment/#cleanup","text":"Once the alignment-nf pipeline has completed successfully and you have removed low coverage strains (see pipeline overview ), all BAM files can be moved to /projects/b1059/data/{species}/WI/alignments/ prior to variant calling.","title":"Cleanup"},{"location":"pipeline-alignment/#archive","text":"The following sections have been integrated into other code that no longer needs to be run manually, but I am keeping the documentation here in case we need to go back to it. It is important to always check that the sample sheet is generated appropriately. If there are errors in teh sample sheet, one can be constructed manually using the following code:","title":"Archive"},{"location":"pipeline-alignment/#construct_sample_sheetsh","text":"The scripts/construct_sample_sheet.sh script generates the WI_sample_sheet.tsv file. Warning The WI_sample_sheet.tsv file should never be generated and/or edited by hand. It should only be generated using the scripts/construct_sample_sheet.tsv script. The construct_sample_sheet.sh script does a few things. (1) Parses FASTQ Filenames Unfortunately, no two sequencing centers are alike and they use different formats for naming sequencing files. For example: ECA768_RET-S11_S79_L001_2P.fq.gz [strain]_[lib_lib#]_[sample_#]_[lane]_[read].fq.gz XZ1734_S573_L007_2P.fq.gz [strain]_[sample_#]_[lane]_[read].fq.gz In some cases they even changed formats over time! The script parses the FASTQ filenames from different sequencing centers, extracting the strain name, and a unique ID. Note that the library and unique sequencing run ID ( id ) are named somewhat arbitrarily. The most imporant aspect of these columns is that any DNA library that has been sequenced multiple times possess the same library , and that every pair of FASTQs possess a unique sequencing ID. Consider the following (fake) example: strain isotype reference_strain id library AB1 AB1 TRUE BGI2-RET2-AB1 RET2 AB1 AB1 TRUE BGI2-RET3-AB1 RET3 AB4 CB4858 FALSE BGI1-RET2-AB4 RET2 AB4 CB4858 FALSE BGI2-RET2-AB4 RET2 AB1 was sequenced twice, however two different DNA libraries were produced for each sequencing run ( RET2 and RET3 ). AB4 was also sequenced twice, but both sequencing runs were of the same DNA library (called RET2 ). Note that the id column is always unique across all sequencing runs. If you look at the WI_sample_sheet.tsv in more detail you will observe that the id and library columns are not consistantly named. This is not ideal, but it works. The inconsistancy does not affect analysis, and exists because the filenames are not consistant, but unique library and sequencing run IDs must be derived from them. (2) Clean up strain names The second thing the construct_sample_sheet.sh script does is that it replaces shorthand strain names or innapropriately named strains with the 3-letter system. For example, N2Baer is renamed to ECA254 . (3) Integrate metadata The C. elegans WI Strain Info google spreadsheet is a master spreadlist containing every strain, reference_strain, and isotype for C. elegans wild isolates. The script downloads this dataset and uses it to integrate the isotype and reference strain into the sample sheet.","title":"construct_sample_sheet.sh"},{"location":"pipeline-alignment/#adding_new_sequencing_datasets","text":"Sequencing data should be added to QUEST and processed through the trimming pipeline before being added to WI_sample_sheet.tsv . Before proceeding, be sure to read pipeline-trimming To add new sequencing datasets you will need to devise a strategy for extracting the strain name, a unique ID, and sequencing library from the FASTQ filenames. This may be the same as a past dataset, in which case you can append the sequencing run folder name to the list with that format. Alternatively, you may need to create a custom set of bash commands for generating the rows corresponding to each FASTQ pair. Here is an example from the construct_sample_sheet.sh script. #===================================# # BGI-20161012-ECA23 # #===================================# out=`mktemp` seq_folder=BGI-20161012-ECA23 >&2 echo ${seq_folder} prefix=${fastq_dir}/WI/dna/processed/$seq_folder for i in `ls -1 $prefix/*1P.fq.gz`; do bname=`basename ${i}`; barcode=`zcat ${i} | grep '@' | cut -f 10 -d ':' | sed 's/_//g' | head -n 100 | uniq -c | sort -k 1,1n | cut -c 9-100 | tail -n 1` echo -e \"${bname}\\t${i}\\t${barcode}\" >> ${out} done; cat ${out} |\\ awk -v prefix=${prefix} -v seq_folder=${seq_folder} '{ fq1 = $1; fq2 = $1; LB = $3; gsub(\"N\", \"\", LB); gsub(\"1P.fq.gz\", \"2P.fq.gz\", fq2); ID = $1; gsub(\"_1P.fq.gz\", \"\", ID); split(ID, a, \"[-_]\") SM=a[2]; print SM \"\\t\" ID \"\\t\" LB \"\\t\" prefix \"/\" fq1 \"\\t\" prefix \"/\" fq2 \"\\t\" seq_folder; }' >> ${fq_sheet} Notes on this snippet: SM = strain , LB = library , and ID = id in the final output file. The sequencing run is listed in the comment box at the top. Barcodes are extracted from each FASTQ in the first forloop. These are used to define the library . The id is defined using the basename of the file. A final column corresponding to the seq_folder is always added.","title":"Adding new sequencing datasets"},{"location":"pipeline-cegwas/","text":"cegwas2-nf \u00b6 cegwas2-nf Overview of the workflow Required software for running on QUEST Required software for running outside of QUEST Required data for running outside of QUEST Testing pipeline using Nextflow Execution of pipeline using Nextflow Profiles Parameters Optional parameters R scripts Output Folder Structure Phenotypes folder Genotype_Matrix folder Mappings folder Data Plots Fine_Mappings folder Data Plots BURDEN folder (Contains two subfolders VT/SKAT with the same structure) Data Plots GWA mapping with C. elegans Overview of the workflow \u00b6 Required software for running on QUEST \u00b6 nextflow-v20.0+ Users can either update Nextflow to the newest version to run OR load a conda environment for Nextflow v20 using the following commands: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Required software for running outside of QUEST \u00b6 These packages should be in the user's PATH R-v3.6.0 nextflow-v20.0+ BCFtools-v1.9 plink-v1.9 bedtools-2.29.2 R-cegwas2 R-tidyverse-v1.2.1 R-coop-0.6-2 R-rrBLUP-v4.6 R-plotly-4.9.2 R-DT-0.12 R-data.table-1.12.8 R-Rcpp-1.0.1 R-genetics-1.3.8.1.2 R-sommer-4.0.4 R-RSpectra-v0.13-1 pandoc=2.12 R-knitr-1.28 R-rmarkdown-2.1 R-cowplot-1.0.0 R-ggbeeswarm-v0.6 Required data for running outside of QUEST \u00b6 VCF(s) - A hard-filtered vcf containing phenotyped samples for mapping - A tabix-generated index hard-filtered vcf (.tbi) - An imputed vcf Testing pipeline using Nextflow \u00b6 Running debug mode is a good way to quickly test if your environment is set up correctly. Entire debug run should take 2-3 minutes. git clone https://github.com/AndersenLab/cegwas2-nf.git cd cegwas2-nf nextflow main.nf --debug Execution of pipeline using Nextflow \u00b6 git clone https://github.com/AndersenLab/cegwas2-nf.git cd cegwas2-nf nextflow main.nf --traitfile <path to traitfile> --annotation bcsq [optional parameters, see below] Profiles \u00b6 Users can select from a number of profiles that each run different processes for the analysis: -profile standard - This is the default profile if no profile is included. This profile runs the GWA mapping, fine mapping, burden mapping, and generates plots and dataframes in the output folder. -profile manplot_only - This profile only runs the GWA mapping and generates manhattan and phenotype-by-genotype plots for each trait. No fine mapping or burden mapping is performed. This profile is good for users with hundreds of traits. -profile reports - This profile runs all the same processes as the standard profile with an additional analysis for haplotypes and divergent regions in the QTL. In addition to the standard outputs, this profile also outputs an html markdown report for each trait with the markdown file that can be run independently on a local computer to reproduce the output files and figures. This profile works best with <30 traits. Parameters \u00b6 --traitfile - is a tab-delimited formatted (.tsv) file that contains trait information. Each phenotype file should be in the following format (replace trait_name with the phenotype of interest): strain trait_name_1 trait_name_2 JU258 32.73 19.34 ECA640 34.065378 12.32 ... ... ... ECA250 34.096 23.1 --annotation - Users can choose between \"snpeff\" annotation or \"bcsq\" annotation. Currently, annotation is only set up with the 20210121 release. nextflow main.nf --help - will display the help message Optional parameters \u00b6 --vcf - CeNDR release date for the VCF file with variant data. Default is \"20210121\". Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. --sthresh - This determines the signficance threshold required for performing post-mapping analysis of a QTL. BF corresponds to Bonferroni correction (DEFAULT), EIGEN corresponds to correcting for the number of independent markers in your data set, and user-specified corresponds to a user-defined threshold, where you replace user-specified with a number. For example --sthresh=4 will set the threshold to a -log10(p) value of 4. We recommend using the strict BF correction as a first pass to see what the resulting data looks like. If the pipeline stops at the summarize_maps process, no significant QTL were discovered with the input threshold. You might want to consider lowering the threshold if this occurs. --p3d - This determines what type of kinship correction to perform prior to mapping. TRUE corresponds to the EMMAx method and FALSE corresponds to the slower EMMA method. We recommend running with --p3d=TRUE to make sure all files of the required files are present and in the proper format, then run with --p3d=FALSE (DEFAULT) for a more exact mapping. --freqUpper - Upper bound for variant allele frequency for a variant to be considered for burden mapping. Default = 0.5 --minburden - The number of strains that must share a variant for that variant to be considered for burden mapping. Default = 2 --refflat - Genomic locations for genes used for burden mapping. A default generated from WS245 is provided in the repositories bin. --genes - Genomic locations for genes formatted for plotting purposes. A default generated from WS245 is provided in the repositories bin. R scripts \u00b6 Get_GenoMatrix_Eigen.R - Takes a genotype matrix and chromosome name as input and identifies the number significant eigenvalues. Fix_Isotype_names_bulk.R - Take sample names present in phenotype data and changes them to isotype names found on CeNDR . Run_Mappings.R - Performs GWA mapping using the rrBLUP R package and the EMMA or EMMAx algorithm for kinship correction. Generates manhattan plot and phenotype by genotype plot for peak positions. Summarize_Mappings.R - Generates plot of all QTL identified in nextflow pipeline. Finemap_QTL_Intervals.R - Run EMMA/EMMAx on QTL region of interest. Generates fine map plot, colored by LD with peak QTL SNV found from genome-wide scan plot_genes.R - Runs SnpEff and generates gene plot. makeped.R - Converts trait .tsv files to .ped format for burden mapping. rvtest - Executable to run burden mapping, can be found at the RVtests homepage plot_burden.R - Plots the results from burden mapping. Output Folder Structure \u00b6 Analysis_Results-{Date} | \u251c\u2500\u2500cegwas2_report_traitname_main.html \u251c\u2500\u2500cegwas2_report_traitname_main.Rmd | \u251c\u2500\u2500Phenotypes \u251c\u2500\u2500 strain_issues.txt \u251c\u2500\u2500 pr_traitname.tsv \u251c\u2500\u2500Genotype_Matrix \u251c\u2500\u2500 Genotype_Matrix.tsv \u251c\u2500\u2500 total_independent_tests.txt \u251c\u2500\u2500Mappings \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname_processed_mapping.tsv \u251c\u2500\u2500 QTL_peaks.tsv \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_manplot.pdf \u251c\u2500\u2500 traitname_pxgplot.pdf \u251c\u2500\u2500 Summarized_mappings.pdf \u251c\u2500\u2500Fine_Mappings \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname_snpeff_genes.tsv \u251c\u2500\u2500 traitname_qtlinterval_prLD_df.tsv \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_qtlinterval_finemap_plot.pdf \u251c\u2500\u2500 traitname_qtlinterval_gene_plot.pdf \u251c\u2500\u2500Divergent_and_haplotype \u251c\u2500\u2500all_QTL_bins.bed \u251c\u2500\u2500all_QTL_div.bed \u251c\u2500\u2500div_isotype_list.txt \u251c\u2500\u2500haplotype_in_QTL_region.txt \u251c\u2500\u2500BURDEN \u251c\u2500\u2500 VT \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname.VariableThresholdPrice.assoc \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_VTprice.pdf \u251c\u2500\u2500 SKAT \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname.Skat.assoc \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_SKAT.pdf Phenotypes folder \u00b6 strain_issues.txt - Output of any strain names that were changed to match vcf (i.e. isotypes that are not reference strains) pr_traitname.tsv - Processed phenotype file for each trait. This is the file that goes into the mapping Genotype_Matrix folder \u00b6 Genotype_Matrix.tsv - pruned LD-pruned genotype matrix used for GWAS and construction of kinship matrix total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix Mappings folder \u00b6 Data \u00b6 traitname_processed_mapping.tsv - Processed mapping data frame for each trait mapped QTL_peaks.tsv - List of signifcant QTL identified across all traits Plots \u00b6 traitname_manplot.pdf - Manhattan plot for each trait that was analyzed. Two significance threshold lines are present, one for the Bonferronit corrected threshold, and another for the spectral decomposition threshold. traitname_pxgplot.pdf - Phenotype by genotype split at peak QTL positions for every significant QTL identified Summarized_mappings.pdf - A summary plot of all QTL identified Fine_Mappings folder \u00b6 Data \u00b6 traitname_snpeff_genes.tsv - Fine-mapping data frame for all significant QTL Plots \u00b6 traitname_qtlinterval_finemap_plot.pdf - Fine map plot of QTL interval, colored by marker LD with the peak QTL identified from the genome-wide scan traitname_qtlinterval_gene_plot.pdf - variant annotation plot overlaid with gene CDS for QTL interval BURDEN folder (Contains two subfolders VT/SKAT with the same structure) \u00b6 Data \u00b6 traitname.VariableThresholdPrice.assoc - Genome-wide burden mapping result using VT price, see RVtests homepage traitname.Skat.assoc - Genome-wide burden mapping result using Skat, see RVtests homepage Plots \u00b6 traitname_VTprice.pdf - Genome-wide burden mapping manhattan plot for VTprice traitname_SKAT.pdf - Genome-wide burden mapping manhattan plot for Skat","title":"cegwas2-nf"},{"location":"pipeline-cegwas/#cegwas2-nf","text":"cegwas2-nf Overview of the workflow Required software for running on QUEST Required software for running outside of QUEST Required data for running outside of QUEST Testing pipeline using Nextflow Execution of pipeline using Nextflow Profiles Parameters Optional parameters R scripts Output Folder Structure Phenotypes folder Genotype_Matrix folder Mappings folder Data Plots Fine_Mappings folder Data Plots BURDEN folder (Contains two subfolders VT/SKAT with the same structure) Data Plots GWA mapping with C. elegans","title":"cegwas2-nf"},{"location":"pipeline-cegwas/#overview_of_the_workflow","text":"","title":"Overview of the workflow"},{"location":"pipeline-cegwas/#required_software_for_running_on_quest","text":"nextflow-v20.0+ Users can either update Nextflow to the newest version to run OR load a conda environment for Nextflow v20 using the following commands: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env","title":"Required software for running on QUEST"},{"location":"pipeline-cegwas/#required_software_for_running_outside_of_quest","text":"These packages should be in the user's PATH R-v3.6.0 nextflow-v20.0+ BCFtools-v1.9 plink-v1.9 bedtools-2.29.2 R-cegwas2 R-tidyverse-v1.2.1 R-coop-0.6-2 R-rrBLUP-v4.6 R-plotly-4.9.2 R-DT-0.12 R-data.table-1.12.8 R-Rcpp-1.0.1 R-genetics-1.3.8.1.2 R-sommer-4.0.4 R-RSpectra-v0.13-1 pandoc=2.12 R-knitr-1.28 R-rmarkdown-2.1 R-cowplot-1.0.0 R-ggbeeswarm-v0.6","title":"Required software for running outside of QUEST"},{"location":"pipeline-cegwas/#required_data_for_running_outside_of_quest","text":"VCF(s) - A hard-filtered vcf containing phenotyped samples for mapping - A tabix-generated index hard-filtered vcf (.tbi) - An imputed vcf","title":"Required data for running outside of QUEST"},{"location":"pipeline-cegwas/#testing_pipeline_using_nextflow","text":"Running debug mode is a good way to quickly test if your environment is set up correctly. Entire debug run should take 2-3 minutes. git clone https://github.com/AndersenLab/cegwas2-nf.git cd cegwas2-nf nextflow main.nf --debug","title":"Testing pipeline using Nextflow"},{"location":"pipeline-cegwas/#execution_of_pipeline_using_nextflow","text":"git clone https://github.com/AndersenLab/cegwas2-nf.git cd cegwas2-nf nextflow main.nf --traitfile <path to traitfile> --annotation bcsq [optional parameters, see below]","title":"Execution of pipeline using Nextflow"},{"location":"pipeline-cegwas/#profiles","text":"Users can select from a number of profiles that each run different processes for the analysis: -profile standard - This is the default profile if no profile is included. This profile runs the GWA mapping, fine mapping, burden mapping, and generates plots and dataframes in the output folder. -profile manplot_only - This profile only runs the GWA mapping and generates manhattan and phenotype-by-genotype plots for each trait. No fine mapping or burden mapping is performed. This profile is good for users with hundreds of traits. -profile reports - This profile runs all the same processes as the standard profile with an additional analysis for haplotypes and divergent regions in the QTL. In addition to the standard outputs, this profile also outputs an html markdown report for each trait with the markdown file that can be run independently on a local computer to reproduce the output files and figures. This profile works best with <30 traits.","title":"Profiles"},{"location":"pipeline-cegwas/#parameters","text":"--traitfile - is a tab-delimited formatted (.tsv) file that contains trait information. Each phenotype file should be in the following format (replace trait_name with the phenotype of interest): strain trait_name_1 trait_name_2 JU258 32.73 19.34 ECA640 34.065378 12.32 ... ... ... ECA250 34.096 23.1 --annotation - Users can choose between \"snpeff\" annotation or \"bcsq\" annotation. Currently, annotation is only set up with the 20210121 release. nextflow main.nf --help - will display the help message","title":"Parameters"},{"location":"pipeline-cegwas/#optional_parameters","text":"--vcf - CeNDR release date for the VCF file with variant data. Default is \"20210121\". Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. --sthresh - This determines the signficance threshold required for performing post-mapping analysis of a QTL. BF corresponds to Bonferroni correction (DEFAULT), EIGEN corresponds to correcting for the number of independent markers in your data set, and user-specified corresponds to a user-defined threshold, where you replace user-specified with a number. For example --sthresh=4 will set the threshold to a -log10(p) value of 4. We recommend using the strict BF correction as a first pass to see what the resulting data looks like. If the pipeline stops at the summarize_maps process, no significant QTL were discovered with the input threshold. You might want to consider lowering the threshold if this occurs. --p3d - This determines what type of kinship correction to perform prior to mapping. TRUE corresponds to the EMMAx method and FALSE corresponds to the slower EMMA method. We recommend running with --p3d=TRUE to make sure all files of the required files are present and in the proper format, then run with --p3d=FALSE (DEFAULT) for a more exact mapping. --freqUpper - Upper bound for variant allele frequency for a variant to be considered for burden mapping. Default = 0.5 --minburden - The number of strains that must share a variant for that variant to be considered for burden mapping. Default = 2 --refflat - Genomic locations for genes used for burden mapping. A default generated from WS245 is provided in the repositories bin. --genes - Genomic locations for genes formatted for plotting purposes. A default generated from WS245 is provided in the repositories bin.","title":"Optional parameters"},{"location":"pipeline-cegwas/#r_scripts","text":"Get_GenoMatrix_Eigen.R - Takes a genotype matrix and chromosome name as input and identifies the number significant eigenvalues. Fix_Isotype_names_bulk.R - Take sample names present in phenotype data and changes them to isotype names found on CeNDR . Run_Mappings.R - Performs GWA mapping using the rrBLUP R package and the EMMA or EMMAx algorithm for kinship correction. Generates manhattan plot and phenotype by genotype plot for peak positions. Summarize_Mappings.R - Generates plot of all QTL identified in nextflow pipeline. Finemap_QTL_Intervals.R - Run EMMA/EMMAx on QTL region of interest. Generates fine map plot, colored by LD with peak QTL SNV found from genome-wide scan plot_genes.R - Runs SnpEff and generates gene plot. makeped.R - Converts trait .tsv files to .ped format for burden mapping. rvtest - Executable to run burden mapping, can be found at the RVtests homepage plot_burden.R - Plots the results from burden mapping.","title":"R scripts"},{"location":"pipeline-cegwas/#output_folder_structure","text":"Analysis_Results-{Date} | \u251c\u2500\u2500cegwas2_report_traitname_main.html \u251c\u2500\u2500cegwas2_report_traitname_main.Rmd | \u251c\u2500\u2500Phenotypes \u251c\u2500\u2500 strain_issues.txt \u251c\u2500\u2500 pr_traitname.tsv \u251c\u2500\u2500Genotype_Matrix \u251c\u2500\u2500 Genotype_Matrix.tsv \u251c\u2500\u2500 total_independent_tests.txt \u251c\u2500\u2500Mappings \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname_processed_mapping.tsv \u251c\u2500\u2500 QTL_peaks.tsv \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_manplot.pdf \u251c\u2500\u2500 traitname_pxgplot.pdf \u251c\u2500\u2500 Summarized_mappings.pdf \u251c\u2500\u2500Fine_Mappings \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname_snpeff_genes.tsv \u251c\u2500\u2500 traitname_qtlinterval_prLD_df.tsv \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_qtlinterval_finemap_plot.pdf \u251c\u2500\u2500 traitname_qtlinterval_gene_plot.pdf \u251c\u2500\u2500Divergent_and_haplotype \u251c\u2500\u2500all_QTL_bins.bed \u251c\u2500\u2500all_QTL_div.bed \u251c\u2500\u2500div_isotype_list.txt \u251c\u2500\u2500haplotype_in_QTL_region.txt \u251c\u2500\u2500BURDEN \u251c\u2500\u2500 VT \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname.VariableThresholdPrice.assoc \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_VTprice.pdf \u251c\u2500\u2500 SKAT \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname.Skat.assoc \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_SKAT.pdf","title":"Output Folder Structure"},{"location":"pipeline-cegwas/#phenotypes_folder","text":"strain_issues.txt - Output of any strain names that were changed to match vcf (i.e. isotypes that are not reference strains) pr_traitname.tsv - Processed phenotype file for each trait. This is the file that goes into the mapping","title":"Phenotypes folder"},{"location":"pipeline-cegwas/#genotype_matrix_folder","text":"Genotype_Matrix.tsv - pruned LD-pruned genotype matrix used for GWAS and construction of kinship matrix total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix","title":"Genotype_Matrix folder"},{"location":"pipeline-cegwas/#mappings_folder","text":"","title":"Mappings folder"},{"location":"pipeline-cegwas/#data","text":"traitname_processed_mapping.tsv - Processed mapping data frame for each trait mapped QTL_peaks.tsv - List of signifcant QTL identified across all traits","title":"Data"},{"location":"pipeline-cegwas/#plots","text":"traitname_manplot.pdf - Manhattan plot for each trait that was analyzed. Two significance threshold lines are present, one for the Bonferronit corrected threshold, and another for the spectral decomposition threshold. traitname_pxgplot.pdf - Phenotype by genotype split at peak QTL positions for every significant QTL identified Summarized_mappings.pdf - A summary plot of all QTL identified","title":"Plots"},{"location":"pipeline-cegwas/#fine_mappings_folder","text":"","title":"Fine_Mappings folder"},{"location":"pipeline-cegwas/#data_1","text":"traitname_snpeff_genes.tsv - Fine-mapping data frame for all significant QTL","title":"Data"},{"location":"pipeline-cegwas/#plots_1","text":"traitname_qtlinterval_finemap_plot.pdf - Fine map plot of QTL interval, colored by marker LD with the peak QTL identified from the genome-wide scan traitname_qtlinterval_gene_plot.pdf - variant annotation plot overlaid with gene CDS for QTL interval","title":"Plots"},{"location":"pipeline-cegwas/#burden_folder_contains_two_subfolders_vtskat_with_the_same_structure","text":"","title":"BURDEN folder (Contains two subfolders VT/SKAT with the same structure)"},{"location":"pipeline-cegwas/#data_2","text":"traitname.VariableThresholdPrice.assoc - Genome-wide burden mapping result using VT price, see RVtests homepage traitname.Skat.assoc - Genome-wide burden mapping result using Skat, see RVtests homepage","title":"Data"},{"location":"pipeline-cegwas/#plots_2","text":"traitname_VTprice.pdf - Genome-wide burden mapping manhattan plot for VTprice traitname_SKAT.pdf - Genome-wide burden mapping manhattan plot for Skat","title":"Plots"},{"location":"pipeline-concordance/","text":"concordance-nf \u00b6 concordance-nf Pipeline overview Software Requirements Usage Profiles Debugging the pipeline on Quest Running the pipeline on Quest Parameters --bam_coverage --vcf --info_sheet --species --concordance_cutoff (optional) --cores (optional) --out (optional) Output The concordance-nf pipeline is used to detect sample swaps and determine which wild isolate strains should be grouped together as an isotype. To determine which strains belong to the same isotype we use two criteria. First we look at the strains that group together with a concordance threshold of 99.95%. Generally this will group most isotypes without issue. However, it is possible that you will run into cases where the grouping is not clean. For example, strain A groups with B, B groups with C, but C does not group with A. In these cases you must examine the data closely to identify why strains are incompletely grouping. Our second criteria we use to group isotypes may address these types of groupings. The second criteria that we use to group isotypes regards looking for regional differences among strains. If two strains are similar but possess a region of their genome (binned at 1 Mb) that differs by more than 2% then we will separate them out into their own isotypes. The process of grouping isotypes is very hand-on. This pipeline will help process the data but you must carefully review the output and investigate closely. Pipeline overview \u00b6 \u250c\u2500\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u2500\u2510\u250c\u252c\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510 \u250c\u2510\u250c\u250c\u2500\u2510 \u2502 \u2502 \u2502\u2502\u2502\u2502\u2502 \u2502 \u2502\u251c\u252c\u2518 \u2502\u2502\u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u251c\u2524\u2500\u2500\u2500\u2502\u2502\u2502\u251c\u2524 \u2514\u2500\u2518\u2514\u2500\u2518\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518\u2534\u2514\u2500\u2500\u2534\u2518\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518 \u2518\u2514\u2518\u2514 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Regular job cores 4 --out Directory to output results concordance-{date} --vcf Hard filtered vcf null --bam_coverage Table with \"strain\" and \"coverage\" as header null --info_sheet Strain sheet containing exisiting isotype assignment null --species 'c_elegans' will check for npr1. All other values will skip this null --concordance_cutoff Cutoff of concordance value to count two strains as same isotype 0.9995 Software Requirements \u00b6 The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update Usage \u00b6 Profiles \u00b6 The nextflow.config file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. Note If you forget to add a -profile , the quest profile will be chosen as default Debugging the pipeline on Quest \u00b6 When running on Quest, you should first run the quest debug profile. The Quest debug profile will use the test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume Running the pipeline on Quest \u00b6 The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest --bam_coverage <path_to_file> --vcf <path_to_file> --species c_elegans --info_sheet <path_to_file> -resume Parameters \u00b6 The nextflow profiles configured in nextflow.config are designed to make it so that you don't need to change the parameters. However, the pipeline offers this flexibility if it is ever called for. --bam_coverage \u00b6 The sample sheet to use. This is generally the same sample sheet used for wi-gatk . The sample sheet should look like this: strain bam bai coverage percent_mapped AB1 AB1.bam AB1.bam.bai 64 99.4 AB4 AB4.bam AB4.bam.bai 52 99.2 BRC20067 BRC20067.bam BRC20067.bam.bai 30 92.5 Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced. --vcf \u00b6 The hard-filtered VCF output from wi-gatk . --info_sheet \u00b6 Strain sheet containing existing isotype assignment. You can download the current wild isolate species master sheet for this input. --species \u00b6 Common options include 'c_elegans', 'c_briggsae', and 'c_tropicalis'. If species == c_elegans, pipeline will check for npr-1 variant, otherwise this step will be skipped. --concordance_cutoff (optional) \u00b6 Cutoff to use to determine isotype groups. Default is 0.9995. --cores (optional) \u00b6 The number of cores to use during alignments and variant calling. --out (optional) \u00b6 A directory in which to output results. By default it will be concordance-YYYYMMDD where YYYYMMDD is todays date. Output \u00b6 \u251c\u2500\u2500 concordance \u251c\u2500\u2500 gtcheck.tsv \u251c\u2500\u2500 isotype_count.txt \u251c\u2500\u2500 isotype_groups.tsv \u251c\u2500\u2500 npr1_allele_strain.tsv \u251c\u2500\u2500 problem_strains.tsv \u251c\u2500\u2500 WI_metadata.tsv \u251c\u2500\u2500 concordance.pdf/png \u251c\u2500\u2500 xconcordance.pdf/png \u2514\u2500\u2500 pairwise \u2514\u2500\u2500 within_group \u2514\u2500\u2500 {isotype_group}.{isotype}.{strain1}_{strain2}.png concordance.png/pdf - An image showing the distribution of pairwise concordances across all strains. The cutoff is at 99.9% above which pairs are considered to be in the same isotype unless issues arise. xconcordance.png/pdf - A close up view of the concordances showing more detail. isotype_groups.tsv - This is the one of the most important output files . It illustrates the isotypes identified for each strain and identifies potential issues. A file with the following structure: group strain isotype latitude longitude coverage unique_isotypes_per_group unique_groups_per_isotype strain_in_multiple_isotypes location_issue strain_conflict 1 AB1 AB1 -34.93 138.59 69.4687 1 1 FALSE FALSE FALSE 112 AB4 CB4858 -34.93 138.59 158.358 1 1 FALSE TRUE TRUE 112 ECA251 CB4858 34.1 -118.1 73.5843 1 1 FALSE TRUE TRUE 112 JU1960 CB4858 34.1897 -118.131 55.0373 1 1 FALSE TRUE TRUE 175 BRC20067 BRC20067 24.073 121.17 33.5934 1 1 FALSE FALSE FALSE 175 BRC20113 BRC20067 24.1242 121.283 38.9916 1 1 FALSE FALSE FALSE 186 BRC20231 MY23 23.5415 120.908 44.1452 1 1 FALSE TRUE TRUE 186 MY23 MY23 51.96 7.53 132.185 1 1 FALSE TRUE TRUE group - A number used to group strains (in each row) into an isotype automatically. This number should be unique with the isotype column (e.g. 1--> AB1, 112 --> CB4858, BRC20067 --> 175). The number can change between analyses. strain - the strain isotype - the currently assigned isotype for a strain taken from the WI Strain Info spreadsheet. When new strains are added this is blank. latitude longitude coverage - Depth of coverage for strain. unique_isotypes_per_group - Number of unique isotypes when grouping by the group column. This should be 1. If it is more than 1, it indicates that multiple isotypes were assigned to a grouping and that a previously assigned isotype is now being called into question. unique_groups_per_isotype Number of unique groups assigned to an isotype. This should be 1. If it is higher than 1, it indicates that a strain is concordant with strains in two different isotypes (including blanks). If it is equal to 2 and contains blanks in the isotype column it likely means that an isotype should be assigned to that strain. strain_in_multiple_isotypes - Indicates that a strain is falling into multiple isotypes (a problem!). location_issue - Indicates a location issue. This occurs when strains fall into an isotype but are located far away from one another. Some are known issues and can be ignored. strain_conflict - TRUE if any issue is present that should be investigated. Note This file might change as you manually adjust the concordance cutoff for each run gtcheck.tsv - the other most important file . File produced using bcftools gtcheck ; Raw genotype differences between strains. This file is used in manual inspection of the isotype groups isotype_count.txt - Gives a count of the number of isotypes identified. concordance/pairwise/ (directory) Contains images showing locations where regional discordance occurs among strains classified as being the isotype. You must look through all these images to ensure there are no strains being grouped that have regions with significant differences (> 2%). The image below illustrates an example of this. ED3049 and ED3046 are highly similar (> 99.9%). However, they differ in a region on the right arm of chromosome II. We believe this was enough reason to consider them separate isotypes. npr1_allele_strain.tsv - if species == c_elegans, this file will be output to show problematic strains that contain the N2 npr-1 allele and should be manually checked. Important If a new strain is flagged in this file, tell Erik, Robyn, and the wild isolate team ASAP so they can address the issue. This strain will likely be removed from further analysis.","title":"concordance-nf"},{"location":"pipeline-concordance/#concordance-nf","text":"concordance-nf Pipeline overview Software Requirements Usage Profiles Debugging the pipeline on Quest Running the pipeline on Quest Parameters --bam_coverage --vcf --info_sheet --species --concordance_cutoff (optional) --cores (optional) --out (optional) Output The concordance-nf pipeline is used to detect sample swaps and determine which wild isolate strains should be grouped together as an isotype. To determine which strains belong to the same isotype we use two criteria. First we look at the strains that group together with a concordance threshold of 99.95%. Generally this will group most isotypes without issue. However, it is possible that you will run into cases where the grouping is not clean. For example, strain A groups with B, B groups with C, but C does not group with A. In these cases you must examine the data closely to identify why strains are incompletely grouping. Our second criteria we use to group isotypes may address these types of groupings. The second criteria that we use to group isotypes regards looking for regional differences among strains. If two strains are similar but possess a region of their genome (binned at 1 Mb) that differs by more than 2% then we will separate them out into their own isotypes. The process of grouping isotypes is very hand-on. This pipeline will help process the data but you must carefully review the output and investigate closely.","title":"concordance-nf"},{"location":"pipeline-concordance/#pipeline_overview","text":"\u250c\u2500\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u2500\u2510\u250c\u252c\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510 \u250c\u2510\u250c\u250c\u2500\u2510 \u2502 \u2502 \u2502\u2502\u2502\u2502\u2502 \u2502 \u2502\u251c\u252c\u2518 \u2502\u2502\u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u251c\u2524\u2500\u2500\u2500\u2502\u2502\u2502\u251c\u2524 \u2514\u2500\u2518\u2514\u2500\u2518\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518\u2534\u2514\u2500\u2500\u2534\u2518\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518 \u2518\u2514\u2518\u2514 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Regular job cores 4 --out Directory to output results concordance-{date} --vcf Hard filtered vcf null --bam_coverage Table with \"strain\" and \"coverage\" as header null --info_sheet Strain sheet containing exisiting isotype assignment null --species 'c_elegans' will check for npr1. All other values will skip this null --concordance_cutoff Cutoff of concordance value to count two strains as same isotype 0.9995","title":"Pipeline overview"},{"location":"pipeline-concordance/#software_requirements","text":"The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update","title":"Software Requirements"},{"location":"pipeline-concordance/#usage","text":"","title":"Usage"},{"location":"pipeline-concordance/#profiles","text":"The nextflow.config file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. Note If you forget to add a -profile , the quest profile will be chosen as default","title":"Profiles"},{"location":"pipeline-concordance/#debugging_the_pipeline_on_quest","text":"When running on Quest, you should first run the quest debug profile. The Quest debug profile will use the test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume","title":"Debugging the pipeline on Quest"},{"location":"pipeline-concordance/#running_the_pipeline_on_quest","text":"The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest --bam_coverage <path_to_file> --vcf <path_to_file> --species c_elegans --info_sheet <path_to_file> -resume","title":"Running the pipeline on Quest"},{"location":"pipeline-concordance/#parameters","text":"The nextflow profiles configured in nextflow.config are designed to make it so that you don't need to change the parameters. However, the pipeline offers this flexibility if it is ever called for.","title":"Parameters"},{"location":"pipeline-concordance/#--bam_coverage","text":"The sample sheet to use. This is generally the same sample sheet used for wi-gatk . The sample sheet should look like this: strain bam bai coverage percent_mapped AB1 AB1.bam AB1.bam.bai 64 99.4 AB4 AB4.bam AB4.bam.bai 52 99.2 BRC20067 BRC20067.bam BRC20067.bam.bai 30 92.5 Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced.","title":"--bam_coverage"},{"location":"pipeline-concordance/#--vcf","text":"The hard-filtered VCF output from wi-gatk .","title":"--vcf"},{"location":"pipeline-concordance/#--info_sheet","text":"Strain sheet containing existing isotype assignment. You can download the current wild isolate species master sheet for this input.","title":"--info_sheet"},{"location":"pipeline-concordance/#--species","text":"Common options include 'c_elegans', 'c_briggsae', and 'c_tropicalis'. If species == c_elegans, pipeline will check for npr-1 variant, otherwise this step will be skipped.","title":"--species"},{"location":"pipeline-concordance/#--concordance_cutoff_optional","text":"Cutoff to use to determine isotype groups. Default is 0.9995.","title":"--concordance_cutoff (optional)"},{"location":"pipeline-concordance/#--cores_optional","text":"The number of cores to use during alignments and variant calling.","title":"--cores (optional)"},{"location":"pipeline-concordance/#--out_optional","text":"A directory in which to output results. By default it will be concordance-YYYYMMDD where YYYYMMDD is todays date.","title":"--out (optional)"},{"location":"pipeline-concordance/#output","text":"\u251c\u2500\u2500 concordance \u251c\u2500\u2500 gtcheck.tsv \u251c\u2500\u2500 isotype_count.txt \u251c\u2500\u2500 isotype_groups.tsv \u251c\u2500\u2500 npr1_allele_strain.tsv \u251c\u2500\u2500 problem_strains.tsv \u251c\u2500\u2500 WI_metadata.tsv \u251c\u2500\u2500 concordance.pdf/png \u251c\u2500\u2500 xconcordance.pdf/png \u2514\u2500\u2500 pairwise \u2514\u2500\u2500 within_group \u2514\u2500\u2500 {isotype_group}.{isotype}.{strain1}_{strain2}.png concordance.png/pdf - An image showing the distribution of pairwise concordances across all strains. The cutoff is at 99.9% above which pairs are considered to be in the same isotype unless issues arise. xconcordance.png/pdf - A close up view of the concordances showing more detail. isotype_groups.tsv - This is the one of the most important output files . It illustrates the isotypes identified for each strain and identifies potential issues. A file with the following structure: group strain isotype latitude longitude coverage unique_isotypes_per_group unique_groups_per_isotype strain_in_multiple_isotypes location_issue strain_conflict 1 AB1 AB1 -34.93 138.59 69.4687 1 1 FALSE FALSE FALSE 112 AB4 CB4858 -34.93 138.59 158.358 1 1 FALSE TRUE TRUE 112 ECA251 CB4858 34.1 -118.1 73.5843 1 1 FALSE TRUE TRUE 112 JU1960 CB4858 34.1897 -118.131 55.0373 1 1 FALSE TRUE TRUE 175 BRC20067 BRC20067 24.073 121.17 33.5934 1 1 FALSE FALSE FALSE 175 BRC20113 BRC20067 24.1242 121.283 38.9916 1 1 FALSE FALSE FALSE 186 BRC20231 MY23 23.5415 120.908 44.1452 1 1 FALSE TRUE TRUE 186 MY23 MY23 51.96 7.53 132.185 1 1 FALSE TRUE TRUE group - A number used to group strains (in each row) into an isotype automatically. This number should be unique with the isotype column (e.g. 1--> AB1, 112 --> CB4858, BRC20067 --> 175). The number can change between analyses. strain - the strain isotype - the currently assigned isotype for a strain taken from the WI Strain Info spreadsheet. When new strains are added this is blank. latitude longitude coverage - Depth of coverage for strain. unique_isotypes_per_group - Number of unique isotypes when grouping by the group column. This should be 1. If it is more than 1, it indicates that multiple isotypes were assigned to a grouping and that a previously assigned isotype is now being called into question. unique_groups_per_isotype Number of unique groups assigned to an isotype. This should be 1. If it is higher than 1, it indicates that a strain is concordant with strains in two different isotypes (including blanks). If it is equal to 2 and contains blanks in the isotype column it likely means that an isotype should be assigned to that strain. strain_in_multiple_isotypes - Indicates that a strain is falling into multiple isotypes (a problem!). location_issue - Indicates a location issue. This occurs when strains fall into an isotype but are located far away from one another. Some are known issues and can be ignored. strain_conflict - TRUE if any issue is present that should be investigated. Note This file might change as you manually adjust the concordance cutoff for each run gtcheck.tsv - the other most important file . File produced using bcftools gtcheck ; Raw genotype differences between strains. This file is used in manual inspection of the isotype groups isotype_count.txt - Gives a count of the number of isotypes identified. concordance/pairwise/ (directory) Contains images showing locations where regional discordance occurs among strains classified as being the isotype. You must look through all these images to ensure there are no strains being grouped that have regions with significant differences (> 2%). The image below illustrates an example of this. ED3049 and ED3046 are highly similar (> 99.9%). However, they differ in a region on the right arm of chromosome II. We believe this was enough reason to consider them separate isotypes. npr1_allele_strain.tsv - if species == c_elegans, this file will be output to show problematic strains that contain the N2 npr-1 allele and should be manually checked. Important If a new strain is flagged in this file, tell Erik, Robyn, and the wild isolate team ASAP so they can address the issue. This strain will likely be removed from further analysis.","title":"Output"},{"location":"pipeline-docker/","text":"Create docker image \u00b6 Create docker image Dockerfile Build docker image Tag image with a version Push docker image to dockerhub Running Nextflow with docker Docker can help us to maintain our computational environments. Each of our Nextflow pipeline has a dedicated docker image in our lab. And all the docker files should be avalible at dockerfile . Dockerfile \u00b6 To simplify the image building, we can use conda to install most of the tools. We can collect the tools avalible on conda cloud into a conda.yml file, which might looks like this. name: concordance-nf channels: - defaults - bioconda - r - biobuilds - conda-forge dependencies: - bwa=0.7.17 - sambamba=0.7.0 - samtools=1.9 - picard=2.20.6 - bcftools=1.9 # - vcfkit=0.1.6 - csvtk=0.18.2 - r=3.6.0 - r-ggplot2=3.1.1 - r-readr=1.3.1 - r-tidyverse=1.2.1 Then, build the Dockerfile as below. FROM continuumio/miniconda MAINTAINER XXX <email> COPY conda.yml . RUN \\ conda env update -n root -f conda.yml \\ && conda clean -a # install other tools not avalible on conda cloud -- tidyverse sometimes need to be installed here separately... RUN apt-get update && apt-get install -y procps RUN R -e \"install.packages('roperators',dependencies=TRUE, repos='http://cran.us.r-project.org')\" Note Put the conda.ymal and Dockerfile under the same folder. Build docker image \u00b6 To build the docker image, you need docker desktop installed on your local machine. Also you should sign up the dockerhub to enable pushing docker image to cloud. Go to the folder which have conda.ymal and Dockerfile , run docker build -t <dockerhub account>/<name of the image> . # don't ingore the dot here You can use docker image ls to check the image list you have in your local machine. Importantly, you have to check if the tools were installed sucessfully in your docker image. To test the docker image, run docker run -ti <dockerhub account>/<name of the image> sh The above command will let you into the docker image, where you can check the tools by their normal commands. Make sure all the tools you need have been installed appropriately. Tag image with a version \u00b6 There are sometimes issues with Nextflow thinking it has the latest docker image when it really doesn't. To avoid this issue, it is useful to tag each updated docker image with a version tag. Remember to update the docker call in nextflow to use the new version! docker image tag <dockerhub account>/<name of the image>:latest <dockerhub account>/<name of the image>:<version tag> Push docker image to dockerhub \u00b6 After the image check, you are ready to push the image to dockerhub which allows you to download the image when ever you need to use. docker push <dockerhub account>/<name of the image>:<version tag> Running Nextflow with docker \u00b6 If running Nextflow locally, a docker container can be used with the following line (check out the documentation ): nextflow run <your script> -with-docker [docker image] Alternatively, a docker container can be specified within the nextflow.config script to avoid adding an extra parameter: process.container = 'nextflow/examples:latest' docker.enabled = true // if on quest: // singularity.enabled = true Important When running Nextflow with a docker container on QUEST, it is necessary to replace the docker command with singularity (although you still must build a docker container). You must also load singularity using module load singularity before starting a run.","title":"Docker"},{"location":"pipeline-docker/#create_docker_image","text":"Create docker image Dockerfile Build docker image Tag image with a version Push docker image to dockerhub Running Nextflow with docker Docker can help us to maintain our computational environments. Each of our Nextflow pipeline has a dedicated docker image in our lab. And all the docker files should be avalible at dockerfile .","title":"Create docker image"},{"location":"pipeline-docker/#dockerfile","text":"To simplify the image building, we can use conda to install most of the tools. We can collect the tools avalible on conda cloud into a conda.yml file, which might looks like this. name: concordance-nf channels: - defaults - bioconda - r - biobuilds - conda-forge dependencies: - bwa=0.7.17 - sambamba=0.7.0 - samtools=1.9 - picard=2.20.6 - bcftools=1.9 # - vcfkit=0.1.6 - csvtk=0.18.2 - r=3.6.0 - r-ggplot2=3.1.1 - r-readr=1.3.1 - r-tidyverse=1.2.1 Then, build the Dockerfile as below. FROM continuumio/miniconda MAINTAINER XXX <email> COPY conda.yml . RUN \\ conda env update -n root -f conda.yml \\ && conda clean -a # install other tools not avalible on conda cloud -- tidyverse sometimes need to be installed here separately... RUN apt-get update && apt-get install -y procps RUN R -e \"install.packages('roperators',dependencies=TRUE, repos='http://cran.us.r-project.org')\" Note Put the conda.ymal and Dockerfile under the same folder.","title":"Dockerfile"},{"location":"pipeline-docker/#build_docker_image","text":"To build the docker image, you need docker desktop installed on your local machine. Also you should sign up the dockerhub to enable pushing docker image to cloud. Go to the folder which have conda.ymal and Dockerfile , run docker build -t <dockerhub account>/<name of the image> . # don't ingore the dot here You can use docker image ls to check the image list you have in your local machine. Importantly, you have to check if the tools were installed sucessfully in your docker image. To test the docker image, run docker run -ti <dockerhub account>/<name of the image> sh The above command will let you into the docker image, where you can check the tools by their normal commands. Make sure all the tools you need have been installed appropriately.","title":"Build docker image"},{"location":"pipeline-docker/#tag_image_with_a_version","text":"There are sometimes issues with Nextflow thinking it has the latest docker image when it really doesn't. To avoid this issue, it is useful to tag each updated docker image with a version tag. Remember to update the docker call in nextflow to use the new version! docker image tag <dockerhub account>/<name of the image>:latest <dockerhub account>/<name of the image>:<version tag>","title":"Tag image with a version"},{"location":"pipeline-docker/#push_docker_image_to_dockerhub","text":"After the image check, you are ready to push the image to dockerhub which allows you to download the image when ever you need to use. docker push <dockerhub account>/<name of the image>:<version tag>","title":"Push docker image to dockerhub"},{"location":"pipeline-docker/#running_nextflow_with_docker","text":"If running Nextflow locally, a docker container can be used with the following line (check out the documentation ): nextflow run <your script> -with-docker [docker image] Alternatively, a docker container can be specified within the nextflow.config script to avoid adding an extra parameter: process.container = 'nextflow/examples:latest' docker.enabled = true // if on quest: // singularity.enabled = true Important When running Nextflow with a docker container on QUEST, it is necessary to replace the docker command with singularity (although you still must build a docker container). You must also load singularity using module load singularity before starting a run.","title":"Running Nextflow with docker"},{"location":"pipeline-genomes-nf/","text":"genomes-nf \u00b6 genomes-nf Pipeline overview Software requirements Usage Parameters -profile (optional) --wb_version (optional) --projects (optional) --output (optional) Output Notes genomes-nf is a Nextflow pipeline for managing reference genomes and annotation files. Important When adding a new WormBase version reference genome, especially for c_elegans it is essential that you use this pipeline instead of downloading and adding the files to QUEST manually. These files and this file structure are essential to many other pipelines in the lab. Pipeline overview \u00b6 >AAGACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAA AGACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC GAAATA CGCG GA ATATA AAAA TCG TCGAT GC GGGC CGATCGA TAGAT GA TATATCGC TTAAC ACTAGAGGGG CTATCGAC CGAA CT GACTA CT GCG AT AGCATCACG TGGGGGCCTATC CGAC AA TCAGCTACGAAAT AGCCC TCTATCATAA TATAT T TCT TC AGCGA GA A A T TC ATAAAT TCGGGATCTAGC A CGC AT ATATATATGC GCGAT TCTAC AG GCGGGGGA AT TA AA AAGAC CG TC AT GC AGCTGGGGGC ACG GA TA AT GA CTATATATATCGC AATGC ACTAGAG GG CTATCGAC ACG A CT GACTA CT AGCGG AT AGCATCACGATGGG GCCTATC ACG C AA TCAGCTACGAAAT ACTCC TCTATCA AA AAATATAT TCTC TC AGCGA GA AAACT TC TTCATAAATCTCGG ATCTAGC ATCG AT TATATATATATGC TTAATA FCG GA ATATA AAA TCG TCGAT GC GG ACGATCGA TAGAT GA CTATATATATCGC AACACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAAT CTACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC parameters description Set/Default ========== =========== ======================== --wb_version wormbase version to build WS276 --projects comma-delimited list of `species/project_id` c_elegans/PRJNA13758,c_briggsae/PRJNA10731,c_tropicalis/PRJNA53597 --output Path of output folder /projects/b1059/data/ This repo contains a nextflow pipeline that downloads, indexes, and builds annotation databases for reference genomes from wormbase. The following outputs are created: A BWA Index SNPeff annotation database CSQ annotation database Samtools faidx index A GATK Sequence dictionary file Software requirements \u00b6 Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env If running pipeline on Quest, you must first load singularity to access the docker container: module load singularity If running locally, Docker must be installed. For further instructions, check out our docker guide Usage \u00b6 The pipeline can be run locally or on Quest. For example: nextflow run main.nf -resume -profile local --wb_version=WS276 --projects=c_elegans/PRJNA13758 Parameters \u00b6 -profile (optional) \u00b6 Can be set to local or quest . The pipeline uses the andersenlab/genomes docker image built from env/genome.Dockerfile . The image is automatically built using github actions. See .github/workflows/build.yml for details. Note The default profile is set to -profile=quest --wb_version (optional) \u00b6 The wormbase version to build. For example, WS279 . Default is WS276 . --projects (optional) \u00b6 A comma-delimited list of species/project_id identifiers. A table below lists the current projects that can be downloaded. This table is regenerated as the first step of the pipeline, and stored as a file called project_species.tsv in the params.output folder ( ./genomes if working locally). By default, the pipeline will generate reference genome indices and annotations for: c_elegans/PRJNA13758 - N2 based reference genome c_briggsae/PRJNA10731 c_tropicalis/PRJNA53597 The current set of available species/projects that can be built are: species project b_xylophilus PRJEA64437 c_briggsae PRJNA10731 c_angaria PRJNA51225 a_ceylanicum PRJNA231479 a_suum PRJNA62057 a_suum PRJNA80881 b_malayi PRJNA10729 c_brenneri PRJNA20035 c_elegans PRJEB28388 c_elegans PRJNA13758 c_elegans PRJNA275000 c_latens PRJNA248912 c_remanei PRJNA248909 c_remanei PRJNA248911 c_remanei PRJNA53967 c_inopinata PRJDB5687 c_japonica PRJNA12591 c_sp11 PRJNA53597 c_sp5 PRJNA194557 c_nigoni PRJNA384657 c_sinica PRJNA194557 c_tropicalis PRJNA53597 d_immitis PRJEB1797 h_bacteriophora PRJNA13977 l_loa PRJNA60051 m_hapla PRJNA29083 m_incognita PRJEA28837 h_contortus PRJEB506 h_contortus PRJNA205202 n_americanus PRJNA72135 p_exspectatus PRJEB6009 o_tipulae PRJEB15512 p_redivivus PRJNA186477 s_ratti PRJEA62033 s_ratti PRJEB125 o_volvulus PRJEB513 p_pacificus PRJNA12644 t_muris PRJEB126 t_spiralis PRJNA12603 t_suis PRJNA208415 t_suis PRJNA208416 --output (optional) \u00b6 Path of output folder with results. Default is /projects/b1059/data/{species}/genomes/{projectID}/{WSbuild}/ Output \u00b6 Outputs are nested under params.output with the following structure: c_elegans (species) \u2514\u2500\u2500 genomes \u2514\u2500\u2500 PRJNA13758 (project) \u2514\u2500\u2500 WS276 (build) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.dict (dict file) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz (fasta) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.amb (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.ann (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.bwt (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.fai (samtools faidx index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.gzi (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.pac (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.sa (bwa index) \u251c\u2500\u2500 csq \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz (CSQ annotation GFF3) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Length.tsv (protein lengths) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Scores.tsv (blosum and grantham scores) \u251c\u2500\u2500 lcr \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz (low complexity regions) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz (low complexity regions) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz.tbi (tabix index) \u2514\u2500\u2500 snpeff \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276 (tabix index) \u2502 \u251c\u2500\u2500 genes.gtf.gz (Reference GTF) \u2502 \u251c\u2500\u2500 sequences.fa (fasta genome (unzipped)) \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin (snpEff annotation db) \u2514\u2500\u2500 snpEff.config (snpEff configuration file) Notes \u00b6 The SNPeff databases are not collected together in one location as is often the case. Instead, they are stored individually with their own configuration files. The GFF3 files for some species are not as developed as C. elegans . As a consequence, the biotype is inferred from the Attributes column of the GFF. See bin/format_csq.R for more details. Warning The updated csq-formated gff script needs to be updated for other species besides C. elegans","title":"genomes-nf"},{"location":"pipeline-genomes-nf/#genomes-nf","text":"genomes-nf Pipeline overview Software requirements Usage Parameters -profile (optional) --wb_version (optional) --projects (optional) --output (optional) Output Notes genomes-nf is a Nextflow pipeline for managing reference genomes and annotation files. Important When adding a new WormBase version reference genome, especially for c_elegans it is essential that you use this pipeline instead of downloading and adding the files to QUEST manually. These files and this file structure are essential to many other pipelines in the lab.","title":"genomes-nf"},{"location":"pipeline-genomes-nf/#pipeline_overview","text":">AAGACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAA AGACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC GAAATA CGCG GA ATATA AAAA TCG TCGAT GC GGGC CGATCGA TAGAT GA TATATCGC TTAAC ACTAGAGGGG CTATCGAC CGAA CT GACTA CT GCG AT AGCATCACG TGGGGGCCTATC CGAC AA TCAGCTACGAAAT AGCCC TCTATCATAA TATAT T TCT TC AGCGA GA A A T TC ATAAAT TCGGGATCTAGC A CGC AT ATATATATGC GCGAT TCTAC AG GCGGGGGA AT TA AA AAGAC CG TC AT GC AGCTGGGGGC ACG GA TA AT GA CTATATATATCGC AATGC ACTAGAG GG CTATCGAC ACG A CT GACTA CT AGCGG AT AGCATCACGATGGG GCCTATC ACG C AA TCAGCTACGAAAT ACTCC TCTATCA AA AAATATAT TCTC TC AGCGA GA AAACT TC TTCATAAATCTCGG ATCTAGC ATCG AT TATATATATATGC TTAATA FCG GA ATATA AAA TCG TCGAT GC GG ACGATCGA TAGAT GA CTATATATATCGC AACACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAAT CTACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC parameters description Set/Default ========== =========== ======================== --wb_version wormbase version to build WS276 --projects comma-delimited list of `species/project_id` c_elegans/PRJNA13758,c_briggsae/PRJNA10731,c_tropicalis/PRJNA53597 --output Path of output folder /projects/b1059/data/ This repo contains a nextflow pipeline that downloads, indexes, and builds annotation databases for reference genomes from wormbase. The following outputs are created: A BWA Index SNPeff annotation database CSQ annotation database Samtools faidx index A GATK Sequence dictionary file","title":"Pipeline overview"},{"location":"pipeline-genomes-nf/#software_requirements","text":"Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env If running pipeline on Quest, you must first load singularity to access the docker container: module load singularity If running locally, Docker must be installed. For further instructions, check out our docker guide","title":"Software requirements"},{"location":"pipeline-genomes-nf/#usage","text":"The pipeline can be run locally or on Quest. For example: nextflow run main.nf -resume -profile local --wb_version=WS276 --projects=c_elegans/PRJNA13758","title":"Usage"},{"location":"pipeline-genomes-nf/#parameters","text":"","title":"Parameters"},{"location":"pipeline-genomes-nf/#-profile_optional","text":"Can be set to local or quest . The pipeline uses the andersenlab/genomes docker image built from env/genome.Dockerfile . The image is automatically built using github actions. See .github/workflows/build.yml for details. Note The default profile is set to -profile=quest","title":"-profile (optional)"},{"location":"pipeline-genomes-nf/#--wb_version_optional","text":"The wormbase version to build. For example, WS279 . Default is WS276 .","title":"--wb_version (optional)"},{"location":"pipeline-genomes-nf/#--projects_optional","text":"A comma-delimited list of species/project_id identifiers. A table below lists the current projects that can be downloaded. This table is regenerated as the first step of the pipeline, and stored as a file called project_species.tsv in the params.output folder ( ./genomes if working locally). By default, the pipeline will generate reference genome indices and annotations for: c_elegans/PRJNA13758 - N2 based reference genome c_briggsae/PRJNA10731 c_tropicalis/PRJNA53597 The current set of available species/projects that can be built are: species project b_xylophilus PRJEA64437 c_briggsae PRJNA10731 c_angaria PRJNA51225 a_ceylanicum PRJNA231479 a_suum PRJNA62057 a_suum PRJNA80881 b_malayi PRJNA10729 c_brenneri PRJNA20035 c_elegans PRJEB28388 c_elegans PRJNA13758 c_elegans PRJNA275000 c_latens PRJNA248912 c_remanei PRJNA248909 c_remanei PRJNA248911 c_remanei PRJNA53967 c_inopinata PRJDB5687 c_japonica PRJNA12591 c_sp11 PRJNA53597 c_sp5 PRJNA194557 c_nigoni PRJNA384657 c_sinica PRJNA194557 c_tropicalis PRJNA53597 d_immitis PRJEB1797 h_bacteriophora PRJNA13977 l_loa PRJNA60051 m_hapla PRJNA29083 m_incognita PRJEA28837 h_contortus PRJEB506 h_contortus PRJNA205202 n_americanus PRJNA72135 p_exspectatus PRJEB6009 o_tipulae PRJEB15512 p_redivivus PRJNA186477 s_ratti PRJEA62033 s_ratti PRJEB125 o_volvulus PRJEB513 p_pacificus PRJNA12644 t_muris PRJEB126 t_spiralis PRJNA12603 t_suis PRJNA208415 t_suis PRJNA208416","title":"--projects (optional)"},{"location":"pipeline-genomes-nf/#--output_optional","text":"Path of output folder with results. Default is /projects/b1059/data/{species}/genomes/{projectID}/{WSbuild}/","title":"--output (optional)"},{"location":"pipeline-genomes-nf/#output","text":"Outputs are nested under params.output with the following structure: c_elegans (species) \u2514\u2500\u2500 genomes \u2514\u2500\u2500 PRJNA13758 (project) \u2514\u2500\u2500 WS276 (build) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.dict (dict file) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz (fasta) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.amb (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.ann (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.bwt (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.fai (samtools faidx index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.gzi (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.pac (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.sa (bwa index) \u251c\u2500\u2500 csq \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz (CSQ annotation GFF3) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Length.tsv (protein lengths) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Scores.tsv (blosum and grantham scores) \u251c\u2500\u2500 lcr \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz (low complexity regions) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz (low complexity regions) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz.tbi (tabix index) \u2514\u2500\u2500 snpeff \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276 (tabix index) \u2502 \u251c\u2500\u2500 genes.gtf.gz (Reference GTF) \u2502 \u251c\u2500\u2500 sequences.fa (fasta genome (unzipped)) \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin (snpEff annotation db) \u2514\u2500\u2500 snpEff.config (snpEff configuration file)","title":"Output"},{"location":"pipeline-genomes-nf/#notes","text":"The SNPeff databases are not collected together in one location as is often the case. Instead, they are stored individually with their own configuration files. The GFF3 files for some species are not as developed as C. elegans . As a consequence, the biotype is inferred from the Attributes column of the GFF. See bin/format_csq.R for more details. Warning The updated csq-formated gff script needs to be updated for other species besides C. elegans","title":"Notes"},{"location":"pipeline-nil-ril/","text":"nil-ril-nf \u00b6 nil-ril-nf Overview Docker image Usage Requirements Profiles and Running the Pipeline Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Testing Parameters --fqs --species --vcf --reference --A, --B --debug --cores --cA, --cB --out --relative --tmpdir Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ Organizing final data The nil-ril-nf pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data. \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --species Choose species for analysis null (option: c_elegans) --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --relative use relative fastq prefix ${params.relative} --reference Reference Genome /Users/dancook/Documents/git/nil-nf/reference/WS245.fa.gz --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default). Overview \u00b6 The nil-ril-nf pipeline: Alignment - Performed using bwa-mem Merge Bams - Combines bam files aligned individually for each fastq-pair. Sambamba is actually used in place of samtools, but it's a drop-in, faster replacement. Bam Stats - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis. Mark Duplicates - Duplicate reads are marked using Picard. Call Variants individual - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population. Pull parental genotypes - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF. Call variants union - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls. Merge VCF - Merges in the parental VCF (which has been filtered only for variants with discordant calls). Call HMM - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data. Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible. Docker image \u00b6 The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image: andersenlab/nil-ril-nf The Dockerfile is stored in the root of the nil-ril-nf github repo and is automatically built on Dockerhub whenever the repo is pushed. Usage \u00b6 Requirements \u00b6 The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Profiles and Running the Pipeline \u00b6 The nextflow.config file included with this pipeline contains four profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. travis - Used by travis-ci for testing purposes. Running the pipeline locally \u00b6 When running locally, the pipeline will run using the andersenlab/nil-ril-nf docker image. You must have docker installed. You will need to obtain a reference genome to run the alignment with as well. You can use the following command to obtain the reference: curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz Run the pipeline locally with: nextflow run main.nf -profile local -resume Debugging the pipeline on Quest \u00b6 If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume Running the pipeline on Quest \u00b6 If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest -resume Testing \u00b6 If you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the -resume option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly. Additionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci here , and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail. The command below can be used to test the pipeline locally. # Downloads a pre-indexed reference curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz # Run nextflow nextflow run andersenlab/nil-ril-nf \\ -with-docker andersenlab/nil-ril-nf \\ --debug \\ --reference=WS276.fa.gz \\ -resume Note that the path to the vcf will change slightly in releases later than WI-20170531; See the wi-gatk pipeline for details. The command above will automatically place results in a folder: NIL-N2-CB4856-YYYY-MM-DD Parameters \u00b6 --fqs \u00b6 In order to process NIL/RIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq can be specified as relative or absolute . By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not include the parental strains in the fq_sheet. If you re-sequenced the parent strains and want to include them in the analysis as a control, you need to rename the parent strains to avoid an error in merging the VCFs (i.e. N2 becomes N2-1). --species \u00b6 If profile = quest , you can choose species as c_elegans ( c_briggsae and c_tropicalis to come soon) to populate the --reference and --vcf fields with the default values for that species. Otherwise, leave --species null (default) and provide your own --reference and --vcf . Note --species = c_elegans populates the following parameters: --vcf = /projects/b1059/analysis/WI-20210121/isotype_only/WI.20210121.hard-filter.isotype.vcf.gz --reference = /projects/b1059/data/genomes/c_elegans/c_elegans.PRJNA13758.WS276.genomic.fa.gz --vcf \u00b6 Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, the most recent C. elegans VCF could be found here: /projects/b1059/analysis/WI-20210121/isotype_only/WI.20210121.hard-filter.isotype.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20210121.hard-filter.isotype.vcf.gz --reference \u00b6 A fasta reference indexed with BWA. For example, the C. elegans reference could be found here: /projects/b1059/data/genomes/c_elegans/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genomic.fa.gz --A, --B \u00b6 Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details. --debug \u00b6 The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the Testing section for more information. --cores \u00b6 The number of cores to use during alignments and variant calling. Default is 4. --cA, --cB \u00b6 The color to use for parental strain A and B on plots. Default is orange and blue. --out \u00b6 A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains. --relative \u00b6 If you want to specify fastqs using an absolute path use --relative=false . Set to true by default. --tmpdir \u00b6 A directory for storing temporary data. Output \u00b6 The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u251c\u2500\u2500 SM_union_vcfs.txt \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u251c\u2500\u2500 gt_hmm.tsv \u2502 \u251c\u2500\u2500 gt_hmm_fill.tsv \u2502 \u251c\u2500\u2500 NIL.filtered.stats.txt \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u2502 \u2514\u2500\u2500 gt_hmm_genotypes.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u2514\u2500 sitelist \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi] log.txt \u00b6 A summary of the nextflow run. duplicates/ \u00b6 bam_duplicates.tsv - A summary of duplicate reads from aligned bams. fq/ \u00b6 fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq SM/ \u00b6 If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. SM_union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz hmm/ \u00b6 Important gt_hmm_fill.tsv is for visualization purposes only. To determine breakpoints you should use gt_hmm.tsv . The --infill and --endfill options are applied to the gt_hmm_fill.tsv file. You need to be cautious when examining this data as it is generated primarily for visualization purposes . gt_hmm.(png/svg) - Haplotype plot using --infill and --endfill . gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation . This file is used to generate the plots. gt_hmm.tsv - Haplotypes defined by region with associated information. Does not use --infill and --endfill gt_hmm_genotypes.tsv - Long form genotypes file. NIL/RIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL/RIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL/RIL.hmm.vcf.gz - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes. plots/ \u00b6 coverage_comparison.png - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/pdf) - Coverage vs. percent duplicated. unmapped_reads.png - Coverage vs. unmapped read percent. sitelist/ \u00b6 <A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. <A>.<B>.sitelist.vcf.gz[+.tbi] - A vcf of sites found to be different between both parental strains. Organizing final data \u00b6","title":"nil-ril-nf"},{"location":"pipeline-nil-ril/#nil-ril-nf","text":"nil-ril-nf Overview Docker image Usage Requirements Profiles and Running the Pipeline Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Testing Parameters --fqs --species --vcf --reference --A, --B --debug --cores --cA, --cB --out --relative --tmpdir Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ Organizing final data The nil-ril-nf pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data. \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --species Choose species for analysis null (option: c_elegans) --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --relative use relative fastq prefix ${params.relative} --reference Reference Genome /Users/dancook/Documents/git/nil-nf/reference/WS245.fa.gz --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default).","title":"nil-ril-nf"},{"location":"pipeline-nil-ril/#overview","text":"The nil-ril-nf pipeline: Alignment - Performed using bwa-mem Merge Bams - Combines bam files aligned individually for each fastq-pair. Sambamba is actually used in place of samtools, but it's a drop-in, faster replacement. Bam Stats - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis. Mark Duplicates - Duplicate reads are marked using Picard. Call Variants individual - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population. Pull parental genotypes - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF. Call variants union - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls. Merge VCF - Merges in the parental VCF (which has been filtered only for variants with discordant calls). Call HMM - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data. Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible.","title":"Overview"},{"location":"pipeline-nil-ril/#docker_image","text":"The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image: andersenlab/nil-ril-nf The Dockerfile is stored in the root of the nil-ril-nf github repo and is automatically built on Dockerhub whenever the repo is pushed.","title":"Docker image"},{"location":"pipeline-nil-ril/#usage","text":"","title":"Usage"},{"location":"pipeline-nil-ril/#requirements","text":"The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env","title":"Requirements"},{"location":"pipeline-nil-ril/#profiles_and_running_the_pipeline","text":"The nextflow.config file included with this pipeline contains four profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. travis - Used by travis-ci for testing purposes.","title":"Profiles and Running the Pipeline"},{"location":"pipeline-nil-ril/#running_the_pipeline_locally","text":"When running locally, the pipeline will run using the andersenlab/nil-ril-nf docker image. You must have docker installed. You will need to obtain a reference genome to run the alignment with as well. You can use the following command to obtain the reference: curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz Run the pipeline locally with: nextflow run main.nf -profile local -resume","title":"Running the pipeline locally"},{"location":"pipeline-nil-ril/#debugging_the_pipeline_on_quest","text":"If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume","title":"Debugging the pipeline on Quest"},{"location":"pipeline-nil-ril/#running_the_pipeline_on_quest","text":"If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest -resume","title":"Running the pipeline on Quest"},{"location":"pipeline-nil-ril/#testing","text":"If you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the -resume option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly. Additionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci here , and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail. The command below can be used to test the pipeline locally. # Downloads a pre-indexed reference curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz # Run nextflow nextflow run andersenlab/nil-ril-nf \\ -with-docker andersenlab/nil-ril-nf \\ --debug \\ --reference=WS276.fa.gz \\ -resume Note that the path to the vcf will change slightly in releases later than WI-20170531; See the wi-gatk pipeline for details. The command above will automatically place results in a folder: NIL-N2-CB4856-YYYY-MM-DD","title":"Testing"},{"location":"pipeline-nil-ril/#parameters","text":"","title":"Parameters"},{"location":"pipeline-nil-ril/#--fqs","text":"In order to process NIL/RIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq can be specified as relative or absolute . By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not include the parental strains in the fq_sheet. If you re-sequenced the parent strains and want to include them in the analysis as a control, you need to rename the parent strains to avoid an error in merging the VCFs (i.e. N2 becomes N2-1).","title":"--fqs"},{"location":"pipeline-nil-ril/#--species","text":"If profile = quest , you can choose species as c_elegans ( c_briggsae and c_tropicalis to come soon) to populate the --reference and --vcf fields with the default values for that species. Otherwise, leave --species null (default) and provide your own --reference and --vcf . Note --species = c_elegans populates the following parameters: --vcf = /projects/b1059/analysis/WI-20210121/isotype_only/WI.20210121.hard-filter.isotype.vcf.gz --reference = /projects/b1059/data/genomes/c_elegans/c_elegans.PRJNA13758.WS276.genomic.fa.gz","title":"--species"},{"location":"pipeline-nil-ril/#--vcf","text":"Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, the most recent C. elegans VCF could be found here: /projects/b1059/analysis/WI-20210121/isotype_only/WI.20210121.hard-filter.isotype.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20210121.hard-filter.isotype.vcf.gz","title":"--vcf"},{"location":"pipeline-nil-ril/#--reference","text":"A fasta reference indexed with BWA. For example, the C. elegans reference could be found here: /projects/b1059/data/genomes/c_elegans/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genomic.fa.gz","title":"--reference"},{"location":"pipeline-nil-ril/#--a_--b","text":"Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details.","title":"--A, --B"},{"location":"pipeline-nil-ril/#--debug","text":"The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the Testing section for more information.","title":"--debug"},{"location":"pipeline-nil-ril/#--cores","text":"The number of cores to use during alignments and variant calling. Default is 4.","title":"--cores"},{"location":"pipeline-nil-ril/#--ca_--cb","text":"The color to use for parental strain A and B on plots. Default is orange and blue.","title":"--cA, --cB"},{"location":"pipeline-nil-ril/#--out","text":"A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains.","title":"--out"},{"location":"pipeline-nil-ril/#--relative","text":"If you want to specify fastqs using an absolute path use --relative=false . Set to true by default.","title":"--relative"},{"location":"pipeline-nil-ril/#--tmpdir","text":"A directory for storing temporary data.","title":"--tmpdir"},{"location":"pipeline-nil-ril/#output","text":"The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u251c\u2500\u2500 SM_union_vcfs.txt \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u251c\u2500\u2500 gt_hmm.tsv \u2502 \u251c\u2500\u2500 gt_hmm_fill.tsv \u2502 \u251c\u2500\u2500 NIL.filtered.stats.txt \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u2502 \u2514\u2500\u2500 gt_hmm_genotypes.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u2514\u2500 sitelist \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi]","title":"Output"},{"location":"pipeline-nil-ril/#logtxt","text":"A summary of the nextflow run.","title":"log.txt"},{"location":"pipeline-nil-ril/#duplicates","text":"bam_duplicates.tsv - A summary of duplicate reads from aligned bams.","title":"duplicates/"},{"location":"pipeline-nil-ril/#fq","text":"fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq","title":"fq/"},{"location":"pipeline-nil-ril/#sm","text":"If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. SM_union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz","title":"SM/"},{"location":"pipeline-nil-ril/#hmm","text":"Important gt_hmm_fill.tsv is for visualization purposes only. To determine breakpoints you should use gt_hmm.tsv . The --infill and --endfill options are applied to the gt_hmm_fill.tsv file. You need to be cautious when examining this data as it is generated primarily for visualization purposes . gt_hmm.(png/svg) - Haplotype plot using --infill and --endfill . gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation . This file is used to generate the plots. gt_hmm.tsv - Haplotypes defined by region with associated information. Does not use --infill and --endfill gt_hmm_genotypes.tsv - Long form genotypes file. NIL/RIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL/RIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL/RIL.hmm.vcf.gz - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes.","title":"hmm/"},{"location":"pipeline-nil-ril/#plots","text":"coverage_comparison.png - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/pdf) - Coverage vs. percent duplicated. unmapped_reads.png - Coverage vs. unmapped read percent.","title":"plots/"},{"location":"pipeline-nil-ril/#sitelist","text":"<A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. <A>.<B>.sitelist.vcf.gz[+.tbi] - A vcf of sites found to be different between both parental strains.","title":"sitelist/"},{"location":"pipeline-nil-ril/#organizing_final_data","text":"","title":"Organizing final data"},{"location":"pipeline-overview/","text":"Pipeline Overview \u00b6 Pipeline Overview Wild isolate sequencing NIL or RIL sequencing Andersen Lab Coding Best Practices General R QUEST Python An overview of the sequencing pipelines is shown below. Wild isolate data are processed by multiple pipelines. NIL/RIL sequence data are only processed by one pipeline. Wild isolate sequencing \u00b6 NIL or RIL sequencing \u00b6 Andersen Lab Coding Best Practices \u00b6 These best practices are just a few of the important coding tips and tricks for reproducible research. If you have more ideas, contact Katie! General \u00b6 You should be doing most (if not all) of your analyses in ~/Dropbox/AndersenLab/LabFolders/YourName (except for QUEST, see below) This is (1) to make sure the data is backed up/saved with version history and (2) to allow other lab members to access your code/scripts when necessary Do NOT use spaces in names of files or folders. Try not to use spaces in column names too (although sometimes it is necessary for a final table output) Computers often have a hard time reading spaces and code used to ignore spaces can vary from program to program Instead, you can use _ or . or - or capitalization ( fileName.txt ) NEVER replace raw data!!!!! You should save your raw data in the rawest format, write a script to analyze it, then if you wish, save the processed data for further use. This is important because it always allows you to go back to the original raw data in case something happens Some suggested project folder structure might look like something below: Include a README.md (or README.txt ) file in each project folder to explain where the data and scripts can be found for certain analyses. Trust me, after a few years you will definitely forget\u2026 And don\u2019t forget to update the README regularly, an old README doesn\u2019t do anyone good! Either use full path names in scripts or be explicit about where the working directory is This is important to allow other people to run your code (or might even be helpful for you if you ever reorganize folders one day) Date your files, especially when you update an existing file. Write dates in YYYYMMDD format As much as possible, ensure that your processed data is \u201ctidy\u201d (see below). This doesn\u2019t work for all complex data types, but it should be a general norm to follow. Each variable must have its own column Each observation must have its own row Each value must have its own cell No color or highlighting No empty cells (fill with NA if necessary) Save data as plain text files ( .csv , .tsv , .txt etc. -- NOT .xls !!!) R \u00b6 ALWAYS use namespaces before functions from packages (i.e. dplyr::filter() instead of filter() ) This includes ggplot2 and especially dplyr !!! Some packages have functions with the same name, so adding a namespace is crucial for reproducibility. Also, this helps other people read your code by knowing which functions came from which packages When piping with Tidyverse ( %>% ), press <Enter> to go to the next line after a pipe This makes your code more readable In fact, general practices state no more than 80-100 characters per line of code EVER to increase readability QUEST \u00b6 You should be doing most (if not all) of your analyses in /projects/b1059/projects/yourName (not your home directory (i.e. /home/netid )) Important Main exception: Nextflow temporary working directories should NOT be on b1059 (it will fill us up!) but rather in the scratch space b1042 (files get automatically deleted here every 30 days). A correctly designed nextflow.config file will take care of this. Python \u00b6 [ Needs filling in from someone who uses python :) ]","title":"Overview"},{"location":"pipeline-overview/#pipeline_overview","text":"Pipeline Overview Wild isolate sequencing NIL or RIL sequencing Andersen Lab Coding Best Practices General R QUEST Python An overview of the sequencing pipelines is shown below. Wild isolate data are processed by multiple pipelines. NIL/RIL sequence data are only processed by one pipeline.","title":"Pipeline Overview"},{"location":"pipeline-overview/#wild_isolate_sequencing","text":"","title":"Wild isolate sequencing"},{"location":"pipeline-overview/#nil_or_ril_sequencing","text":"","title":"NIL or RIL sequencing"},{"location":"pipeline-overview/#andersen_lab_coding_best_practices","text":"These best practices are just a few of the important coding tips and tricks for reproducible research. If you have more ideas, contact Katie!","title":"Andersen Lab Coding Best Practices"},{"location":"pipeline-overview/#general","text":"You should be doing most (if not all) of your analyses in ~/Dropbox/AndersenLab/LabFolders/YourName (except for QUEST, see below) This is (1) to make sure the data is backed up/saved with version history and (2) to allow other lab members to access your code/scripts when necessary Do NOT use spaces in names of files or folders. Try not to use spaces in column names too (although sometimes it is necessary for a final table output) Computers often have a hard time reading spaces and code used to ignore spaces can vary from program to program Instead, you can use _ or . or - or capitalization ( fileName.txt ) NEVER replace raw data!!!!! You should save your raw data in the rawest format, write a script to analyze it, then if you wish, save the processed data for further use. This is important because it always allows you to go back to the original raw data in case something happens Some suggested project folder structure might look like something below: Include a README.md (or README.txt ) file in each project folder to explain where the data and scripts can be found for certain analyses. Trust me, after a few years you will definitely forget\u2026 And don\u2019t forget to update the README regularly, an old README doesn\u2019t do anyone good! Either use full path names in scripts or be explicit about where the working directory is This is important to allow other people to run your code (or might even be helpful for you if you ever reorganize folders one day) Date your files, especially when you update an existing file. Write dates in YYYYMMDD format As much as possible, ensure that your processed data is \u201ctidy\u201d (see below). This doesn\u2019t work for all complex data types, but it should be a general norm to follow. Each variable must have its own column Each observation must have its own row Each value must have its own cell No color or highlighting No empty cells (fill with NA if necessary) Save data as plain text files ( .csv , .tsv , .txt etc. -- NOT .xls !!!)","title":"General"},{"location":"pipeline-overview/#r","text":"ALWAYS use namespaces before functions from packages (i.e. dplyr::filter() instead of filter() ) This includes ggplot2 and especially dplyr !!! Some packages have functions with the same name, so adding a namespace is crucial for reproducibility. Also, this helps other people read your code by knowing which functions came from which packages When piping with Tidyverse ( %>% ), press <Enter> to go to the next line after a pipe This makes your code more readable In fact, general practices state no more than 80-100 characters per line of code EVER to increase readability","title":"R"},{"location":"pipeline-overview/#quest","text":"You should be doing most (if not all) of your analyses in /projects/b1059/projects/yourName (not your home directory (i.e. /home/netid )) Important Main exception: Nextflow temporary working directories should NOT be on b1059 (it will fill us up!) but rather in the scratch space b1042 (files get automatically deleted here every 30 days). A correctly designed nextflow.config file will take care of this.","title":"QUEST"},{"location":"pipeline-overview/#python","text":"[ Needs filling in from someone who uses python :) ]","title":"Python"},{"location":"pipeline-postGATK/","text":"post-gatk-nf \u00b6 post-gatk-nf Pipeline overview Software Requirements Usage Testing on Quest Running on Quest Parameters --debug --sample_sheet --vcf --species (optional) --project (optional) --ws_build (optional) --output (optional) Output Data storage Cleanup Updating CeNDR Updating cegwas/NemaScan The post-gatk-nf pipeline performs two main tasks: (1) variant annotation for the VCF at the isotype level and (2) population genetics analyses (such as identifying shared haplotypes and divergent regions) at the isotype level. The VCFs output from this pipeline are used within the lab and also released to the world via CeNDR. This page details how to run the pipeline. Pipeline overview \u00b6 * * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * ** *** * * * * * * * * *** * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode (optional) --vcf Hard filtered vcf to calculate variant density (required) --sample_sheet TSV with column iso-ref strain, bam, bai (no header) (required) --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' c_elegans --project Project name for species reference PRJNA13758 --ws_build WormBase version for species reference WS276 --output Output folder name. popgen-date (in current folder) The logo above looks better in your terminal! Software Requirements \u00b6 The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update Important This pipeline currently only supports analysis on Quest, cannot be run locally Usage \u00b6 Testing on Quest \u00b6 This command uses a test dataset nextflow run main.nf --debug Running on Quest \u00b6 You should run this in a screen session. nextflow run main.nf --vcf <path_to_vcf> --sample_sheet <path_to_sample_sheet> Parameters \u00b6 --debug \u00b6 You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run main.nf --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv --sample_sheet \u00b6 A custom sample sheet can be specified using --sample_sheet . The sample sheet is generated from the sample sheet used as input for wi-gatk-nf with only columns for strain, bam, and bai subsetted. Make sure to remove any strains that you do not want to include in this analysis. Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . Important There is no header for the sample sheet! The sample sheet has the following columns: strain - the name of the strain bam - name of the bam alignment file bai - name of the bam alignment index file Note As of 20210501, bam and bam.bai files for all strains of a particular species can be found in one singular location: /projects/b1059/data/{species}/WI/alignments/ so there is no longer need to provide the location of the bam files. --vcf \u00b6 Path to the hard-filtered vcf output from wi-gatk . VCF should contain ALL strains, the first step will be to subset isotype reference strains for further analysis. Note This should be the hard-filtered VCF --species (optional) \u00b6 default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis --project (optional) \u00b6 default = PRJNA13758 WormBase project ID for selected species. Choose from some examples here --ws_build (optional) \u00b6 default = WS276 WormBase version to use for reference genome. --output (optional) \u00b6 default - popgen-YYYYMMDD A directory in which to output results. If you have set --debug true , the default output directory will be popgen-YYYYMMDD-debug . Output \u00b6 \u251c\u2500\u2500 bcsq \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.tbi \u2502 \u2514\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.stats.txt \u251c\u2500\u2500 divergent_regions \u2502 \u251c\u2500\u2500 Mask_DF \u2502 \u2502 \u2514\u2500\u2500 [strain]_Mask_DF.tsv | \u2514\u2500\u2500 divergent_regions_strain.bed \u251c\u2500\u2500 haplotype \u2502 \u251c\u2500\u2500 haplotype_length.pdf \u2502 \u251c\u2500\u2500 sweep_summary.tsv \u2502 \u251c\u2500\u2500 max_haplotype_genome_wide.pdf \u2502 \u251c\u2500\u2500 haplotype.pdf \u2502 \u251c\u2500\u2500 haplotype.tsv \u2502 \u251c\u2500\u2500 [chr].ibd \u2502 \u2514\u2500\u2500 haplotype_plot_df.Rda \u251c\u2500\u2500 snpeff \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz.tbi \u2502 \u2514\u2500\u2500 snpeff.stats.csv \u251c\u2500\u2500 tree \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.min4.tree \u2502 \u2514\u2500\u2500 WI.{date}.hard-filter.min4.tree \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz \u2514\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz.tbi Data storage \u00b6 Cleanup \u00b6 Once the pipeline has complete successfully and you are satisfied with the results, the final data can be moved to their final storage place on Quest accordingly: Everything in the haplotype foder can be moved to /projects/b1059/data/{species}/WI/haplotype/{date} Everything in the divergent_regions folder can be moved to /projects/b1059/data/{species}/WI/divergent_regions/{date} Everything in the tree folder can be moved to /projects/b1059/data/{species}/WI/tree/{date} ALL strain and isotype vcf and index files (including snpeff and bcsq annotated vcfs) can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf Strain-specific vcf and index files can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf/strain_vcf If applicable, all snpeff .bed files (HIGH, LOW, MODERATE, etc.) can be moved to /projects/b1059/data/{species}/WI/variation/{date}/tracks/ Updating CeNDR \u00b6 Check out the CeNDR page for more information about updating a new data release for CeNDR. Updating cegwas / NemaScan \u00b6 Once a new CeNDR release is ready, it is important to also update the genome-wide association mapping packages to ensure users can appropriately analyze data from new strains as well as old strains. Here is a list of things that need to be updated: The default vcf should be changed to the newest release date (i.e. from 20200815 to 20210121). Users will still have the option to use an earlier vcf. The divergent_regions_strain.bed file needs to be copied into the bin/ of the mapping pipeline and renamed divergent_df_isotype.bed The file div_isotype_list.txt needs to be created in the bin/ by running the following command: bcftools query -l WI.{date}.hard-filter.vcf.gz > div_isotype_list.txt The haplotype_df_isotype.bed file needs to be copied into the bin/ of the mapping pipeline The strain_isotype_lookup.tsv file needs to be updated to include the most recent strains. This file can be created using the following commands: #!/usr/bin/env Rscript library(gsheet) library(dplyr) wi <- gsheet::gsheet2tbl(\"<path_to_wi_master_sheet>\") %>% dplyr::select(strain, previous_names, isotype) %>% readr::write_tsv(\"strain_isotype_lookup.tsv\") Note Although users will have the option to use an older vcf, the divergent region data will always be pulled from the most recent release. There could be minor changes from release to release. If this is a concern, we could switch to pulling the divergent data directly from b1059 instead of including it in the bin/ of the mapping pipeline.","title":"post-gatk-nf"},{"location":"pipeline-postGATK/#post-gatk-nf","text":"post-gatk-nf Pipeline overview Software Requirements Usage Testing on Quest Running on Quest Parameters --debug --sample_sheet --vcf --species (optional) --project (optional) --ws_build (optional) --output (optional) Output Data storage Cleanup Updating CeNDR Updating cegwas/NemaScan The post-gatk-nf pipeline performs two main tasks: (1) variant annotation for the VCF at the isotype level and (2) population genetics analyses (such as identifying shared haplotypes and divergent regions) at the isotype level. The VCFs output from this pipeline are used within the lab and also released to the world via CeNDR. This page details how to run the pipeline.","title":"post-gatk-nf"},{"location":"pipeline-postGATK/#pipeline_overview","text":"* * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * ** *** * * * * * * * * *** * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode (optional) --vcf Hard filtered vcf to calculate variant density (required) --sample_sheet TSV with column iso-ref strain, bam, bai (no header) (required) --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' c_elegans --project Project name for species reference PRJNA13758 --ws_build WormBase version for species reference WS276 --output Output folder name. popgen-date (in current folder) The logo above looks better in your terminal!","title":"Pipeline overview"},{"location":"pipeline-postGATK/#software_requirements","text":"The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update Important This pipeline currently only supports analysis on Quest, cannot be run locally","title":"Software Requirements"},{"location":"pipeline-postGATK/#usage","text":"","title":"Usage"},{"location":"pipeline-postGATK/#testing_on_quest","text":"This command uses a test dataset nextflow run main.nf --debug","title":"Testing on Quest"},{"location":"pipeline-postGATK/#running_on_quest","text":"You should run this in a screen session. nextflow run main.nf --vcf <path_to_vcf> --sample_sheet <path_to_sample_sheet>","title":"Running on Quest"},{"location":"pipeline-postGATK/#parameters","text":"","title":"Parameters"},{"location":"pipeline-postGATK/#--debug","text":"You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run main.nf --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv","title":"--debug"},{"location":"pipeline-postGATK/#--sample_sheet","text":"A custom sample sheet can be specified using --sample_sheet . The sample sheet is generated from the sample sheet used as input for wi-gatk-nf with only columns for strain, bam, and bai subsetted. Make sure to remove any strains that you do not want to include in this analysis. Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . Important There is no header for the sample sheet! The sample sheet has the following columns: strain - the name of the strain bam - name of the bam alignment file bai - name of the bam alignment index file Note As of 20210501, bam and bam.bai files for all strains of a particular species can be found in one singular location: /projects/b1059/data/{species}/WI/alignments/ so there is no longer need to provide the location of the bam files.","title":"--sample_sheet"},{"location":"pipeline-postGATK/#--vcf","text":"Path to the hard-filtered vcf output from wi-gatk . VCF should contain ALL strains, the first step will be to subset isotype reference strains for further analysis. Note This should be the hard-filtered VCF","title":"--vcf"},{"location":"pipeline-postGATK/#--species_optional","text":"default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis","title":"--species (optional)"},{"location":"pipeline-postGATK/#--project_optional","text":"default = PRJNA13758 WormBase project ID for selected species. Choose from some examples here","title":"--project (optional)"},{"location":"pipeline-postGATK/#--ws_build_optional","text":"default = WS276 WormBase version to use for reference genome.","title":"--ws_build (optional)"},{"location":"pipeline-postGATK/#--output_optional","text":"default - popgen-YYYYMMDD A directory in which to output results. If you have set --debug true , the default output directory will be popgen-YYYYMMDD-debug .","title":"--output (optional)"},{"location":"pipeline-postGATK/#output","text":"\u251c\u2500\u2500 bcsq \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.tbi \u2502 \u2514\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.stats.txt \u251c\u2500\u2500 divergent_regions \u2502 \u251c\u2500\u2500 Mask_DF \u2502 \u2502 \u2514\u2500\u2500 [strain]_Mask_DF.tsv | \u2514\u2500\u2500 divergent_regions_strain.bed \u251c\u2500\u2500 haplotype \u2502 \u251c\u2500\u2500 haplotype_length.pdf \u2502 \u251c\u2500\u2500 sweep_summary.tsv \u2502 \u251c\u2500\u2500 max_haplotype_genome_wide.pdf \u2502 \u251c\u2500\u2500 haplotype.pdf \u2502 \u251c\u2500\u2500 haplotype.tsv \u2502 \u251c\u2500\u2500 [chr].ibd \u2502 \u2514\u2500\u2500 haplotype_plot_df.Rda \u251c\u2500\u2500 snpeff \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz.tbi \u2502 \u2514\u2500\u2500 snpeff.stats.csv \u251c\u2500\u2500 tree \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.min4.tree \u2502 \u2514\u2500\u2500 WI.{date}.hard-filter.min4.tree \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz \u2514\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz.tbi","title":"Output"},{"location":"pipeline-postGATK/#data_storage","text":"","title":"Data storage"},{"location":"pipeline-postGATK/#cleanup","text":"Once the pipeline has complete successfully and you are satisfied with the results, the final data can be moved to their final storage place on Quest accordingly: Everything in the haplotype foder can be moved to /projects/b1059/data/{species}/WI/haplotype/{date} Everything in the divergent_regions folder can be moved to /projects/b1059/data/{species}/WI/divergent_regions/{date} Everything in the tree folder can be moved to /projects/b1059/data/{species}/WI/tree/{date} ALL strain and isotype vcf and index files (including snpeff and bcsq annotated vcfs) can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf Strain-specific vcf and index files can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf/strain_vcf If applicable, all snpeff .bed files (HIGH, LOW, MODERATE, etc.) can be moved to /projects/b1059/data/{species}/WI/variation/{date}/tracks/","title":"Cleanup"},{"location":"pipeline-postGATK/#updating_cendr","text":"Check out the CeNDR page for more information about updating a new data release for CeNDR.","title":"Updating CeNDR"},{"location":"pipeline-postGATK/#updating_cegwasnemascan","text":"Once a new CeNDR release is ready, it is important to also update the genome-wide association mapping packages to ensure users can appropriately analyze data from new strains as well as old strains. Here is a list of things that need to be updated: The default vcf should be changed to the newest release date (i.e. from 20200815 to 20210121). Users will still have the option to use an earlier vcf. The divergent_regions_strain.bed file needs to be copied into the bin/ of the mapping pipeline and renamed divergent_df_isotype.bed The file div_isotype_list.txt needs to be created in the bin/ by running the following command: bcftools query -l WI.{date}.hard-filter.vcf.gz > div_isotype_list.txt The haplotype_df_isotype.bed file needs to be copied into the bin/ of the mapping pipeline The strain_isotype_lookup.tsv file needs to be updated to include the most recent strains. This file can be created using the following commands: #!/usr/bin/env Rscript library(gsheet) library(dplyr) wi <- gsheet::gsheet2tbl(\"<path_to_wi_master_sheet>\") %>% dplyr::select(strain, previous_names, isotype) %>% readr::write_tsv(\"strain_isotype_lookup.tsv\") Note Although users will have the option to use an older vcf, the divergent region data will always be pulled from the most recent release. There could be minor changes from release to release. If this is a concern, we could switch to pulling the divergent data directly from b1059 instead of including it in the bin/ of the mapping pipeline.","title":"Updating cegwas/NemaScan"},{"location":"pipeline-trimming/","text":"trim-fq-nf \u00b6 trim-fq-nf Pipeline overview Software requirements Usage Testing the pipeline on QUEST Running the pipeline on QUEST Parameters --debug --fastq_folder --raw_path (optional) --processed_path (optional) --trim_only (optional) --genome_sheet (optional) --out (optional) --subsample_read_count (optional) Output Data storage Backup Poor quality data Cleanup The trim-fq-nf workflow performs FASTQ trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. You should not use the trim-fq-nf workflow on low-coverage NIL or RIL data. Recent updates in 2021 also include running a species check on the FASTQ and generating a sample sheet of high-quality, species-confirmed samples for alignment . Pipeline overview \u00b6 ____ .__ _____ _____ _/ |_ _______ |__| _____ _/ ____\\\\ ______ ____ _/ ____\\\\ \\\\ __\\\\\\\\_ __ \\\\| | / \\\\ ______ \\\\ __\\\\ / ____/ ______ / \\\\ \\\\ __\\\\ | | | | \\\\/| || Y Y \\\\ /_____/ | | < <_| | /_____/ | | \\\\ | | |__| |__| |__||__|_| / |__| \\\\__ | |___| / |__| \\\\/ |__| \\\\/ parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode ${params.debug} --fastq_folder Name of the raw fastq folder ${params.fastq_folder} --raw_path Path to raw fastq folder ${params.raw_path} --processed_path Path to processed fastq folder (output) ${params.processed_path} --trim_only Whether to skip species check and only trim ${params.trim_only} --genome_sheet File with fasta locations for species check ${params.genome_sheet} --out Folder name to write results ${params.out} --subsample_read_count How many reads to use for species check ${params.subsample_read_count} You have downloaded FASTQ Data to a subdirectory within a raw directory. For wild isolates this will be /projects/b1059/data/transfer/<folder_name> FASTQs must end in a .fq.gz extension for the pipeline to work. You have modified FASTQ names if necessary to add strain names or other identifying information. You have installed software-requirements (see below for more info) Software requirements \u00b6 Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Currently only runs on Quest with conda environments installed at /projects/b1059/software/conda_envs/ Note All FASTQs should end with a _R1_001.fastq.gz or a _R2_001.fastq.gz . You can rename FASTQs using the rename command: rename --dry-run --subst .fq.gz .fastq.gz --subst _1 _R1_001 --subst _2 _R2_001 *.fq.gz The --dry-run flag will output how files will be renamed. Review the output and remove the flag when you are ready. Usage \u00b6 Testing the pipeline on QUEST \u00b6 This command uses a test dataset nextflow run main.nf --debug Running the pipeline on QUEST \u00b6 nextflow run main.nf --fastq_folder <name_of_folder> Important The pipeline expects the folder containing raw fastq files to be located at /projects/b1059/data/transfer/raw/ . And all processed fastq files will be output to /projects/b1059/data/transfer/processed/ Parameters \u00b6 --debug \u00b6 You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data/raw folder). For example: nextflow run main.nf --debug -resume Using --debug will automatically set the fastq_folder to test_data/raw/20210406_test1 --fastq_folder \u00b6 This should be the name of the folder containing all fastq files located at /projects/b1059/data/transfer/raw/ . As long as there are no overlapping file names (be sure to check this first), you can combine multiple pools sequenced at the same time into one larger folder at this step. --raw_path (optional) \u00b6 The path to the fastq_folder if not default ( /projects/b1059/data/transfer/raw/ ) --processed_path (optional) \u00b6 The path to output folder if not default ( /projects/b1059/data/transfer/processed/ ) --trim_only (optional) \u00b6 Boolean option to only trim fastq files and not perform any species check or further analysis. Default = FALSE --genome_sheet (optional) \u00b6 Path to a tsv file listing project IDs for species. Default is located in bin/genome_sheet.tsv --out (optional) \u00b6 Name of output folder with results. Default is \"processFQ-{fastq_folder}\" --subsample_read_count (optional) \u00b6 How many reads to use for species check. Default = 10,000 Output \u00b6 \u251c\u2500\u2500 b1059/data/transfer/processed/ \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}.fq.gz - - - - - - - - - - - - - - - - - - - - - - - - - - - - \u251c\u2500\u2500 multi_QC \u2502 \u251c\u2500\u2500 multiqc_data \u2502 \u2502 \u251c\u2500\u2500 *.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_data.json \u2502 \u251c\u2500\u2500 multiqc_report.html \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}_fastp.html \u251c\u2500\u2500 multiqc_data \u2502 \u2514\u2500\u2500 multiqc_samtools_stats.txt \u251c\u2500\u2500 sample_sheet \u2502 \u251c\u2500\u2500 sample_sheet_{species}_{date}_ALL.tsv \u2502 \u2514\u2500\u2500 sample_sheet_{species}_{date}_NEW.tsv \u251c\u2500\u2500 species_check \u2502 \u251c\u2500\u2500 species_check_{fastq_folder}.html \u2502 \u251c\u2500\u2500 {library}_multiple_librarires.tsv \u2502 \u251c\u2500\u2500 {library}_strains_most_likely_species.tsv \u2502 \u251c\u2500\u2500 {library}_strains_not_in_master_sheet.tsv \u2502 \u251c\u2500\u2500 {library}_strains_possibly_diff_species.tsv \u2502 \u2514\u2500\u2500 WI_all_{date}.tsv \u251c\u2500\u2500 sample_sheet_{fastq_folder}_all_temp.tsv \u2514\u2500\u2500 log.txt The resulting trimmed FASTQs will be output in the b1059/data/transfer/processed directory. The rest of the output files and reports will be generated in a new folder in the directory in which you ran the nextflow pipeline, labeled by processFQ-{fastq_folder} . MultiQC multi_QC/{strain}_{library}_fastp.html - fastp html report detailing trimming and quality multi_QC/multiqc_report.html - aggregate multi QC report for all strains pre and post trimming multi_QC/multiqc_data/*.txt or .json - files used to make previous reports Species check If a species check is run, the multiqc_data/multiqc_samtools_stats.txt will contain the results of reads mapped to each species. Furthermore, several reports and sample sheets will be generated: species_check/species_check_{date}_{library}.html is an HTML report showing how many strains have issues (not in master sheet, possibly different species, etc.) species_check/{library}_multiple_libraries.tsv - strains sequenced in multiple libraries species_check/{library}_strains_most_likely_species.tsv - list of all strains in library labeled by (1) the species in record and (2) most likely species by sequencing species_check/{library}_strains_not_in_master.tsv - list of strains not found in Robyn's wild isolate master sheets for CE, CB, or CT species_check/{library}_strains_possibly_diff_species.tsv - list of strains whose species in record does not match the likely species by sequencing species_check/WI_all_{date}.tsv - copy of all strains (CE, CB, and CT) and species designation in record sample_sheet_{date}_{library}_all_temp.tsv - temporary sample sheet with all strains for all species combined. DO NOT USE THIS FOR ALIGNMENT. Sample sheets If a species check is run, the species_check/sample_sheet folder will also contain 6 sample sheets to be used for alignment: sample_sheet/sample_sheet_{species}_{date}_ALL.tsv - sample sheet for alignment-nf using ALL strains of a particular species (i.e. c_elegans). This is useful for species we have not performed any alignments for or when we update the reference genome and need to re-align all strains. sample_sheet/sample_sheet_{species}_{date}_NEW.tsv - sample sheet for alignment-nf using all fastq from any library for ONLY strains sequenced in this particular library of a particular species (i.e. c_elegans, RET63). This is useful when the reference genome does not change and there is no need to re-align thousands of strains to save on computational power. Note The \"new\" sample sheet will still contain old fastq sequenced in a previous pool (i.e. RET55) if that strain was re-sequenced in the current pool (i.e. RET63). After running alignment-nf , this will create a new BAM file incorporating all fastq for that strain. Data storage \u00b6 Backup \u00b6 Once you have completed the trim-fq-nf pipeline you should backup the raw FASTQs. More information on this is available in the backup Poor quality data \u00b6 If you observe poor quality sequence data you should notify Robyn through the appropriate channels and then remove the data from further analysis. Cleanup \u00b6 If you have triple-checked everything and are satisfied with the results, the original raw sequence data can be deleted. The processed sequence data (FASTQ files) should be moved to their appropriate location, split by species ( /projects/b1059/data/{species}/WI/fastq/dna/ ). The following line can be used to move processed fastq prior to running alignment-nf : # change directories into the folder containing the processed fastq files cd /projects/b1059/data/transfer/processed/20210510_RET63/ # move files one species at a time (might be a more efficient line of code for this, but it works...) # !!!! make sure to change the file name !!!!! # file name ~ - ~ CHANGE THIS ~ - ~ file='/projects/b1059/Katie/trim-fq-nf/20210510_RET63/species_check/sample_sheet/sample_sheet_c_tropicalis_20201222a_NEW.tsv' # species sp=\"c_`echo $file | xargs -n1 basename | awk -F[__] '{print $4}'`\" # get list of files to move from file awk NR\\>1 $file > temp.tsv cat temp.tsv | awk '{print $4}' > files_to_move.txt cat temp.tsv | awk '{print $5}' >> files_to_move.txt # move files cat files_to_move.txt | while read line do mv $line /projects/b1059/data/$sp/WI/fastq/dna/ done # remove temp file rm files_to_move.txt rm temp.tsv Note The sample sheets ONLY contain strains that species in record matches most likely species by sequencing. If, after moving all the FASTQ for each species to their proper folder, you have FASTQ remaining, these are likely to be found in strains_possibly_diff_species.tsv . You should notify Robyn and Erik about these strains through the appropriate channels and delete the FASTQ or move to another temporary location until it can be re-sequenced.","title":"trim-fq-nf"},{"location":"pipeline-trimming/#trim-fq-nf","text":"trim-fq-nf Pipeline overview Software requirements Usage Testing the pipeline on QUEST Running the pipeline on QUEST Parameters --debug --fastq_folder --raw_path (optional) --processed_path (optional) --trim_only (optional) --genome_sheet (optional) --out (optional) --subsample_read_count (optional) Output Data storage Backup Poor quality data Cleanup The trim-fq-nf workflow performs FASTQ trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. You should not use the trim-fq-nf workflow on low-coverage NIL or RIL data. Recent updates in 2021 also include running a species check on the FASTQ and generating a sample sheet of high-quality, species-confirmed samples for alignment .","title":"trim-fq-nf"},{"location":"pipeline-trimming/#pipeline_overview","text":"____ .__ _____ _____ _/ |_ _______ |__| _____ _/ ____\\\\ ______ ____ _/ ____\\\\ \\\\ __\\\\\\\\_ __ \\\\| | / \\\\ ______ \\\\ __\\\\ / ____/ ______ / \\\\ \\\\ __\\\\ | | | | \\\\/| || Y Y \\\\ /_____/ | | < <_| | /_____/ | | \\\\ | | |__| |__| |__||__|_| / |__| \\\\__ | |___| / |__| \\\\/ |__| \\\\/ parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode ${params.debug} --fastq_folder Name of the raw fastq folder ${params.fastq_folder} --raw_path Path to raw fastq folder ${params.raw_path} --processed_path Path to processed fastq folder (output) ${params.processed_path} --trim_only Whether to skip species check and only trim ${params.trim_only} --genome_sheet File with fasta locations for species check ${params.genome_sheet} --out Folder name to write results ${params.out} --subsample_read_count How many reads to use for species check ${params.subsample_read_count} You have downloaded FASTQ Data to a subdirectory within a raw directory. For wild isolates this will be /projects/b1059/data/transfer/<folder_name> FASTQs must end in a .fq.gz extension for the pipeline to work. You have modified FASTQ names if necessary to add strain names or other identifying information. You have installed software-requirements (see below for more info)","title":"Pipeline overview"},{"location":"pipeline-trimming/#software_requirements","text":"Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Currently only runs on Quest with conda environments installed at /projects/b1059/software/conda_envs/ Note All FASTQs should end with a _R1_001.fastq.gz or a _R2_001.fastq.gz . You can rename FASTQs using the rename command: rename --dry-run --subst .fq.gz .fastq.gz --subst _1 _R1_001 --subst _2 _R2_001 *.fq.gz The --dry-run flag will output how files will be renamed. Review the output and remove the flag when you are ready.","title":"Software requirements"},{"location":"pipeline-trimming/#usage","text":"","title":"Usage"},{"location":"pipeline-trimming/#testing_the_pipeline_on_quest","text":"This command uses a test dataset nextflow run main.nf --debug","title":"Testing the pipeline on QUEST"},{"location":"pipeline-trimming/#running_the_pipeline_on_quest","text":"nextflow run main.nf --fastq_folder <name_of_folder> Important The pipeline expects the folder containing raw fastq files to be located at /projects/b1059/data/transfer/raw/ . And all processed fastq files will be output to /projects/b1059/data/transfer/processed/","title":"Running the pipeline on QUEST"},{"location":"pipeline-trimming/#parameters","text":"","title":"Parameters"},{"location":"pipeline-trimming/#--debug","text":"You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data/raw folder). For example: nextflow run main.nf --debug -resume Using --debug will automatically set the fastq_folder to test_data/raw/20210406_test1","title":"--debug"},{"location":"pipeline-trimming/#--fastq_folder","text":"This should be the name of the folder containing all fastq files located at /projects/b1059/data/transfer/raw/ . As long as there are no overlapping file names (be sure to check this first), you can combine multiple pools sequenced at the same time into one larger folder at this step.","title":"--fastq_folder"},{"location":"pipeline-trimming/#--raw_path_optional","text":"The path to the fastq_folder if not default ( /projects/b1059/data/transfer/raw/ )","title":"--raw_path (optional)"},{"location":"pipeline-trimming/#--processed_path_optional","text":"The path to output folder if not default ( /projects/b1059/data/transfer/processed/ )","title":"--processed_path (optional)"},{"location":"pipeline-trimming/#--trim_only_optional","text":"Boolean option to only trim fastq files and not perform any species check or further analysis. Default = FALSE","title":"--trim_only (optional)"},{"location":"pipeline-trimming/#--genome_sheet_optional","text":"Path to a tsv file listing project IDs for species. Default is located in bin/genome_sheet.tsv","title":"--genome_sheet (optional)"},{"location":"pipeline-trimming/#--out_optional","text":"Name of output folder with results. Default is \"processFQ-{fastq_folder}\"","title":"--out (optional)"},{"location":"pipeline-trimming/#--subsample_read_count_optional","text":"How many reads to use for species check. Default = 10,000","title":"--subsample_read_count (optional)"},{"location":"pipeline-trimming/#output","text":"\u251c\u2500\u2500 b1059/data/transfer/processed/ \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}.fq.gz - - - - - - - - - - - - - - - - - - - - - - - - - - - - \u251c\u2500\u2500 multi_QC \u2502 \u251c\u2500\u2500 multiqc_data \u2502 \u2502 \u251c\u2500\u2500 *.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_data.json \u2502 \u251c\u2500\u2500 multiqc_report.html \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}_fastp.html \u251c\u2500\u2500 multiqc_data \u2502 \u2514\u2500\u2500 multiqc_samtools_stats.txt \u251c\u2500\u2500 sample_sheet \u2502 \u251c\u2500\u2500 sample_sheet_{species}_{date}_ALL.tsv \u2502 \u2514\u2500\u2500 sample_sheet_{species}_{date}_NEW.tsv \u251c\u2500\u2500 species_check \u2502 \u251c\u2500\u2500 species_check_{fastq_folder}.html \u2502 \u251c\u2500\u2500 {library}_multiple_librarires.tsv \u2502 \u251c\u2500\u2500 {library}_strains_most_likely_species.tsv \u2502 \u251c\u2500\u2500 {library}_strains_not_in_master_sheet.tsv \u2502 \u251c\u2500\u2500 {library}_strains_possibly_diff_species.tsv \u2502 \u2514\u2500\u2500 WI_all_{date}.tsv \u251c\u2500\u2500 sample_sheet_{fastq_folder}_all_temp.tsv \u2514\u2500\u2500 log.txt The resulting trimmed FASTQs will be output in the b1059/data/transfer/processed directory. The rest of the output files and reports will be generated in a new folder in the directory in which you ran the nextflow pipeline, labeled by processFQ-{fastq_folder} . MultiQC multi_QC/{strain}_{library}_fastp.html - fastp html report detailing trimming and quality multi_QC/multiqc_report.html - aggregate multi QC report for all strains pre and post trimming multi_QC/multiqc_data/*.txt or .json - files used to make previous reports Species check If a species check is run, the multiqc_data/multiqc_samtools_stats.txt will contain the results of reads mapped to each species. Furthermore, several reports and sample sheets will be generated: species_check/species_check_{date}_{library}.html is an HTML report showing how many strains have issues (not in master sheet, possibly different species, etc.) species_check/{library}_multiple_libraries.tsv - strains sequenced in multiple libraries species_check/{library}_strains_most_likely_species.tsv - list of all strains in library labeled by (1) the species in record and (2) most likely species by sequencing species_check/{library}_strains_not_in_master.tsv - list of strains not found in Robyn's wild isolate master sheets for CE, CB, or CT species_check/{library}_strains_possibly_diff_species.tsv - list of strains whose species in record does not match the likely species by sequencing species_check/WI_all_{date}.tsv - copy of all strains (CE, CB, and CT) and species designation in record sample_sheet_{date}_{library}_all_temp.tsv - temporary sample sheet with all strains for all species combined. DO NOT USE THIS FOR ALIGNMENT. Sample sheets If a species check is run, the species_check/sample_sheet folder will also contain 6 sample sheets to be used for alignment: sample_sheet/sample_sheet_{species}_{date}_ALL.tsv - sample sheet for alignment-nf using ALL strains of a particular species (i.e. c_elegans). This is useful for species we have not performed any alignments for or when we update the reference genome and need to re-align all strains. sample_sheet/sample_sheet_{species}_{date}_NEW.tsv - sample sheet for alignment-nf using all fastq from any library for ONLY strains sequenced in this particular library of a particular species (i.e. c_elegans, RET63). This is useful when the reference genome does not change and there is no need to re-align thousands of strains to save on computational power. Note The \"new\" sample sheet will still contain old fastq sequenced in a previous pool (i.e. RET55) if that strain was re-sequenced in the current pool (i.e. RET63). After running alignment-nf , this will create a new BAM file incorporating all fastq for that strain.","title":"Output"},{"location":"pipeline-trimming/#data_storage","text":"","title":"Data storage"},{"location":"pipeline-trimming/#backup","text":"Once you have completed the trim-fq-nf pipeline you should backup the raw FASTQs. More information on this is available in the backup","title":"Backup"},{"location":"pipeline-trimming/#poor_quality_data","text":"If you observe poor quality sequence data you should notify Robyn through the appropriate channels and then remove the data from further analysis.","title":"Poor quality data"},{"location":"pipeline-trimming/#cleanup","text":"If you have triple-checked everything and are satisfied with the results, the original raw sequence data can be deleted. The processed sequence data (FASTQ files) should be moved to their appropriate location, split by species ( /projects/b1059/data/{species}/WI/fastq/dna/ ). The following line can be used to move processed fastq prior to running alignment-nf : # change directories into the folder containing the processed fastq files cd /projects/b1059/data/transfer/processed/20210510_RET63/ # move files one species at a time (might be a more efficient line of code for this, but it works...) # !!!! make sure to change the file name !!!!! # file name ~ - ~ CHANGE THIS ~ - ~ file='/projects/b1059/Katie/trim-fq-nf/20210510_RET63/species_check/sample_sheet/sample_sheet_c_tropicalis_20201222a_NEW.tsv' # species sp=\"c_`echo $file | xargs -n1 basename | awk -F[__] '{print $4}'`\" # get list of files to move from file awk NR\\>1 $file > temp.tsv cat temp.tsv | awk '{print $4}' > files_to_move.txt cat temp.tsv | awk '{print $5}' >> files_to_move.txt # move files cat files_to_move.txt | while read line do mv $line /projects/b1059/data/$sp/WI/fastq/dna/ done # remove temp file rm files_to_move.txt rm temp.tsv Note The sample sheets ONLY contain strains that species in record matches most likely species by sequencing. If, after moving all the FASTQ for each species to their proper folder, you have FASTQ remaining, these are likely to be found in strains_possibly_diff_species.tsv . You should notify Robyn and Erik about these strains through the appropriate channels and delete the FASTQ or move to another temporary location until it can be re-sequenced.","title":"Cleanup"},{"location":"pipeline-wiGATK/","text":"wi-gatk \u00b6 wi-gatk Pipeline Overview Software Requirements Usage Profiles Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Parameters --sample_sheet --bam_location (optional) --species (optional) --project (optional) --ws_build (optional) --reference (optional) --mito_name (optional) --output (optional) Output Data Storage Cleanup The wi-gatk pipeline filters and calls variants from wild isolate sequence data. Pipeline Overview \u00b6 _______ _______ _______ __ __ _______ _______ | __| _ |_ _| |/ | | | | ___| | | | | | | | < | | ___| |_______|___|___| |___| |__|\\\\__| |__|____|___| parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --output Release Directory WI-{date} --sample_sheet Sample sheet null --bam_location Directory of bam files /projects/b1059/data/{species}/WI/alignments/ --mito_name Contig not to polarize hetero sites MtDNA Reference Genome --------------- --reference_base Location of ref genomes /projects/b1059/data/{species}/genomes/ --species/project/build These 4 params form --reference {species} / {project} / {ws_build} Variant Filters --------------- --min_depth Minimum variant depth 5 --qual Variant QUAL score 30 --strand_odds_ratio SOR_strand_odds_ratio 5 --quality_by_depth QD_quality_by_depth 20 --fisherstrand FS_fisher_strand 100 --high_missing Max % missing genotypes 0.95 --high_heterozygosity Max % max heterozygosity 0.10 Software Requirements \u00b6 The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update This pipeline also uses a docker image andersenlab/gatk4 to manage software and reproducibility. To access this docker image, first load the singularity module on QUEST. module load singularity Usage \u00b6 Profiles \u00b6 The nextflow.config file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. Note If you forget to add a -profile , the quest profile will be chosen as default Running the pipeline locally \u00b6 When running locally, the pipeline will run using the andersenlab/gatk4 docker image. You must have docker installed. nextflow run main.nf -profile local -resume Debugging the pipeline on Quest \u00b6 When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume Running the pipeline on Quest \u00b6 The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest --sample_sheet <path_to_sheet> -resume Parameters \u00b6 Most configuration is handled using the -profile flag and nextflow.config ; If you want to fine tune things you can use the options below. --sample_sheet \u00b6 The sample sheet is the output of 5.low_map_cov_for_seq_sheet.Rmd after running alignment-nf (soon to be integrated into alignment-nf). The sample sheet contains 5 columns as detailed below: strain bam bai coverage percent_mapped AB1 AB1.bam AB1.bam.bai 64 99.4 AB4 AB4.bam AB4.bam.bai 52 99.2 BRC20067 BRC20067.bam BRC20067.bam.bai 30 92.5 Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced. --bam_location (optional) \u00b6 Path to directory holding all the alignment files for strains in the analysis. Defaults to /projects/b1059/data/{species}/WI/alignments/ Important Remember to move your bam files output from alignment-nf to this location prior to running wi-gatk . In most cases, you will want to run wi-gatk on all samples, new and old combined. --species (optional) \u00b6 default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis --project (optional) \u00b6 default = PRJNA13758 WormBase project ID for selected species. Choose from some examples here --ws_build (optional) \u00b6 default = WS276 WormBase version to use for reference genome. --reference (optional) \u00b6 A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz Note If running on QUEST, instead of changing the reference parameter, opt to change the species , project , and ws_build for other species like c_briggsae (and then the reference will change automatically) --mito_name (optional) \u00b6 Name of contig to skip het polarization. Might need to change for other species besides c_elegans if the mitochondria contig is named differently. Defaults to MtDNA . --output (optional) \u00b6 A directory in which to output results. By default it will be WI-YYYYMMDD where YYYYMMDD is todays date. Output \u00b6 The final output directory looks like this: \u251c\u2500\u2500 variation \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.hard-filter.stats.txt \u2502 \u251c\u2500\u2500 *.hard-filter.filter_stats.txt \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.soft-filter.stats.txt \u2502 \u2514\u2500\u2500 *.soft-filter.filter_stats.txt \u2514\u2500\u2500 report \u251c\u2500\u2500 multiqc.html \u2514\u2500\u2500 multiqc_data \u2514\u2500\u2500 multiqc_*.json Data Storage \u00b6 Cleanup \u00b6 The hard-filter.vcf is the input for both the concordance-nf pipeline and the post-gatk-nf pipeline. Once both pipelines have been completed successfully, the hard and soft filter vcf and index files (everything output in the variation folder) can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf .","title":"wi-gatk"},{"location":"pipeline-wiGATK/#wi-gatk","text":"wi-gatk Pipeline Overview Software Requirements Usage Profiles Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Parameters --sample_sheet --bam_location (optional) --species (optional) --project (optional) --ws_build (optional) --reference (optional) --mito_name (optional) --output (optional) Output Data Storage Cleanup The wi-gatk pipeline filters and calls variants from wild isolate sequence data.","title":"wi-gatk"},{"location":"pipeline-wiGATK/#pipeline_overview","text":"_______ _______ _______ __ __ _______ _______ | __| _ |_ _| |/ | | | | ___| | | | | | | | < | | ___| |_______|___|___| |___| |__|\\\\__| |__|____|___| parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --output Release Directory WI-{date} --sample_sheet Sample sheet null --bam_location Directory of bam files /projects/b1059/data/{species}/WI/alignments/ --mito_name Contig not to polarize hetero sites MtDNA Reference Genome --------------- --reference_base Location of ref genomes /projects/b1059/data/{species}/genomes/ --species/project/build These 4 params form --reference {species} / {project} / {ws_build} Variant Filters --------------- --min_depth Minimum variant depth 5 --qual Variant QUAL score 30 --strand_odds_ratio SOR_strand_odds_ratio 5 --quality_by_depth QD_quality_by_depth 20 --fisherstrand FS_fisher_strand 100 --high_missing Max % missing genotypes 0.95 --high_heterozygosity Max % max heterozygosity 0.10","title":"Pipeline Overview"},{"location":"pipeline-wiGATK/#software_requirements","text":"The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update This pipeline also uses a docker image andersenlab/gatk4 to manage software and reproducibility. To access this docker image, first load the singularity module on QUEST. module load singularity","title":"Software Requirements"},{"location":"pipeline-wiGATK/#usage","text":"","title":"Usage"},{"location":"pipeline-wiGATK/#profiles","text":"The nextflow.config file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. Note If you forget to add a -profile , the quest profile will be chosen as default","title":"Profiles"},{"location":"pipeline-wiGATK/#running_the_pipeline_locally","text":"When running locally, the pipeline will run using the andersenlab/gatk4 docker image. You must have docker installed. nextflow run main.nf -profile local -resume","title":"Running the pipeline locally"},{"location":"pipeline-wiGATK/#debugging_the_pipeline_on_quest","text":"When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run main.nf -profile quest_debug -resume","title":"Debugging the pipeline on Quest"},{"location":"pipeline-wiGATK/#running_the_pipeline_on_quest","text":"The pipeline can be run on Quest using the following command: nextflow run main.nf -profile quest --sample_sheet <path_to_sheet> -resume","title":"Running the pipeline on Quest"},{"location":"pipeline-wiGATK/#parameters","text":"Most configuration is handled using the -profile flag and nextflow.config ; If you want to fine tune things you can use the options below.","title":"Parameters"},{"location":"pipeline-wiGATK/#--sample_sheet","text":"The sample sheet is the output of 5.low_map_cov_for_seq_sheet.Rmd after running alignment-nf (soon to be integrated into alignment-nf). The sample sheet contains 5 columns as detailed below: strain bam bai coverage percent_mapped AB1 AB1.bam AB1.bam.bai 64 99.4 AB4 AB4.bam AB4.bam.bai 52 99.2 BRC20067 BRC20067.bam BRC20067.bam.bai 30 92.5 Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced.","title":"--sample_sheet"},{"location":"pipeline-wiGATK/#--bam_location_optional","text":"Path to directory holding all the alignment files for strains in the analysis. Defaults to /projects/b1059/data/{species}/WI/alignments/ Important Remember to move your bam files output from alignment-nf to this location prior to running wi-gatk . In most cases, you will want to run wi-gatk on all samples, new and old combined.","title":"--bam_location (optional)"},{"location":"pipeline-wiGATK/#--species_optional","text":"default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis","title":"--species (optional)"},{"location":"pipeline-wiGATK/#--project_optional","text":"default = PRJNA13758 WormBase project ID for selected species. Choose from some examples here","title":"--project (optional)"},{"location":"pipeline-wiGATK/#--ws_build_optional","text":"default = WS276 WormBase version to use for reference genome.","title":"--ws_build (optional)"},{"location":"pipeline-wiGATK/#--reference_optional","text":"A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz Note If running on QUEST, instead of changing the reference parameter, opt to change the species , project , and ws_build for other species like c_briggsae (and then the reference will change automatically)","title":"--reference (optional)"},{"location":"pipeline-wiGATK/#--mito_name_optional","text":"Name of contig to skip het polarization. Might need to change for other species besides c_elegans if the mitochondria contig is named differently. Defaults to MtDNA .","title":"--mito_name (optional)"},{"location":"pipeline-wiGATK/#--output_optional","text":"A directory in which to output results. By default it will be WI-YYYYMMDD where YYYYMMDD is todays date.","title":"--output (optional)"},{"location":"pipeline-wiGATK/#output","text":"The final output directory looks like this: \u251c\u2500\u2500 variation \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.hard-filter.stats.txt \u2502 \u251c\u2500\u2500 *.hard-filter.filter_stats.txt \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.soft-filter.stats.txt \u2502 \u2514\u2500\u2500 *.soft-filter.filter_stats.txt \u2514\u2500\u2500 report \u251c\u2500\u2500 multiqc.html \u2514\u2500\u2500 multiqc_data \u2514\u2500\u2500 multiqc_*.json","title":"Output"},{"location":"pipeline-wiGATK/#data_storage","text":"","title":"Data Storage"},{"location":"pipeline-wiGATK/#cleanup","text":"The hard-filter.vcf is the input for both the concordance-nf pipeline and the post-gatk-nf pipeline. Once both pipelines have been completed successfully, the hard and soft filter vcf and index files (everything output in the variation folder) can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf .","title":"Cleanup"},{"location":"quest-conda/","text":"Using conda on Quest \u00b6 Using conda on Quest Why Conda Setting up Conda on Quest Using Conda Running Nextflow with conda Notes on conda versions on Quest Why Conda \u00b6 Computational Reproducibility is the ability to reproduce an analysis exactly. In order for computational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub (for help, see the Github page ). Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software, package and environment managers such as Conda and Docker are very useful. Conda works similarly to brew or pyenv pyenv that were used in the legacy Andersen-Lab-Env. Note The software environments on Mac and Linux are not exactly identical...but they are very close. Setting up Conda on Quest \u00b6 Anaconda is a Python distribution that contains many packages including conda. Miniconda is a more compact version of Anaconda that also includes conda. So in order to install conda, we usually either install Miniconda or Anaconda. On Quest, Anaconda is already installed. However there are many versions of Anaconda, each can have a different version of Python and Conda. The current lab environments mainly used module load python/anaconda (See Notes below for more info). In your home directory ~/ , create a file called .condarc and put the following lines into it. It sets the channel priority when conda searches for packages. If possible, in one environment it is good to use packages from the same channel. channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /projects/b1059/software/conda_envs/ Using Conda \u00b6 Conda Documentation Conda Cheatsheet After loading the Anaconda module, one can create an environment and install packages into that environment: # Create conda environment named \"name_of_env\" conda create --name name_of_env # Create conda environment in a specific folder conda create -p /projects/b1059/software/conda_envs/test # activate conda environment # for some `conda activate` works and for others `source activate` conda activate test source activate test # install package into conda environment conda install bcftools When looking to install a package, one resource to check out is anaconda.org , search for your package and run the install command listed on the page. Note Remember to keep in mind the different versions of software/packages. You could have bcftools-v1.10 or bcftools-v.1.12, so make sure you install the correct one! Important You can also install R packages with conda, however conda and R don't always work well together. Check out our quick fix here Running Nextflow with conda \u00b6 When running Nextflow, conda environments can be specified as part of a process or in the nextflow.config file to apply to the entire pipeline (check out the documentation ): Conda within a process: process foo { conda '/projects/b1059/software/conda_envs/cegwas2-nf_env' ''' your_command --here ''' } Conda for the entire pipeline: // in the nextflow.config file: conda { conda.enabled = true conda.cacheDir = \".env\" } process { conda = \"/projects/b1059/software/conda_envs/cegwas2-nf_env\" } Important When running Nextflow with a docker container on QUEST, it is necessary to replace the docker command with singularity (although you still must build a docker container). You must also load singularity using module load singularity before starting a run. Notes on conda versions on Quest \u00b6 tl;dr; If having trouble with conda, or Nextflow gives conda-related errors, try to load a different version of anaconda on Quest. At some point it may be worth re-creating all conda environments in the lab with a consistent version of conda. As of the end of 2020, existing conda environments for the lab were mostly created by module load python/anaconda (which got automatically loaded with module git by accident). It loads Python version 2.7.18 and conda 4.5.2. The other environments were created with module load python/anaconda3.6 which loads Python 3.6.0 and conda 4.3.30. To see versions, use conda info or conda -V . Once you activate an environment with conda activate env_name or source activate env_name , the default conda usually get re-directed to the conda that were originally used to create the environment. This is good because it helps ensure that all packages in the same environment uses the same version of conda. One can go to cd ~/.conda/env_name/bin and readlink -f conda or readlink -f activate to see which version of conda is used by this environment. This is exactly how Nextflow determins which conda to use when using an existing conda environment. Quest people recommended module load python-anaconda3 , but that version does not mix well with the versions mentioned above. But if one were to re-install all environments we have, this is probably the version to stick to.","title":"Conda"},{"location":"quest-conda/#using_conda_on_quest","text":"Using conda on Quest Why Conda Setting up Conda on Quest Using Conda Running Nextflow with conda Notes on conda versions on Quest","title":"Using conda on Quest"},{"location":"quest-conda/#why_conda","text":"Computational Reproducibility is the ability to reproduce an analysis exactly. In order for computational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub (for help, see the Github page ). Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software, package and environment managers such as Conda and Docker are very useful. Conda works similarly to brew or pyenv pyenv that were used in the legacy Andersen-Lab-Env. Note The software environments on Mac and Linux are not exactly identical...but they are very close.","title":"Why Conda"},{"location":"quest-conda/#setting_up_conda_on_quest","text":"Anaconda is a Python distribution that contains many packages including conda. Miniconda is a more compact version of Anaconda that also includes conda. So in order to install conda, we usually either install Miniconda or Anaconda. On Quest, Anaconda is already installed. However there are many versions of Anaconda, each can have a different version of Python and Conda. The current lab environments mainly used module load python/anaconda (See Notes below for more info). In your home directory ~/ , create a file called .condarc and put the following lines into it. It sets the channel priority when conda searches for packages. If possible, in one environment it is good to use packages from the same channel. channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /projects/b1059/software/conda_envs/","title":"Setting up Conda on Quest"},{"location":"quest-conda/#using_conda","text":"Conda Documentation Conda Cheatsheet After loading the Anaconda module, one can create an environment and install packages into that environment: # Create conda environment named \"name_of_env\" conda create --name name_of_env # Create conda environment in a specific folder conda create -p /projects/b1059/software/conda_envs/test # activate conda environment # for some `conda activate` works and for others `source activate` conda activate test source activate test # install package into conda environment conda install bcftools When looking to install a package, one resource to check out is anaconda.org , search for your package and run the install command listed on the page. Note Remember to keep in mind the different versions of software/packages. You could have bcftools-v1.10 or bcftools-v.1.12, so make sure you install the correct one! Important You can also install R packages with conda, however conda and R don't always work well together. Check out our quick fix here","title":"Using Conda"},{"location":"quest-conda/#running_nextflow_with_conda","text":"When running Nextflow, conda environments can be specified as part of a process or in the nextflow.config file to apply to the entire pipeline (check out the documentation ): Conda within a process: process foo { conda '/projects/b1059/software/conda_envs/cegwas2-nf_env' ''' your_command --here ''' } Conda for the entire pipeline: // in the nextflow.config file: conda { conda.enabled = true conda.cacheDir = \".env\" } process { conda = \"/projects/b1059/software/conda_envs/cegwas2-nf_env\" } Important When running Nextflow with a docker container on QUEST, it is necessary to replace the docker command with singularity (although you still must build a docker container). You must also load singularity using module load singularity before starting a run.","title":"Running Nextflow with conda"},{"location":"quest-conda/#notes_on_conda_versions_on_quest","text":"tl;dr; If having trouble with conda, or Nextflow gives conda-related errors, try to load a different version of anaconda on Quest. At some point it may be worth re-creating all conda environments in the lab with a consistent version of conda. As of the end of 2020, existing conda environments for the lab were mostly created by module load python/anaconda (which got automatically loaded with module git by accident). It loads Python version 2.7.18 and conda 4.5.2. The other environments were created with module load python/anaconda3.6 which loads Python 3.6.0 and conda 4.3.30. To see versions, use conda info or conda -V . Once you activate an environment with conda activate env_name or source activate env_name , the default conda usually get re-directed to the conda that were originally used to create the environment. This is good because it helps ensure that all packages in the same environment uses the same version of conda. One can go to cd ~/.conda/env_name/bin and readlink -f conda or readlink -f activate to see which version of conda is used by this environment. This is exactly how Nextflow determins which conda to use when using an existing conda environment. Quest people recommended module load python-anaconda3 , but that version does not mix well with the versions mentioned above. But if one were to re-install all environments we have, this is probably the version to stick to.","title":"Notes on conda versions on Quest"},{"location":"quest-intro/","text":"Introduction \u00b6 Introduction New Users Signing into Quest Login Nodes Home Directory Projects and Queues Running interactive jobs on Quest Using screen or nohup to keep jobs from timing out Using packages already installed on Quest Submitting jobs to Quest Monitoring SLURM Jobs on Quest The Andersen Lab makes use of Quest, the supercomputer at Northwestern. Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up: Quest Documentation Note New to the command line? Check out the quick and dirty bash intro or this introduction first! New Users \u00b6 To gain access to Quest: Register new user with your NetID here . Apply to be added to partition b1059 here . Allocation manager: Erik Andersen, erik.andersen@northwestern.edu Apply to be added to partition b1042 here . Allocation manager: Janna Nugent, janna.nugent@northwestern.edu Quest has its Slack channel: genomics-rcs.slack.com ( here ) and help email: quest-help@northwestern.edu for users to get help. Signing into Quest \u00b6 After you gain access to the cluster you can login using: ssh <netid>@quest.it.northwestern.edu To avoid typing in the password everytime, one can set up a ssh key . I recommend setting an alias in your ~/.bash_profile to make logging in quicker: alias quest=\"ssh <netid>@quest.it.northwestern.edu\" The above line makes it so you simply type quest and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below). Login Nodes \u00b6 There are four login nodes we use: quser21-24. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired ( e.g. ssh quser21 ). Warning When using screen to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those screen sessions. Home Directory \u00b6 Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 80 Gb. More information on quotas, storage, etc . You can check your storgae space using the following command: du -hs * More information is provided below to help install and use software. Projects and Queues \u00b6 Quest is broadly organized into projects (or partitions). Projects have associated with them storage, nodes, and users. The Andersen lab has access to two projects. b1042 - The 'Genomics' Project has 200 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at /projects/b1042/AndersenLab/ . By default, files are deleted after 30 days. One can submit jobs to --partition=genomicsguestA to use this partition, with a max job duration of 48hr. b1059 - The Andersen Lab Project. b1059 has 3 computing nodes qnode9031 - qnode9033 with 192Gb of RAM and 40 cores each, and has 60 Tb of storage. b1059 storage is located at: /projects/b1059/ . One can submit jobs to --partition=b1059 to use this partition, with no limit on job duration. Note Anyone who uses quest should build your own project folder under /projects/b1059/projects with your name. You should only write and revise files under your project folder. You can read/copy data from b1059 but don't write any data out of your project folder. Important It is important that we keep the 60 Tb of storage space on b1059 from filling up with extraneous or intermediate files. It is good practice to clean up old data/files and backup important data at least every few months. Running interactive jobs on Quest \u00b6 If you are running a few simple commands or want to experiment with files directly you can start an interactive session on Quest. The command below will give you access to a node where you can run your commands srun -A b1042 --partition=genomicsguestA -N 1 -n 24 --mem=64G --time=12:00:00 --pty bash -i Important Do not run commands for big data on quser21-24 . These are login nodes and are not meant for running heavy-load workflows. Using screen or nohup to keep jobs from timing out \u00b6 If you have ever tried to run a pipeline or script that takes a long time (think cegwas2-nf ), you know that if you close down your terminal or if your QUEST session logs out, it will cause your jobs to end prematurely. There are several ways to avoid this: screen Perhaps the most common way to deal with scripts that run for a long time is screen . For the most simple case use, type screen to open a new screen session and then run your script like normal. Below are some more intermediate commands for taking full advantage of screen : screen -S <some_descriptive_name> : Use this command to name your screen session. Especially useful if you have several scren sessions running and/or want to get back to this particular one later. Ctrl+a + Ctrl+d to detach from the current screen session exit to end the current screen session screen -ls lists the IDs of all screen sessions currently running screen -r <screen_id> : Use this command to resume a particular screen session. If you only have one session running you can simply use screen -r Important When using screen on QUEST, take note that screen sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh quser22 ). nohup Another way to avoid SIGHUP errors is to use nohup . nohup is very simple to use, simply type nohup before your normal command and that's it! nohup nextflow run main.nf Running a command with nohup will look a little different than usual because it will not print anything to the console. Instead, it prints all console outputs into a file in the current directory called nohup.out . You can redirect the output to another filename using: nohup nextflow run main.nf > cegwas_{date}_output.txt When you exit this window and open a new session, you can always look at the contents of the output file using cat nohup.out to see the progress. Note You can also run nohup in the background to continue using the same window for other processes by running nohup nextflow run main.nf & . Using packages already installed on Quest \u00b6 Quest has a collection of packages installed. You can run module avail to see what packages are currently available on Quest. You can use module load bcftools or module load bcftools/1.10.1 to load packages with the default or a specific version ( module add does the same thing). If you do echo $PATH before and after loading modules, you can see what module does is simply appending paths to the packages into your $PATH so the packages can be found in your environment. module purge will remove all loaded packages and can be helpful to try when troubleshooting. Note You can also load/install software packages into conda environments for use with a specific project/pipeline. Check out the conda tutorial here . Submitting jobs to Quest \u00b6 Jobs on Quest are managed by SLURM . While most of our pipelines are managed by Nextflow, it is useful to know how to submit jobs directly to SLURM. Below is a template to submit an array job to SLURM that will run 10 parallel jobs. One can run it with sbatch script.sh . If you dig into the Nextflow working directory, you can see Nextflow actually generates such scripts and submits them to SLURM on your behalf. Thanks, Nextflow! #!/bin/bash #SBATCH -J name # Name of job #SBATCH -A b1042 # Allocation #SBATCH -p genomicsguestA # Queue #SBATCH -t 24:00:00 # Walltime/duration of the job (hh:mm:ss) #SBATCH --cpus-per-task=1 # Number of cores (= processors = cpus) for each task #SBATCH --mem-per-cpu=3G # Memory per core needed for a job #SBATCH --array=0-9 # number of parallel jobs to run counting from 0. Make sure to change this to match total number of files. # make a list of all the files you want to process # this script will run a parallel process for each file in this list name_list=(*.bam) # take the nth ($SGE_TASK_ID-th) file in name_list # don't change this line Input=${name_list[$SLURM_ARRAY_TASK_ID]} # then do your operation on this file Output=`echo $Input | sed 's/.bam/.sam/'` Monitoring SLURM Jobs on Quest \u00b6 Here are some commmon commands used to interact with SLURM and check on job status. For more, check out this page . squeue -u <netid> to check all jobs of a user. scancel <jobid> to cancel a job. scancel {30000..32000} to cancel a range of jobs (job id between 30000 and 32000). sinfo | grep b1059 to check status of nodes. alloc means all cores on a node are completed engaged; mix means some cores on a node are engaged; idle means all cores on a node are available to accept jobs.","title":"Introduction"},{"location":"quest-intro/#introduction","text":"Introduction New Users Signing into Quest Login Nodes Home Directory Projects and Queues Running interactive jobs on Quest Using screen or nohup to keep jobs from timing out Using packages already installed on Quest Submitting jobs to Quest Monitoring SLURM Jobs on Quest The Andersen Lab makes use of Quest, the supercomputer at Northwestern. Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up: Quest Documentation Note New to the command line? Check out the quick and dirty bash intro or this introduction first!","title":"Introduction"},{"location":"quest-intro/#new_users","text":"To gain access to Quest: Register new user with your NetID here . Apply to be added to partition b1059 here . Allocation manager: Erik Andersen, erik.andersen@northwestern.edu Apply to be added to partition b1042 here . Allocation manager: Janna Nugent, janna.nugent@northwestern.edu Quest has its Slack channel: genomics-rcs.slack.com ( here ) and help email: quest-help@northwestern.edu for users to get help.","title":"New Users"},{"location":"quest-intro/#signing_into_quest","text":"After you gain access to the cluster you can login using: ssh <netid>@quest.it.northwestern.edu To avoid typing in the password everytime, one can set up a ssh key . I recommend setting an alias in your ~/.bash_profile to make logging in quicker: alias quest=\"ssh <netid>@quest.it.northwestern.edu\" The above line makes it so you simply type quest and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below).","title":"Signing into Quest"},{"location":"quest-intro/#login_nodes","text":"There are four login nodes we use: quser21-24. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired ( e.g. ssh quser21 ). Warning When using screen to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those screen sessions.","title":"Login Nodes"},{"location":"quest-intro/#home_directory","text":"Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 80 Gb. More information on quotas, storage, etc . You can check your storgae space using the following command: du -hs * More information is provided below to help install and use software.","title":"Home Directory"},{"location":"quest-intro/#projects_and_queues","text":"Quest is broadly organized into projects (or partitions). Projects have associated with them storage, nodes, and users. The Andersen lab has access to two projects. b1042 - The 'Genomics' Project has 200 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at /projects/b1042/AndersenLab/ . By default, files are deleted after 30 days. One can submit jobs to --partition=genomicsguestA to use this partition, with a max job duration of 48hr. b1059 - The Andersen Lab Project. b1059 has 3 computing nodes qnode9031 - qnode9033 with 192Gb of RAM and 40 cores each, and has 60 Tb of storage. b1059 storage is located at: /projects/b1059/ . One can submit jobs to --partition=b1059 to use this partition, with no limit on job duration. Note Anyone who uses quest should build your own project folder under /projects/b1059/projects with your name. You should only write and revise files under your project folder. You can read/copy data from b1059 but don't write any data out of your project folder. Important It is important that we keep the 60 Tb of storage space on b1059 from filling up with extraneous or intermediate files. It is good practice to clean up old data/files and backup important data at least every few months.","title":"Projects and Queues"},{"location":"quest-intro/#running_interactive_jobs_on_quest","text":"If you are running a few simple commands or want to experiment with files directly you can start an interactive session on Quest. The command below will give you access to a node where you can run your commands srun -A b1042 --partition=genomicsguestA -N 1 -n 24 --mem=64G --time=12:00:00 --pty bash -i Important Do not run commands for big data on quser21-24 . These are login nodes and are not meant for running heavy-load workflows.","title":"Running interactive jobs on Quest"},{"location":"quest-intro/#using_screen_or_nohup_to_keep_jobs_from_timing_out","text":"If you have ever tried to run a pipeline or script that takes a long time (think cegwas2-nf ), you know that if you close down your terminal or if your QUEST session logs out, it will cause your jobs to end prematurely. There are several ways to avoid this: screen Perhaps the most common way to deal with scripts that run for a long time is screen . For the most simple case use, type screen to open a new screen session and then run your script like normal. Below are some more intermediate commands for taking full advantage of screen : screen -S <some_descriptive_name> : Use this command to name your screen session. Especially useful if you have several scren sessions running and/or want to get back to this particular one later. Ctrl+a + Ctrl+d to detach from the current screen session exit to end the current screen session screen -ls lists the IDs of all screen sessions currently running screen -r <screen_id> : Use this command to resume a particular screen session. If you only have one session running you can simply use screen -r Important When using screen on QUEST, take note that screen sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh quser22 ). nohup Another way to avoid SIGHUP errors is to use nohup . nohup is very simple to use, simply type nohup before your normal command and that's it! nohup nextflow run main.nf Running a command with nohup will look a little different than usual because it will not print anything to the console. Instead, it prints all console outputs into a file in the current directory called nohup.out . You can redirect the output to another filename using: nohup nextflow run main.nf > cegwas_{date}_output.txt When you exit this window and open a new session, you can always look at the contents of the output file using cat nohup.out to see the progress. Note You can also run nohup in the background to continue using the same window for other processes by running nohup nextflow run main.nf & .","title":"Using screen or nohup to keep jobs from timing out"},{"location":"quest-intro/#using_packages_already_installed_on_quest","text":"Quest has a collection of packages installed. You can run module avail to see what packages are currently available on Quest. You can use module load bcftools or module load bcftools/1.10.1 to load packages with the default or a specific version ( module add does the same thing). If you do echo $PATH before and after loading modules, you can see what module does is simply appending paths to the packages into your $PATH so the packages can be found in your environment. module purge will remove all loaded packages and can be helpful to try when troubleshooting. Note You can also load/install software packages into conda environments for use with a specific project/pipeline. Check out the conda tutorial here .","title":"Using packages already installed on Quest"},{"location":"quest-intro/#submitting_jobs_to_quest","text":"Jobs on Quest are managed by SLURM . While most of our pipelines are managed by Nextflow, it is useful to know how to submit jobs directly to SLURM. Below is a template to submit an array job to SLURM that will run 10 parallel jobs. One can run it with sbatch script.sh . If you dig into the Nextflow working directory, you can see Nextflow actually generates such scripts and submits them to SLURM on your behalf. Thanks, Nextflow! #!/bin/bash #SBATCH -J name # Name of job #SBATCH -A b1042 # Allocation #SBATCH -p genomicsguestA # Queue #SBATCH -t 24:00:00 # Walltime/duration of the job (hh:mm:ss) #SBATCH --cpus-per-task=1 # Number of cores (= processors = cpus) for each task #SBATCH --mem-per-cpu=3G # Memory per core needed for a job #SBATCH --array=0-9 # number of parallel jobs to run counting from 0. Make sure to change this to match total number of files. # make a list of all the files you want to process # this script will run a parallel process for each file in this list name_list=(*.bam) # take the nth ($SGE_TASK_ID-th) file in name_list # don't change this line Input=${name_list[$SLURM_ARRAY_TASK_ID]} # then do your operation on this file Output=`echo $Input | sed 's/.bam/.sam/'`","title":"Submitting jobs to Quest"},{"location":"quest-intro/#monitoring_slurm_jobs_on_quest","text":"Here are some commmon commands used to interact with SLURM and check on job status. For more, check out this page . squeue -u <netid> to check all jobs of a user. scancel <jobid> to cancel a job. scancel {30000..32000} to cancel a range of jobs (job id between 30000 and 32000). sinfo | grep b1059 to check status of nodes. alloc means all cores on a node are completed engaged; mix means some cores on a node are engaged; idle means all cores on a node are available to accept jobs.","title":"Monitoring SLURM Jobs on Quest"},{"location":"quest-mount/","text":"1. Download and Install Fuse for Mac OS https://osxfuse.github.io/ 2. Install sshfs You can use the link on https://osxfuse.github.io/ or use: brew install sshfs 3. Create a folder in your documents called b1059 mkdir ~/b1059 4. Mount our labs quest project folder ( b1059 ) to the b1059 folder you created locally sshfs <NETID>@quest.it.northwestern.edu:/projects/b1059/ ~/Documents/b1059 -ovolname=b1059 To mount alignments of isotypes at this location: sshfs <NETID>@quest.it.northwestern.edu:/projects/b1059/data/alignments/WI/isotype ~/Documents/b1059 -ovolname=b1059","title":"Mounting Quest"},{"location":"quest-nextflow/","text":"Nextflow \u00b6 Nextflow Installation Nextflow versions Quest cluster configuration Global Configuration: ~/.nextflow/config Running Nextflow Running Nextflow from a local directory Running Nextflow from a remote directory Resume Writing Nextflow Pipelines The working directory Where am I? Print - debugger's best friend Channel.fromPath(\"A.txt\") in channel creation input: path(\"A.txt\") in the process section DSL2 Run reports Require users to sepcify a parameter value Resources Nextflow is an awesome program that allows you to write a computational pipeline by making it simpler to put together many different tasks, maybe even using different programming languages. Nextflow makes parallelism easy and works with SLURM to schedule jobs so you don't have to! Nextflow also supports docker containers and conda enviornments to streamline reproducible pipelines and can be easily adapted to run on different systems - including Google Cloud! Not convinced? Check out this intro . Installation \u00b6 Nextflow can be installed with two easy steps: # download with wget or curl wget -qO- https://get.nextflow.io | bash # or # curl -s https://get.nextflow.io | bash # make binary executable on your system chmod +x nextflow # Optional - move nextflow to directory accessible by your $PATH to avoid having to type full path to nextflow each time # you can check your $PATH with: echo $PATH # it likely contains `/usr/local/bin` so you could move nextflow there mv nextflow /usr/local/bin/ Nextflow versions \u00b6 You might also want to update nextflow or be able to run different versions. This can be done in several different ways Update Nextflow nextflow self-update Use a specific version of Nextflow NXF_VER=20.04.0 nextflow run main.nf Use the nf20 conda environment built for AndersenLab pipelines module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env (You can check your Nextflow version with nextflow -v or nextflow --version ) Note If you load this conda environment, it is not necessary to have Nextflow version 20 installed on your system -- you don't even need to have Nextflow installed at all! Because different versions might have updates that affect the running of Nextflow, it is important to keep track of the version of Nextflow you are using, as well as all software packages. Important Many of the Andersen Lab pipelines (if not all) are written with the new DSL2 (see below) which requires Nextflow-v20.0+. Loading the nf20 conda environment is a great way to run these pipelines Quest cluster configuration \u00b6 Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config . Global Configuration: ~/.nextflow/config \u00b6 In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { executor = 'slurm' queue = 'genomicsguestA' clusterOptions = '-A b1042 -t 24:00:00 -e errlog.txt' } workDir = \"/projects/b1042/AndersenLab/work/<your folder>\" tmpDir = \"/projects/b1042/AndersenLab/tmp\" This configuration file does the following: Sets the executor to slurm (which is what Quest uses) Sets the queue to genomicsguestA which submits jobs to genomics nodes. The genomicsguestA will submit jobs to our dedicated nodes first, which we have high priority. If our dedicated nodes are full, it will submit to other nodes we don't have priority. So far, our lab have 2 dedicated nodes, with 28 cores and related memory (close to 1:5) for each dedicated node. We will have more in the future. clusterOptions - Sets the account to b1042 ; granting access to genomics-dedicated scratch space. workDir - Sets the working directory to scratch space on b1042. To better organization, Please build your own folder under /projects/b1042/AndersenLab/work/ , and define it here. tmpDir - Creates a temporary working directory. This can be used within workflows when necessary. Running Nextflow \u00b6 Theoretically, running a Nextflow pipeline should be very straightforward (although with any developing pipelines there are always bugs to work out). Running Nextflow from a local directory \u00b6 The most common way our lab runs Nextflow is by first cloning the git repo to your directory and then running: nextflow run <path_to_git_repo>/main.nf --in test.tsv # or if the script you want to run is called main.nf, you don't need to specify it nextflow run <path_to_git_repo> --in test.tsv In this example above, you are located in a different directory than the nextflow main.nf script. For most pipelines, you can be in the same or a different directory and it will run just fine. Note Parameters or arguments to Nextflow scripts are designated with the -- . In the case above, there is a parameter called \"in\" which we are setting to be the test.tsv file. Note The standard name of a nextflow script is main.nf but it doesn't have to be! If you just call nextflow run cegwas2-nf it will automatically choose the main.nf script. It is best practice to always write out the script name though When Nextflow is running, it will print to the console the name of each process in the pipeline as well as update on the progress of the script: For example, in the above screenshot from a NemaScan run, there are 14 different processes in the pipeline. Notice that the fix_strain_names_bulk process has already completed! Meanwhile, the vcf_t_geno_matrix process is still running. You can also see that the prepare_gcta_files is actually run 4 times (in this case, because it is run once per 4 traits in my dataset). Another important piece of information from this Nextflow run is the hash that designates the working directory of each process. the [ad/eea615] next to the fix_strain_names_bulk process indicates that the working directory is located at path_to_nextflow_working_directory/ad/eea615... . This can be helpful if you want to go into that directory to see what is actually happening or trouble shoot errors. Note I highly recommend adding this function to your ~/.bash_profile to easily access the Nexflow working directory: gw() {cd /projects/b1042/AndersenLab/work/<your_name>/$1*} so that when you type gw 3f/6a21a5 (the hash Nextflow shows that indicates the specific working directory for a process) you will go to that folder automatically. Running Nextflow from a remote directory \u00b6 Nextflow can also be run without first cloning the git repo. You can just tell Nextflow which git repo to use and it will do the rest! This can be helpful to reduce clutter and avoid making changes to the actual pipeline. nextflow run AndersenLab/cegwas2-nf --traitfile test.tsv --annotation bcsq --vcf 20210121 The above command should pull the most recent commit from the master branch for the cegwas2-nf repo. Note, sometimes Nextflow does not seem to pull the most recent changes and I am not sure why. Resume \u00b6 Another great thing about Nextflow is that it caches all the processes that completed successfully, so if you have an error at the middle or end of your pipeline, you can re-run the same Nextflow command with -resume and it will start where it finished last time! nextflow run AndersenLab/cegwas2-nf --traitfile test.tsv --annotation bcsq --vcf 20210121 -resume There is usually no downside to adding -resume , so you can get into the habit of always adding it if you want. Important There is some confusion with how the -resume works and sometimes it doesn't work as expected. Check out this and this other guide for more help. One thing I've learned is there is a difference between process.cache = 'deep' vs. 'lenient' . Writing Nextflow Pipelines \u00b6 Check out the Nextflow documentation for help getting started! Note Learning to script with Nextflow definitely has a high learning curve. Don't get discouraged! Start with something small and simple. Maybe convert a current script you have that uses a large for loop into a nextflow pipeline to start getting the hang of things! The working directory \u00b6 Each execution of a process happens in its own temporary working directory. Specify the location of working directory with workDir = '/path_to_tmp/' in nextflow.config, or with -w option when running nextflow main.nf . The working directory is the folder named like /path_to_tmp/4d9c3b333734a5b63d66f0bc0cfcdc that Nextflow points you to when there is an error in execution. This folder contains the error log that could be useful for debugging. One can find the folder path in the .nextflow.log or in the report.html. This folder only contains files (usually in form of symlinks, see below) from the input channel, so it's isolated from the rest of the file system. This folder will also contain all output files (unless specifically directed elsewhere), and only those specified in the output channels and publishDir will be moved or copied to the publishDir . Note that with publishDir \"path\", mode: 'move' , the output file will be moved outside of the working directory and Nextflow will not be able to use it as input for another process, so only use it when there is not a following process that uses the output file. Be mindful that if the \"\"\" (script section) \"\"\" involves changing directory, such as cd or rmarkdown::render( knit_root_dir = \"folder/\" ) , Nextflow will still only search the working directory for output files. Run nextflow clean -f in the excecution folder to clean up the working directories. Where am I? \u00b6 In Nextflow scripts (.nf files), one can use ${workflow.projectDir} to refer where the project locates (usually the folder of main.nf). For example: publishDir \"${workflow.projectDir}/output\", mode: 'copy' or Rscript ${workflow.projectDir}/bin/task.R . ${workflow.launchDir} to refer to where the script is called from. $baseDir usually refers to the same folder as ${workflow.projectDir} but it can also be used in the config file, where ${workflow.projectDir} and ${workflow.launchDir} are not accessible. They are much more reiable than $PWD or $pwd . Print - debugger's best friend \u00b6 To print a channel, use .view() . It's especially useful to resolve WARN: Input tuple does not match input set cardinality declared by process . (Don't forget to remove .view() after debugging) channel_vcf .combine(channel_index) .combine(channel_chr) .view() To print from the script section inside the processes, add echo true . process test { echo true // this will print the stdout from the script section on Terminal input: path(vcf) \"\"\" head $vcf \"\"\" } Channel.fromPath(\"A.txt\") in channel creation \u00b6 Channel.from( \"A.txt\" ) will put A.txt as is into the channel Channel.fromPath( \"A.txt\" ) will add a full path (usually current directory) and put /path/A.txt into the channel. Channel.fromPath( \"folder/A.txt\" ) will add a full path (usually current directory) and put /path/folder/A.txt into the channel. Channel.fromPath( \"/path/A.txt\" ) will put /path/A.txt into the channel. In other words, Channel.fromPath will only add a full path if there isn't already one and ensure there is always a full path in the resulting channel. This goes hand in hand with input: path(\"A.txt\") inside the process, where Nextflow actually creates a symlink named A.txt (note the path from first / to last / is stripped) linking to /path/A.txt in the working directory , so it can be accessed within the working directory by the script cat A.txt without specifying a path. input: path(\"A.txt\") in the process section \u00b6 With input: path(\"A.txt\") one can refer to the file in the script as A.txt . Side note A.txt doesn't have to be the same name as in channel creation, it can be anything, input: path(\"B.txt\") , input: path(\"n\") etc. With input: path(A) one can refer to the file in the script as $A , and the value of $A will be the original file name (without path, see section above). input: path(\"A.txt\") and input: path \"A.txt\" generally both work. Occasionally had errors that required the following (tip from @danielecook ): If not in a tuple, use input: path \"A.txt\" If in a tuple, use input: tuple path(\"A.txt\"), path(\"B.txt\") This goes the same for output . From @pditommaso : path(A) is almost the same as file(A) , however the first interprets a value of type string as the input file path (ie the location in the file system where it's stored), the latter interprets a value of type string and materialise it to a temporary files. It's recommended the use of path since it's less ambiguous and fits better in most use-cases. DSL2 \u00b6 Moving to DSL2 is a one-way street. It's so intuitive with clean and readable code. In DSL1, each queue channel can only be used once. In DSL2, a channel can be fed into multiple processes In DSL2, each process can only be called once. The solution is either .concat() the input channels so they run as parallel processes, or put the process in a module and import multiple times from the module. (One may be able to call a process in different workflows, haven't tested yet). DSL2 also enforces that all inputs needs to be combined into 1 channel before it goes into a process. See the cheatsheet for useful operators. Simple steps to convert from original syntax to DSL2 Deprecated operators . Run reports \u00b6 nextflow main.nf -with-report -with-timeline -with-dag -with-report Nextflow html report contains resource usage for each process, and details (most useful being the status and working directory) for each process -with-timeline How much wait time and run time each process took for the run. Very useful reference for optimizing resource allocation and improving run time. -with-dag Make a flowchart to show the relationship of channels and processes. Software dependencies to use these features. Note the differences on Mac and Linux. How to set them up in the nextflow.config so they are automatically generated for each run. Credit @danielecook Require users to sepcify a parameter value \u00b6 There are 2 types of paramters: (a) one with no actual value (b) one with actual values. (a) If a parameter is specified but no value is given, it is implicitly considered true . So one can use this to run debug mode nextflow main.nf --debug if (params.debug) { ... (set parameters for debug mode) } else { ... (set parameters for normal use) } or to print help message nextflow main.nf --help if (params.help) { println \"\"\" ... (help msg here) \"\"\" exit 0 } (b) For parameters that need to contain a value, Nextflow recommends to set a default and let users to overwrite it as needed. However, if you want to require it to be specified by the user: params.reference = null // no quotes. this line is optional, since without initialising the parameter it will default to null. if (params.reference == null) error \"Please specify a reference genome with --reference\" Below works as long as the user always append a value: --reference=something . It will not print the error message with: nextflow main.nf --reference (without specifying a value) because this will set params.reference to true (see point (a) ) and !params.reference will be false . if (!params.reference) error \"Please specify a reference genome with --reference\" Resources \u00b6 Nextflow documentation Nextflow cheatsheet Nextflow gitter Awesome Nextflow pipeline examples - Repository of great nextflow pipelines. Official Nextflow patterns Google group","title":"Nextflow"},{"location":"quest-nextflow/#nextflow","text":"Nextflow Installation Nextflow versions Quest cluster configuration Global Configuration: ~/.nextflow/config Running Nextflow Running Nextflow from a local directory Running Nextflow from a remote directory Resume Writing Nextflow Pipelines The working directory Where am I? Print - debugger's best friend Channel.fromPath(\"A.txt\") in channel creation input: path(\"A.txt\") in the process section DSL2 Run reports Require users to sepcify a parameter value Resources Nextflow is an awesome program that allows you to write a computational pipeline by making it simpler to put together many different tasks, maybe even using different programming languages. Nextflow makes parallelism easy and works with SLURM to schedule jobs so you don't have to! Nextflow also supports docker containers and conda enviornments to streamline reproducible pipelines and can be easily adapted to run on different systems - including Google Cloud! Not convinced? Check out this intro .","title":"Nextflow"},{"location":"quest-nextflow/#installation","text":"Nextflow can be installed with two easy steps: # download with wget or curl wget -qO- https://get.nextflow.io | bash # or # curl -s https://get.nextflow.io | bash # make binary executable on your system chmod +x nextflow # Optional - move nextflow to directory accessible by your $PATH to avoid having to type full path to nextflow each time # you can check your $PATH with: echo $PATH # it likely contains `/usr/local/bin` so you could move nextflow there mv nextflow /usr/local/bin/","title":"Installation"},{"location":"quest-nextflow/#nextflow_versions","text":"You might also want to update nextflow or be able to run different versions. This can be done in several different ways Update Nextflow nextflow self-update Use a specific version of Nextflow NXF_VER=20.04.0 nextflow run main.nf Use the nf20 conda environment built for AndersenLab pipelines module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env (You can check your Nextflow version with nextflow -v or nextflow --version ) Note If you load this conda environment, it is not necessary to have Nextflow version 20 installed on your system -- you don't even need to have Nextflow installed at all! Because different versions might have updates that affect the running of Nextflow, it is important to keep track of the version of Nextflow you are using, as well as all software packages. Important Many of the Andersen Lab pipelines (if not all) are written with the new DSL2 (see below) which requires Nextflow-v20.0+. Loading the nf20 conda environment is a great way to run these pipelines","title":"Nextflow versions"},{"location":"quest-nextflow/#quest_cluster_configuration","text":"Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config .","title":"Quest cluster configuration"},{"location":"quest-nextflow/#global_configuration_nextflowconfig","text":"In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { executor = 'slurm' queue = 'genomicsguestA' clusterOptions = '-A b1042 -t 24:00:00 -e errlog.txt' } workDir = \"/projects/b1042/AndersenLab/work/<your folder>\" tmpDir = \"/projects/b1042/AndersenLab/tmp\" This configuration file does the following: Sets the executor to slurm (which is what Quest uses) Sets the queue to genomicsguestA which submits jobs to genomics nodes. The genomicsguestA will submit jobs to our dedicated nodes first, which we have high priority. If our dedicated nodes are full, it will submit to other nodes we don't have priority. So far, our lab have 2 dedicated nodes, with 28 cores and related memory (close to 1:5) for each dedicated node. We will have more in the future. clusterOptions - Sets the account to b1042 ; granting access to genomics-dedicated scratch space. workDir - Sets the working directory to scratch space on b1042. To better organization, Please build your own folder under /projects/b1042/AndersenLab/work/ , and define it here. tmpDir - Creates a temporary working directory. This can be used within workflows when necessary.","title":"Global Configuration: ~/.nextflow/config"},{"location":"quest-nextflow/#running_nextflow","text":"Theoretically, running a Nextflow pipeline should be very straightforward (although with any developing pipelines there are always bugs to work out).","title":"Running Nextflow"},{"location":"quest-nextflow/#running_nextflow_from_a_local_directory","text":"The most common way our lab runs Nextflow is by first cloning the git repo to your directory and then running: nextflow run <path_to_git_repo>/main.nf --in test.tsv # or if the script you want to run is called main.nf, you don't need to specify it nextflow run <path_to_git_repo> --in test.tsv In this example above, you are located in a different directory than the nextflow main.nf script. For most pipelines, you can be in the same or a different directory and it will run just fine. Note Parameters or arguments to Nextflow scripts are designated with the -- . In the case above, there is a parameter called \"in\" which we are setting to be the test.tsv file. Note The standard name of a nextflow script is main.nf but it doesn't have to be! If you just call nextflow run cegwas2-nf it will automatically choose the main.nf script. It is best practice to always write out the script name though When Nextflow is running, it will print to the console the name of each process in the pipeline as well as update on the progress of the script: For example, in the above screenshot from a NemaScan run, there are 14 different processes in the pipeline. Notice that the fix_strain_names_bulk process has already completed! Meanwhile, the vcf_t_geno_matrix process is still running. You can also see that the prepare_gcta_files is actually run 4 times (in this case, because it is run once per 4 traits in my dataset). Another important piece of information from this Nextflow run is the hash that designates the working directory of each process. the [ad/eea615] next to the fix_strain_names_bulk process indicates that the working directory is located at path_to_nextflow_working_directory/ad/eea615... . This can be helpful if you want to go into that directory to see what is actually happening or trouble shoot errors. Note I highly recommend adding this function to your ~/.bash_profile to easily access the Nexflow working directory: gw() {cd /projects/b1042/AndersenLab/work/<your_name>/$1*} so that when you type gw 3f/6a21a5 (the hash Nextflow shows that indicates the specific working directory for a process) you will go to that folder automatically.","title":"Running Nextflow from a local directory"},{"location":"quest-nextflow/#running_nextflow_from_a_remote_directory","text":"Nextflow can also be run without first cloning the git repo. You can just tell Nextflow which git repo to use and it will do the rest! This can be helpful to reduce clutter and avoid making changes to the actual pipeline. nextflow run AndersenLab/cegwas2-nf --traitfile test.tsv --annotation bcsq --vcf 20210121 The above command should pull the most recent commit from the master branch for the cegwas2-nf repo. Note, sometimes Nextflow does not seem to pull the most recent changes and I am not sure why.","title":"Running Nextflow from a remote directory"},{"location":"quest-nextflow/#resume","text":"Another great thing about Nextflow is that it caches all the processes that completed successfully, so if you have an error at the middle or end of your pipeline, you can re-run the same Nextflow command with -resume and it will start where it finished last time! nextflow run AndersenLab/cegwas2-nf --traitfile test.tsv --annotation bcsq --vcf 20210121 -resume There is usually no downside to adding -resume , so you can get into the habit of always adding it if you want. Important There is some confusion with how the -resume works and sometimes it doesn't work as expected. Check out this and this other guide for more help. One thing I've learned is there is a difference between process.cache = 'deep' vs. 'lenient' .","title":"Resume"},{"location":"quest-nextflow/#writing_nextflow_pipelines","text":"Check out the Nextflow documentation for help getting started! Note Learning to script with Nextflow definitely has a high learning curve. Don't get discouraged! Start with something small and simple. Maybe convert a current script you have that uses a large for loop into a nextflow pipeline to start getting the hang of things!","title":"Writing Nextflow Pipelines"},{"location":"quest-nextflow/#the_working_directory","text":"Each execution of a process happens in its own temporary working directory. Specify the location of working directory with workDir = '/path_to_tmp/' in nextflow.config, or with -w option when running nextflow main.nf . The working directory is the folder named like /path_to_tmp/4d9c3b333734a5b63d66f0bc0cfcdc that Nextflow points you to when there is an error in execution. This folder contains the error log that could be useful for debugging. One can find the folder path in the .nextflow.log or in the report.html. This folder only contains files (usually in form of symlinks, see below) from the input channel, so it's isolated from the rest of the file system. This folder will also contain all output files (unless specifically directed elsewhere), and only those specified in the output channels and publishDir will be moved or copied to the publishDir . Note that with publishDir \"path\", mode: 'move' , the output file will be moved outside of the working directory and Nextflow will not be able to use it as input for another process, so only use it when there is not a following process that uses the output file. Be mindful that if the \"\"\" (script section) \"\"\" involves changing directory, such as cd or rmarkdown::render( knit_root_dir = \"folder/\" ) , Nextflow will still only search the working directory for output files. Run nextflow clean -f in the excecution folder to clean up the working directories.","title":"The working directory"},{"location":"quest-nextflow/#where_am_i","text":"In Nextflow scripts (.nf files), one can use ${workflow.projectDir} to refer where the project locates (usually the folder of main.nf). For example: publishDir \"${workflow.projectDir}/output\", mode: 'copy' or Rscript ${workflow.projectDir}/bin/task.R . ${workflow.launchDir} to refer to where the script is called from. $baseDir usually refers to the same folder as ${workflow.projectDir} but it can also be used in the config file, where ${workflow.projectDir} and ${workflow.launchDir} are not accessible. They are much more reiable than $PWD or $pwd .","title":"Where am I?"},{"location":"quest-nextflow/#print_-_debuggers_best_friend","text":"To print a channel, use .view() . It's especially useful to resolve WARN: Input tuple does not match input set cardinality declared by process . (Don't forget to remove .view() after debugging) channel_vcf .combine(channel_index) .combine(channel_chr) .view() To print from the script section inside the processes, add echo true . process test { echo true // this will print the stdout from the script section on Terminal input: path(vcf) \"\"\" head $vcf \"\"\" }","title":"Print - debugger's best friend"},{"location":"quest-nextflow/#channelfrompathatxt_in_channel_creation","text":"Channel.from( \"A.txt\" ) will put A.txt as is into the channel Channel.fromPath( \"A.txt\" ) will add a full path (usually current directory) and put /path/A.txt into the channel. Channel.fromPath( \"folder/A.txt\" ) will add a full path (usually current directory) and put /path/folder/A.txt into the channel. Channel.fromPath( \"/path/A.txt\" ) will put /path/A.txt into the channel. In other words, Channel.fromPath will only add a full path if there isn't already one and ensure there is always a full path in the resulting channel. This goes hand in hand with input: path(\"A.txt\") inside the process, where Nextflow actually creates a symlink named A.txt (note the path from first / to last / is stripped) linking to /path/A.txt in the working directory , so it can be accessed within the working directory by the script cat A.txt without specifying a path.","title":"Channel.fromPath(\"A.txt\") in channel creation"},{"location":"quest-nextflow/#input_pathatxt_in_the_process_section","text":"With input: path(\"A.txt\") one can refer to the file in the script as A.txt . Side note A.txt doesn't have to be the same name as in channel creation, it can be anything, input: path(\"B.txt\") , input: path(\"n\") etc. With input: path(A) one can refer to the file in the script as $A , and the value of $A will be the original file name (without path, see section above). input: path(\"A.txt\") and input: path \"A.txt\" generally both work. Occasionally had errors that required the following (tip from @danielecook ): If not in a tuple, use input: path \"A.txt\" If in a tuple, use input: tuple path(\"A.txt\"), path(\"B.txt\") This goes the same for output . From @pditommaso : path(A) is almost the same as file(A) , however the first interprets a value of type string as the input file path (ie the location in the file system where it's stored), the latter interprets a value of type string and materialise it to a temporary files. It's recommended the use of path since it's less ambiguous and fits better in most use-cases.","title":"input: path(\"A.txt\") in the process section"},{"location":"quest-nextflow/#dsl2","text":"Moving to DSL2 is a one-way street. It's so intuitive with clean and readable code. In DSL1, each queue channel can only be used once. In DSL2, a channel can be fed into multiple processes In DSL2, each process can only be called once. The solution is either .concat() the input channels so they run as parallel processes, or put the process in a module and import multiple times from the module. (One may be able to call a process in different workflows, haven't tested yet). DSL2 also enforces that all inputs needs to be combined into 1 channel before it goes into a process. See the cheatsheet for useful operators. Simple steps to convert from original syntax to DSL2 Deprecated operators .","title":"DSL2"},{"location":"quest-nextflow/#run_reports","text":"nextflow main.nf -with-report -with-timeline -with-dag -with-report Nextflow html report contains resource usage for each process, and details (most useful being the status and working directory) for each process -with-timeline How much wait time and run time each process took for the run. Very useful reference for optimizing resource allocation and improving run time. -with-dag Make a flowchart to show the relationship of channels and processes. Software dependencies to use these features. Note the differences on Mac and Linux. How to set them up in the nextflow.config so they are automatically generated for each run. Credit @danielecook","title":"Run reports"},{"location":"quest-nextflow/#require_users_to_sepcify_a_parameter_value","text":"There are 2 types of paramters: (a) one with no actual value (b) one with actual values. (a) If a parameter is specified but no value is given, it is implicitly considered true . So one can use this to run debug mode nextflow main.nf --debug if (params.debug) { ... (set parameters for debug mode) } else { ... (set parameters for normal use) } or to print help message nextflow main.nf --help if (params.help) { println \"\"\" ... (help msg here) \"\"\" exit 0 } (b) For parameters that need to contain a value, Nextflow recommends to set a default and let users to overwrite it as needed. However, if you want to require it to be specified by the user: params.reference = null // no quotes. this line is optional, since without initialising the parameter it will default to null. if (params.reference == null) error \"Please specify a reference genome with --reference\" Below works as long as the user always append a value: --reference=something . It will not print the error message with: nextflow main.nf --reference (without specifying a value) because this will set params.reference to true (see point (a) ) and !params.reference will be false . if (!params.reference) error \"Please specify a reference genome with --reference\"","title":"Require users to sepcify a parameter value"},{"location":"quest-nextflow/#resources","text":"Nextflow documentation Nextflow cheatsheet Nextflow gitter Awesome Nextflow pipeline examples - Repository of great nextflow pipelines. Official Nextflow patterns Google group","title":"Resources"},{"location":"r/","text":"R \u00b6 R Andersen Lab R Packages cegwas2 linkagemapping COPASutils easysorter easyXpress General R resources Andersen Lab R Packages \u00b6 The Andersen lab maintains several R packages useful for high-throughput data analysis. cegwas2 \u00b6 This package contains a set of functions to process phenotype data, perform GWAS, and perform post-mapping data processing for C. elegans . In 2019, the cegwas2-nf Nextflow pipeline was developed to perform GWA mapping on QUEST using this cegwas2 R package. However, mapping is rarely if never done with cegwas2 in R manually. To learn more about the cegwas2 R package, see the andersenlab/cegwas2 repo. For help running a GWA mapping using cegwas2, see cegwas2-nf or the dry guide Note cegwas2 was preceeded by cegwas and will soon be superceeded by NemaScan linkagemapping \u00b6 This package includes all data and functions necessary to complete a mapping for the phenotype of your choice using the recombinant inbred lines from Andersen, et al. 2015 (G3) . Included with this package are the cross and map objects for this strain set as well a markers.rds file containing a lookup table for the physical positions of all markers used for mapping. To learn more about linkagemapping including how to install and use the package, check out the andersenlab/linkagemapping repo. Note Also check out the linkagemapping-nf repo for a reproducible Nextflow pipeline for linkage mapping and two-dimensional genome scans (scan2) for one or several traits. COPASutils \u00b6 The R package COPASutils provides a logical workflow for the reading, processing, and visualization of data obtained from the Union Biometrica Complex Object Parametric Analyzer and Sorter (COPAS) or the BioSorter large-particle flow cytometers. Data obtained from these powerful experimental platforms can be unwieldy, leading to difficulties in the ability to process and visualize the data using existing tools. Researchers studying small organisms, such as Caenorhabditis elegans, Anopheles gambiae, and Danio rerio, and using these devices will benefit from this streamlined and extensible R package. COPASutils offers a powerful suite of functions for the rapid processing and analysis of large high-throughput screening data sets. To learn more about COPASutils including how to install and use the package, check out the andersenlab/COPASutils repo and the COPASutils manuscript easysorter \u00b6 This package is effectively version 2 of the COPASutils package. This package is specialized for use with worms and includes additional functionality on top of that provided by COPASutils, including division of recorded objects by larval stage and the ability to regress out control phenotypes from those recorded in experimental conditions To learn more about easysorter including how to install and use the package, check out the andersenlab/easysorter repo. Here are some of the papers using easysorter : The first easysorter paper A Powerful New Quantitative Genetics Platform, Combining Caenorhabditis elegans High-Throughput Fitness Assays with a Large Collection of Recombinant Strains ( Andersen et al. 2015 ) The first \"V3\" easysorter paper The Gene scb-1 Underlies Variation in Caenorhabditis elegans Chemotherapeutic Responses ( Evans and Andersen 2020 ) The first dominance/hemizygosity easysorter paper A Novel Gene Underlies Bleomycin-Response Variation in Caenorhabditis elegans ( Brady et al. 2019 ) Almost every paper published from the lab has used easysorter, for more, check out our lab papers Note The easysorter package requires COPASutils installation as well. easyXpress \u00b6 This package is designed for the reading, processing, and visualization of images obtained from the Molecular Devices ImageExpress Nano Imager, and processed with CellProfiler's WormToolbox. To learn more about easyXpress including how to install and use the package, check out the andersenlab/easyXpress repo and the easyXpress manuscript . General R resources \u00b6 Tidyverse workshop and resources Andersen Lab R Knowledge base & Cheatsheet Also check out the lab_code slack channel for help/questions!","title":"R"},{"location":"r/#r","text":"R Andersen Lab R Packages cegwas2 linkagemapping COPASutils easysorter easyXpress General R resources","title":"R"},{"location":"r/#andersen_lab_r_packages","text":"The Andersen lab maintains several R packages useful for high-throughput data analysis.","title":"Andersen Lab R Packages"},{"location":"r/#cegwas2","text":"This package contains a set of functions to process phenotype data, perform GWAS, and perform post-mapping data processing for C. elegans . In 2019, the cegwas2-nf Nextflow pipeline was developed to perform GWA mapping on QUEST using this cegwas2 R package. However, mapping is rarely if never done with cegwas2 in R manually. To learn more about the cegwas2 R package, see the andersenlab/cegwas2 repo. For help running a GWA mapping using cegwas2, see cegwas2-nf or the dry guide Note cegwas2 was preceeded by cegwas and will soon be superceeded by NemaScan","title":"cegwas2"},{"location":"r/#linkagemapping","text":"This package includes all data and functions necessary to complete a mapping for the phenotype of your choice using the recombinant inbred lines from Andersen, et al. 2015 (G3) . Included with this package are the cross and map objects for this strain set as well a markers.rds file containing a lookup table for the physical positions of all markers used for mapping. To learn more about linkagemapping including how to install and use the package, check out the andersenlab/linkagemapping repo. Note Also check out the linkagemapping-nf repo for a reproducible Nextflow pipeline for linkage mapping and two-dimensional genome scans (scan2) for one or several traits.","title":"linkagemapping"},{"location":"r/#copasutils","text":"The R package COPASutils provides a logical workflow for the reading, processing, and visualization of data obtained from the Union Biometrica Complex Object Parametric Analyzer and Sorter (COPAS) or the BioSorter large-particle flow cytometers. Data obtained from these powerful experimental platforms can be unwieldy, leading to difficulties in the ability to process and visualize the data using existing tools. Researchers studying small organisms, such as Caenorhabditis elegans, Anopheles gambiae, and Danio rerio, and using these devices will benefit from this streamlined and extensible R package. COPASutils offers a powerful suite of functions for the rapid processing and analysis of large high-throughput screening data sets. To learn more about COPASutils including how to install and use the package, check out the andersenlab/COPASutils repo and the COPASutils manuscript","title":"COPASutils"},{"location":"r/#easysorter","text":"This package is effectively version 2 of the COPASutils package. This package is specialized for use with worms and includes additional functionality on top of that provided by COPASutils, including division of recorded objects by larval stage and the ability to regress out control phenotypes from those recorded in experimental conditions To learn more about easysorter including how to install and use the package, check out the andersenlab/easysorter repo. Here are some of the papers using easysorter : The first easysorter paper A Powerful New Quantitative Genetics Platform, Combining Caenorhabditis elegans High-Throughput Fitness Assays with a Large Collection of Recombinant Strains ( Andersen et al. 2015 ) The first \"V3\" easysorter paper The Gene scb-1 Underlies Variation in Caenorhabditis elegans Chemotherapeutic Responses ( Evans and Andersen 2020 ) The first dominance/hemizygosity easysorter paper A Novel Gene Underlies Bleomycin-Response Variation in Caenorhabditis elegans ( Brady et al. 2019 ) Almost every paper published from the lab has used easysorter, for more, check out our lab papers Note The easysorter package requires COPASutils installation as well.","title":"easysorter"},{"location":"r/#easyxpress","text":"This package is designed for the reading, processing, and visualization of images obtained from the Molecular Devices ImageExpress Nano Imager, and processed with CellProfiler's WormToolbox. To learn more about easyXpress including how to install and use the package, check out the andersenlab/easyXpress repo and the easyXpress manuscript .","title":"easyXpress"},{"location":"r/#general_r_resources","text":"Tidyverse workshop and resources Andersen Lab R Knowledge base & Cheatsheet Also check out the lab_code slack channel for help/questions!","title":"General R resources"},{"location":"travis-ci/","text":"Setting up Quest \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Travis ci"},{"location":"travis-ci/#setting_up_quest","text":"For full documentation visit mkdocs.org .","title":"Setting up Quest"},{"location":"travis-ci/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"travis-ci/#project_layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"archive/quest-andersen-lab-env/","text":"The Andersen Lab Software Environment (no longer supported) \u00b6 The Andersen Lab Software Environment (no longer supported) andersen-lab-env pyenv Setting the global version Setting the local version pyenv-virtualenv conda conda integrates with pyenv and pyenv-virtualenv pyenv environments are inherited andersen-lab-env structure Installing the andersen-lab-env andersen-lab-env git structure adding new software andersen-lab-env \u00b6 Computational Reproducibility is the ability to reproduce an analysis exactly. In order for comutational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub. Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software we've developed the andersen-lab-env . The andersen-lab-env is a set of software environments that can be used in conjunction with the bioinformatic pipelines we have developed for Quest. These environments can be installed locally on a Mac or on Quest. The andersen-lab-env is designed to change over time, but we explicitly define the software versions, and we track changes to the environments over time. The system is in some ways complex. This page is designed to try to explain how it works. We rely on three different tools to manage software environments. In concert they provide a lot of flexibility when it comes to setting up the software environment. Note The software environments on Mac and Linux are not exactly identical...but they are very close. There is an installation script you can use to install the andersen-lab-env , but it is recommended that you read this page before doing so. pyenv \u00b6 pyenv documentation pyenv is used to install and manage different versions of python. For example, you might have a python 3 script for one project and a python 2 script for another. You want to be able to run both scripts on your system. One option is to modify the python 2 script to work with python 3, but this is not always an option. The solution is to be able to install multiple versions of python simultaneously. pyenv allows you to do this. More than this, pyenv allows you to set the python version that you want to use at the local or global level. local - Sets the python version to a specific directory. global - Sets the python version to use everywhere unless a local version is set. Lets look at an example of this. First, lets install two different versions of python. pyenv install 2.7.11 pyenv install 3.6.0 Now you can see the installed versions by typing pyenv versions : $ pyenv versions * system 2.7.11 3.6.0 The * indicates that that is the current version of python you are using. In the case above it is set to use the system python which is preinstalled and is often python 2. Setting the global version \u00b6 Example setting the global version of python to 3.6.0 pyenv global 3.6.0 Now when we run python it will use python version 3.6.0. Setting the local version \u00b6 Example setting the local version of python to 2.7.11 mkdir my_python2_project cd my_python2_project pyenv local 2.7.11 Now if we go into a particular directoy and type pyenv local 2.7.11 , a .python-version file is created that says 2.7.11 and makes it so that directory always uses python 2.7.11. Now lets see what this looks like: As is illustrated above, versions are inherited from parent directories. When you cd to a directory, pyenv searches up through each directory looking for a .python-version file to identify which version of python to use. If it reaches the top before finding one it uses the global version. tl;dr; - pyenv allows us to install separate versions of python and set them at the directory level. pyenv-virtualenv \u00b6 Documentation pyenv lets us install multiple versions of python, and lets us use those versions locally within certain directories or globally. But what if we have two projects that use Python 2.7.11 and one requires a python module with a specific version: networkx==1.0 . Another project the same module greater than version 2.0 networkx>2.0 . How can we simultaneously work on both projects on the same system? virtualenv is a python tool for creating isolated python environments (also known as virtualenvs; The usage tends to be specific for python virtual environments and is short for 'virtual environment'). You can create a virtualenv for every project that you do - and these can be used to ensure that when you update or install modules for a given project that they do not interfere with each other. We won't be using virtualenv directly, but instead will the pyenv flavor of virtualenvs. pyenv-virtualenv is a tool that can create virtual environments that operate similar to the way pyenv python environments do. You can create virtualenvs that act globally or you can create virtualenvs that are local to a specific directory. To create a pyenv-virtualenv you must provide a base python environment that you have installed and a name for the environment. For example, below the python environment is 2.7.11 and the name of the environment is c_elegans_project : pyenv virtualenv 2.7.11 c_elegans_project Then you can set that virtualenv to a local directory using: mkdir c_elegans_project cd c_elegans_project pyenv local c_elegans_project Notice that the folder name is the same as the virtualenv. This can be a good idea for clarity. You can see a list of python versions and virtual environments by typing: pyenv versions Output: 2.7.11 * 2.7.11/envs/c_elegans_project 3.6.0 Virtual environments are designated as <version>/envs/<name> . Now we can also install the module we need for that specific project. pyenv installs a python-specific package manager called pip : pip install networkx==1.0 Notice that at this point we have isolated independent environments that do not interfere with one another. If we leave them alone for a year we should be able to come back and the software environment should be the same... and if they work with data they should reproduce the identical result. tl;dr - pyenv-virtualenv can define custom isolated python environments and set them the same way pyenv sets python installations. conda \u00b6 Conda Documentation Thus far we've managed to install multiple versions of python and figured out how to use them in independent, isolated environments. But we obviously use a lot more than just Python. We need to be able to install things like bcftools to work with variant data. We need to be able to install Java packages, and R packages, and all kinds of software. Conda can help us with this. Conda is a language-agnostic package manager. That means it can be used to install packages from python, R, Java, C/C++, etc. For example, the command below will install R and the R Tidyverse . conda install r-tidyverse conda integrates with pyenv and pyenv-virtualenv \u00b6 Important for our purposes, conda can be installed by pyenv . When I stated earlier that pyenv is used to install and manage versions of python I ommitted the fact that it can also install conda to avoid confusion. conda is not a version of python , but it is written in python, and it can be used to install python modules in addition to lots of other stuff. Similar to python virtualenvs, isolated conda environments can be created as was demonstrated above. You would run something like: pyenv install miniconda3-4.3.27 pyenv virtualenv miniconda3-4.3.27 my_new_conda_env pyenv local my_new_conda_env conda install bcftools pip install requests # This version of pip is specific gto What is great about these environments is that we can create custom software environments to suit any project. We can install R packages, python modules, C/C++ executables, and more. pyenv environments are inherited \u00b6 We can now install custom environments for each project. Even better, pyenv allows you to specify multiple environments together. Consider the example in this diagram: There are two environments defined: env_1 bcftools v1.6 bedtools v1.2 R-tidyverse 1.0 env_2 bcftools v1.6 vcf-kit v1.6 Those environments on their own appear in blue above. If we were to use the following command to specify these environments: pyenv local env_2 env_1 base_version We would produce the green environment in the diagram. What you are seeing are two environments being combined. However, the order you specify them in matters. Notice that bcftools v1.7 is used and not bcftools v1.6 . This is because env_2 is searched first when commands libraries are retrieved. After pulling all the libraries in env_2 , the combined library will inherit anything remaining in env_1 . This allows to easily combine environments for analysis. Remember that each of these virtual environments is based on a version of python or conda. But you can also put a plain version of python or conda as your last environment. This is useful when using conda because the conda command does not inherit from conda-based virtualenvs. andersen-lab-env structure \u00b6 The anderse-lab-env uses two conda environments: primary - The primary environment contains the majority of the tools required for performing sequence analysis. py2 - For programs that require python 2. You can create and use your own conda environments for projects, but these are designed to be comprehensive. Installing the andersen-lab-env \u00b6 If you are on Quest Edit your .bashrc file to contain the following: # .bashrc export PKG_CONFIG_PATH=/usr/share/pkgconfig:$PKG_CONFIG_PATH # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi Installation The andersen-lab-env can be installed by running the following command: cd && if cd ~/andersen-lab-env; then git pull; else git clone http://www.github.com/andersenlab/andersen-lab-env; fi bash setup.sh This command will clone the repo, cd into it, and run the setup.sh script. When you run the setup.sh script it will install the latest version of the primary and py2 environments, and it will assign these environments globally as: pyenv primary-(date) py2-(date) minicondax-x.x.x You should not need to change your global environment Note If you have existing versions of the primary and py2 environments installed they will remain. You can set them locally at the project level if necessary. andersen-lab-env git structure \u00b6 The andersen-lab-env is used to manage and version the software environments. The repo has the following structure: \u251c\u2500\u2500 Brewfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 primary.environment.yaml \u251c\u2500\u2500 py2.environment.yaml \u251c\u2500\u2500 rebuild_envs.sh \u251c\u2500\u2500 setup.sh \u251c\u2500\u2500 user_bash_profile.sh \u2514\u2500\u2500 versions \u251c\u2500\u2500 Linux.2018-03-08.primary.yaml \u251c\u2500\u2500 Linux.2018-03-08.py2.yaml \u251c\u2500\u2500 Mac.2018-03-08.primary.yaml \u2514\u2500\u2500 Mac.2018-03-08.py2.yaml primary.environment.yaml - base primary environment. This lists the software to be installed, but not specific versions of it. py2.environment.yaml - The base py2 environment. This lists the software to be installed, but not specific versions of it. Brewfile - Defines the software software-dependencies to be installed when running setup.sh rebuild_envs - Used to construct new versions of the environments. Note that you need to do this on a Linux and Mac computer. user_bash_profile.sh - The optional bash profile that is created with setup.sh . versions/ - Software required for each environment with all dependencies. Versioned in git and by platform and date. adding new software \u00b6 When you want to add new software a new version of the primary and py2 environments should be created. You must modify the primary.environment.yaml or py2.environment.yaml files and build the files you see in the versions folder which define the required software by specific version and includes all the dependencies. bash rebuild_envs.sh This will output two new versions specific to your platform in the versions/ folder. You must run this script and generate the appropriate version files on both Mac and Linux. Commit the updated versions to git. Other users can then install them by running the command in installing the andersen-lab-env","title":"The Andersen Lab Software Environment (no longer supported)"},{"location":"archive/quest-andersen-lab-env/#the_andersen_lab_software_environment_no_longer_supported","text":"The Andersen Lab Software Environment (no longer supported) andersen-lab-env pyenv Setting the global version Setting the local version pyenv-virtualenv conda conda integrates with pyenv and pyenv-virtualenv pyenv environments are inherited andersen-lab-env structure Installing the andersen-lab-env andersen-lab-env git structure adding new software","title":"The Andersen Lab Software Environment (no longer supported)"},{"location":"archive/quest-andersen-lab-env/#andersen-lab-env","text":"Computational Reproducibility is the ability to reproduce an analysis exactly. In order for comutational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub. Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software we've developed the andersen-lab-env . The andersen-lab-env is a set of software environments that can be used in conjunction with the bioinformatic pipelines we have developed for Quest. These environments can be installed locally on a Mac or on Quest. The andersen-lab-env is designed to change over time, but we explicitly define the software versions, and we track changes to the environments over time. The system is in some ways complex. This page is designed to try to explain how it works. We rely on three different tools to manage software environments. In concert they provide a lot of flexibility when it comes to setting up the software environment. Note The software environments on Mac and Linux are not exactly identical...but they are very close. There is an installation script you can use to install the andersen-lab-env , but it is recommended that you read this page before doing so.","title":"andersen-lab-env"},{"location":"archive/quest-andersen-lab-env/#pyenv","text":"pyenv documentation pyenv is used to install and manage different versions of python. For example, you might have a python 3 script for one project and a python 2 script for another. You want to be able to run both scripts on your system. One option is to modify the python 2 script to work with python 3, but this is not always an option. The solution is to be able to install multiple versions of python simultaneously. pyenv allows you to do this. More than this, pyenv allows you to set the python version that you want to use at the local or global level. local - Sets the python version to a specific directory. global - Sets the python version to use everywhere unless a local version is set. Lets look at an example of this. First, lets install two different versions of python. pyenv install 2.7.11 pyenv install 3.6.0 Now you can see the installed versions by typing pyenv versions : $ pyenv versions * system 2.7.11 3.6.0 The * indicates that that is the current version of python you are using. In the case above it is set to use the system python which is preinstalled and is often python 2.","title":"pyenv"},{"location":"archive/quest-andersen-lab-env/#setting_the_global_version","text":"Example setting the global version of python to 3.6.0 pyenv global 3.6.0 Now when we run python it will use python version 3.6.0.","title":"Setting the global version"},{"location":"archive/quest-andersen-lab-env/#setting_the_local_version","text":"Example setting the local version of python to 2.7.11 mkdir my_python2_project cd my_python2_project pyenv local 2.7.11 Now if we go into a particular directoy and type pyenv local 2.7.11 , a .python-version file is created that says 2.7.11 and makes it so that directory always uses python 2.7.11. Now lets see what this looks like: As is illustrated above, versions are inherited from parent directories. When you cd to a directory, pyenv searches up through each directory looking for a .python-version file to identify which version of python to use. If it reaches the top before finding one it uses the global version. tl;dr; - pyenv allows us to install separate versions of python and set them at the directory level.","title":"Setting the local version"},{"location":"archive/quest-andersen-lab-env/#pyenv-virtualenv","text":"Documentation pyenv lets us install multiple versions of python, and lets us use those versions locally within certain directories or globally. But what if we have two projects that use Python 2.7.11 and one requires a python module with a specific version: networkx==1.0 . Another project the same module greater than version 2.0 networkx>2.0 . How can we simultaneously work on both projects on the same system? virtualenv is a python tool for creating isolated python environments (also known as virtualenvs; The usage tends to be specific for python virtual environments and is short for 'virtual environment'). You can create a virtualenv for every project that you do - and these can be used to ensure that when you update or install modules for a given project that they do not interfere with each other. We won't be using virtualenv directly, but instead will the pyenv flavor of virtualenvs. pyenv-virtualenv is a tool that can create virtual environments that operate similar to the way pyenv python environments do. You can create virtualenvs that act globally or you can create virtualenvs that are local to a specific directory. To create a pyenv-virtualenv you must provide a base python environment that you have installed and a name for the environment. For example, below the python environment is 2.7.11 and the name of the environment is c_elegans_project : pyenv virtualenv 2.7.11 c_elegans_project Then you can set that virtualenv to a local directory using: mkdir c_elegans_project cd c_elegans_project pyenv local c_elegans_project Notice that the folder name is the same as the virtualenv. This can be a good idea for clarity. You can see a list of python versions and virtual environments by typing: pyenv versions Output: 2.7.11 * 2.7.11/envs/c_elegans_project 3.6.0 Virtual environments are designated as <version>/envs/<name> . Now we can also install the module we need for that specific project. pyenv installs a python-specific package manager called pip : pip install networkx==1.0 Notice that at this point we have isolated independent environments that do not interfere with one another. If we leave them alone for a year we should be able to come back and the software environment should be the same... and if they work with data they should reproduce the identical result. tl;dr - pyenv-virtualenv can define custom isolated python environments and set them the same way pyenv sets python installations.","title":"pyenv-virtualenv"},{"location":"archive/quest-andersen-lab-env/#conda","text":"Conda Documentation Thus far we've managed to install multiple versions of python and figured out how to use them in independent, isolated environments. But we obviously use a lot more than just Python. We need to be able to install things like bcftools to work with variant data. We need to be able to install Java packages, and R packages, and all kinds of software. Conda can help us with this. Conda is a language-agnostic package manager. That means it can be used to install packages from python, R, Java, C/C++, etc. For example, the command below will install R and the R Tidyverse . conda install r-tidyverse","title":"conda"},{"location":"archive/quest-andersen-lab-env/#conda_integrates_with_pyenv_and_pyenv-virtualenv","text":"Important for our purposes, conda can be installed by pyenv . When I stated earlier that pyenv is used to install and manage versions of python I ommitted the fact that it can also install conda to avoid confusion. conda is not a version of python , but it is written in python, and it can be used to install python modules in addition to lots of other stuff. Similar to python virtualenvs, isolated conda environments can be created as was demonstrated above. You would run something like: pyenv install miniconda3-4.3.27 pyenv virtualenv miniconda3-4.3.27 my_new_conda_env pyenv local my_new_conda_env conda install bcftools pip install requests # This version of pip is specific gto What is great about these environments is that we can create custom software environments to suit any project. We can install R packages, python modules, C/C++ executables, and more.","title":"conda integrates with pyenv and pyenv-virtualenv"},{"location":"archive/quest-andersen-lab-env/#pyenv_environments_are_inherited","text":"We can now install custom environments for each project. Even better, pyenv allows you to specify multiple environments together. Consider the example in this diagram: There are two environments defined: env_1 bcftools v1.6 bedtools v1.2 R-tidyverse 1.0 env_2 bcftools v1.6 vcf-kit v1.6 Those environments on their own appear in blue above. If we were to use the following command to specify these environments: pyenv local env_2 env_1 base_version We would produce the green environment in the diagram. What you are seeing are two environments being combined. However, the order you specify them in matters. Notice that bcftools v1.7 is used and not bcftools v1.6 . This is because env_2 is searched first when commands libraries are retrieved. After pulling all the libraries in env_2 , the combined library will inherit anything remaining in env_1 . This allows to easily combine environments for analysis. Remember that each of these virtual environments is based on a version of python or conda. But you can also put a plain version of python or conda as your last environment. This is useful when using conda because the conda command does not inherit from conda-based virtualenvs.","title":"pyenv environments are inherited"},{"location":"archive/quest-andersen-lab-env/#andersen-lab-env_structure","text":"The anderse-lab-env uses two conda environments: primary - The primary environment contains the majority of the tools required for performing sequence analysis. py2 - For programs that require python 2. You can create and use your own conda environments for projects, but these are designed to be comprehensive.","title":"andersen-lab-env structure"},{"location":"archive/quest-andersen-lab-env/#installing_the_andersen-lab-env","text":"If you are on Quest Edit your .bashrc file to contain the following: # .bashrc export PKG_CONFIG_PATH=/usr/share/pkgconfig:$PKG_CONFIG_PATH # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi Installation The andersen-lab-env can be installed by running the following command: cd && if cd ~/andersen-lab-env; then git pull; else git clone http://www.github.com/andersenlab/andersen-lab-env; fi bash setup.sh This command will clone the repo, cd into it, and run the setup.sh script. When you run the setup.sh script it will install the latest version of the primary and py2 environments, and it will assign these environments globally as: pyenv primary-(date) py2-(date) minicondax-x.x.x You should not need to change your global environment Note If you have existing versions of the primary and py2 environments installed they will remain. You can set them locally at the project level if necessary.","title":"Installing the andersen-lab-env"},{"location":"archive/quest-andersen-lab-env/#andersen-lab-env_git_structure","text":"The andersen-lab-env is used to manage and version the software environments. The repo has the following structure: \u251c\u2500\u2500 Brewfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 primary.environment.yaml \u251c\u2500\u2500 py2.environment.yaml \u251c\u2500\u2500 rebuild_envs.sh \u251c\u2500\u2500 setup.sh \u251c\u2500\u2500 user_bash_profile.sh \u2514\u2500\u2500 versions \u251c\u2500\u2500 Linux.2018-03-08.primary.yaml \u251c\u2500\u2500 Linux.2018-03-08.py2.yaml \u251c\u2500\u2500 Mac.2018-03-08.primary.yaml \u2514\u2500\u2500 Mac.2018-03-08.py2.yaml primary.environment.yaml - base primary environment. This lists the software to be installed, but not specific versions of it. py2.environment.yaml - The base py2 environment. This lists the software to be installed, but not specific versions of it. Brewfile - Defines the software software-dependencies to be installed when running setup.sh rebuild_envs - Used to construct new versions of the environments. Note that you need to do this on a Linux and Mac computer. user_bash_profile.sh - The optional bash profile that is created with setup.sh . versions/ - Software required for each environment with all dependencies. Versioned in git and by platform and date.","title":"andersen-lab-env git structure"},{"location":"archive/quest-andersen-lab-env/#adding_new_software","text":"When you want to add new software a new version of the primary and py2 environments should be created. You must modify the primary.environment.yaml or py2.environment.yaml files and build the files you see in the versions folder which define the required software by specific version and includes all the dependencies. bash rebuild_envs.sh This will output two new versions specific to your platform in the versions/ folder. You must run this script and generate the appropriate version files on both Mac and Linux. Commit the updated versions to git. Other users can then install them by running the command in installing the andersen-lab-env","title":"adding new software"},{"location":"archive/sample-sheets/","text":"Sample Sheets \u00b6 Sample Sheets Creating sample sheets wi-nf and concordance-nf pipelines nil-ril-nf Sample-Sheet Format Absolute vs. relative paths The wi-nf , concordance-nf , and nil-ril-nf pipelines all make use of sample sheets. Sample sheets specify which fastqs belong to a given strain or isotype. Creating sample sheets \u00b6 wi-nf and concordance-nf pipelines \u00b6 For the wi-nf and concordance-nf pipelines, sample-sheets are generated using the file located (in each of these repos) in the scripts/construct_sample_sheet.sh . Importantly, these scripts are almost identical except that the concordance-nf pipeline constructs a sample sheet for strains whereas the wi-nf sample sheet is for isotypes . When adding new sequence data you need to update these scripts. Note The nomenclature regarding sample sheets and scripts was changed in March of 2018 to make it clearer. You may encounter older files with the following names that correspond to the newer names SM_sample_sheet --> sample_sheet.tsv construct_SM_sheet.sh --> construct_sample_sheet.tsv nil-ril-nf \u00b6 For the nil-ril-nf pipelines you must manually create the sample sheets according to the format below. Sample-Sheet Format \u00b6 The sample sheet defines which FASTQs belong to which strain/isotype and specifies additional information regarding a sample. Additional information specfieid are the FASTQ ID (a unique identifier for a FASTQ-pair), Sequencing POOL (which defines the group of samples that were sequenced together), the locations of the FASTQs, and the sequencing folder. Note Internally, the 'sequencing pool' information as treated as the DNA-library identifier by BWA ( LB ). Our lab processes sequence data such that the pool name uniquely identifies DNA-libraries for each sample. Sample sheet structure All columns are required. Sample Identifier - How FASTQs should be grouped in the pipeline. Usually this is by strain or isotype. FASTQ ID - A unique ID for the FASTQ pair. It must be unique for all sequencing runs defined in the sample sheet. Sequencing pool - The sequencing pool is often defined arbitrarily. It refers to the set of strains that were sequenced together. It acts as an identifer of the DNA library within the pipelines. FASTQ1 - A relative or absolute path to the first FASTQ. FASTQ2 - A relative or absolute path to the second FASTQ. Sequencing Folder - This column is provided for informational purposes. It generally refers to the name of the folder containing the FASTQs. Example AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set Notice that the file does not include a header . The table with corresponding header included below look like this: Sample Identifeir FASTQ ID Sequencing Pool fastq-1-path fastq-2-path sequencing_folder AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set Absolute vs. relative paths \u00b6 When constructing the sample sheet for the wi-nf and concordance-nf pipelines you are required to use the absolute paths to each FASTQ. The nil-ril-nf pipeline can use relative paths to FASTQs by specifying the --fq_file_prefix option to the parent directory containing FASTQs.","title":"Sample Sheets"},{"location":"archive/sample-sheets/#sample_sheets","text":"Sample Sheets Creating sample sheets wi-nf and concordance-nf pipelines nil-ril-nf Sample-Sheet Format Absolute vs. relative paths The wi-nf , concordance-nf , and nil-ril-nf pipelines all make use of sample sheets. Sample sheets specify which fastqs belong to a given strain or isotype.","title":"Sample Sheets"},{"location":"archive/sample-sheets/#creating_sample_sheets","text":"","title":"Creating sample sheets"},{"location":"archive/sample-sheets/#wi-nf_and_concordance-nf_pipelines","text":"For the wi-nf and concordance-nf pipelines, sample-sheets are generated using the file located (in each of these repos) in the scripts/construct_sample_sheet.sh . Importantly, these scripts are almost identical except that the concordance-nf pipeline constructs a sample sheet for strains whereas the wi-nf sample sheet is for isotypes . When adding new sequence data you need to update these scripts. Note The nomenclature regarding sample sheets and scripts was changed in March of 2018 to make it clearer. You may encounter older files with the following names that correspond to the newer names SM_sample_sheet --> sample_sheet.tsv construct_SM_sheet.sh --> construct_sample_sheet.tsv","title":"wi-nf and concordance-nf pipelines"},{"location":"archive/sample-sheets/#nil-ril-nf","text":"For the nil-ril-nf pipelines you must manually create the sample sheets according to the format below.","title":"nil-ril-nf"},{"location":"archive/sample-sheets/#sample-sheet_format","text":"The sample sheet defines which FASTQs belong to which strain/isotype and specifies additional information regarding a sample. Additional information specfieid are the FASTQ ID (a unique identifier for a FASTQ-pair), Sequencing POOL (which defines the group of samples that were sequenced together), the locations of the FASTQs, and the sequencing folder. Note Internally, the 'sequencing pool' information as treated as the DNA-library identifier by BWA ( LB ). Our lab processes sequence data such that the pool name uniquely identifies DNA-libraries for each sample. Sample sheet structure All columns are required. Sample Identifier - How FASTQs should be grouped in the pipeline. Usually this is by strain or isotype. FASTQ ID - A unique ID for the FASTQ pair. It must be unique for all sequencing runs defined in the sample sheet. Sequencing pool - The sequencing pool is often defined arbitrarily. It refers to the set of strains that were sequenced together. It acts as an identifer of the DNA library within the pipelines. FASTQ1 - A relative or absolute path to the first FASTQ. FASTQ2 - A relative or absolute path to the second FASTQ. Sequencing Folder - This column is provided for informational purposes. It generally refers to the name of the folder containing the FASTQs. Example AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set Notice that the file does not include a header . The table with corresponding header included below look like this: Sample Identifeir FASTQ ID Sequencing Pool fastq-1-path fastq-2-path sequencing_folder AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set","title":"Sample-Sheet Format"},{"location":"archive/sample-sheets/#absolute_vs_relative_paths","text":"When constructing the sample sheet for the wi-nf and concordance-nf pipelines you are required to use the absolute paths to each FASTQ. The nil-ril-nf pipeline can use relative paths to FASTQs by specifying the --fq_file_prefix option to the parent directory containing FASTQs.","title":"Absolute vs. relative paths"}]}