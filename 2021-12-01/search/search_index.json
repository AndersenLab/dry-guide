{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Andersen Lab Dry Guide \u00b6 The Dry Guide details the computational infrastructure and tasks used in the Andersen Lab. Looking for more resources? Check out our Andersen Lab code club GitHub repo for more coding tips and tricks! Other links: \u00b6 Andersen Lab website CeNDR","title":"Home"},{"location":"#welcome_to_the_andersen_lab_dry_guide","text":"The Dry Guide details the computational infrastructure and tasks used in the Andersen Lab. Looking for more resources? Check out our Andersen Lab code club GitHub repo for more coding tips and tricks!","title":"Welcome to the Andersen Lab Dry Guide"},{"location":"#other_links","text":"Andersen Lab website CeNDR","title":"Other links:"},{"location":"adding-seq-data/","text":"Adding NIL sequence data to lab site \u00b6 Adding NIL sequence data to lab site Basic Commands To Know Prior Your Sequencing Data Step By Step Instructions Viewing your results This section is for adding genomic sequencing data (.tsv files) onto the existing dataset provided and displayed on the NILs browser page on Andersenlab.org . Basic Commands To Know Prior \u00b6 You should freshen up on the following terminal commands. cd - change directories rm - delete files cp - make a copy git Your Sequencing Data \u00b6 You will use the gt_hmm_fill.tsv file output from the nil-ril-nf pipeline for this step. Important In order for your sequencing data to be properly added, it is important to make sure that there are no empty/additional lines located at the bottom of your .tsv file. What you do not want What you do want Once your file has no empty lines at the bottom, save the file and move onto the next instructions below. Step By Step Instructions \u00b6 Clone the andersenlab.org repo git clone https://github.com/AndersenLab/andersenlab.github.io.git [Optional] Create a new branch. This is a good idea if you are newer to github and want to make sure you don't break the lab website by doing something weird. Add your .tsv file into the pages folder of your Andersenlab github directory. Open terminal and use the cd command to change directories into the pages folder in your Andersenlab github directory. If you did everything correctly, when you type ls into your terminal, it should look something like this. Then run the following commands in your terminal (while still in your pages directory): # create a new file called 'copy.tsv' with your data cp yourFileName.tsv copy.tsv # run python script which will add your data to full dataset python addDataTogt_hmm.tsv.py After running the above commands, your sequencing data has now been added to the existing NILs dataset on Andersenlab.org. You can now remove your .tsv from the pages directory by using the rm command in your terminal. rm yourFileName.tsv Finally, commit your changes and push your code to update the Andersenlab github. [Optional] If you created a new branch, you need to merge this new branch back into the master branch for changes to take effect. Viewing your results \u00b6 Once all changes are pushed, you should be able to view your NIL genotypes in the NIL browser shiny app .","title":"Adding NIL sequence data to lab site"},{"location":"adding-seq-data/#adding_nil_sequence_data_to_lab_site","text":"Adding NIL sequence data to lab site Basic Commands To Know Prior Your Sequencing Data Step By Step Instructions Viewing your results This section is for adding genomic sequencing data (.tsv files) onto the existing dataset provided and displayed on the NILs browser page on Andersenlab.org .","title":"Adding NIL sequence data to lab site"},{"location":"adding-seq-data/#basic_commands_to_know_prior","text":"You should freshen up on the following terminal commands. cd - change directories rm - delete files cp - make a copy git","title":"Basic Commands To Know Prior"},{"location":"adding-seq-data/#your_sequencing_data","text":"You will use the gt_hmm_fill.tsv file output from the nil-ril-nf pipeline for this step. Important In order for your sequencing data to be properly added, it is important to make sure that there are no empty/additional lines located at the bottom of your .tsv file. What you do not want What you do want Once your file has no empty lines at the bottom, save the file and move onto the next instructions below.","title":"Your Sequencing Data"},{"location":"adding-seq-data/#step_by_step_instructions","text":"Clone the andersenlab.org repo git clone https://github.com/AndersenLab/andersenlab.github.io.git [Optional] Create a new branch. This is a good idea if you are newer to github and want to make sure you don't break the lab website by doing something weird. Add your .tsv file into the pages folder of your Andersenlab github directory. Open terminal and use the cd command to change directories into the pages folder in your Andersenlab github directory. If you did everything correctly, when you type ls into your terminal, it should look something like this. Then run the following commands in your terminal (while still in your pages directory): # create a new file called 'copy.tsv' with your data cp yourFileName.tsv copy.tsv # run python script which will add your data to full dataset python addDataTogt_hmm.tsv.py After running the above commands, your sequencing data has now been added to the existing NILs dataset on Andersenlab.org. You can now remove your .tsv from the pages directory by using the rm command in your terminal. rm yourFileName.tsv Finally, commit your changes and push your code to update the Andersenlab github. [Optional] If you created a new branch, you need to merge this new branch back into the master branch for changes to take effect.","title":"Step By Step Instructions"},{"location":"adding-seq-data/#viewing_your_results","text":"Once all changes are pushed, you should be able to view your NIL genotypes in the NIL browser shiny app .","title":"Viewing your results"},{"location":"b1059/","text":"Data storage on b1059 \u00b6 Data storage on b1059 analysis data docs modules projects software to_be_deleted workflows In 2021, the lab underwent a large restructuring of the data storage on b1059. The goal of this restructure was 1) to make it easy and clear to find data of a specific type and 2) to allow the expansion from mostly C. elegans data to other species, particularly C. briggsae and C. tropicalis . b1059 is broken down into several main parts, here I will describe them all: analysis \u00b6 This directory contains non-data output and analyses from general lab pipelines (i.e. wi-gatk or alignment-nf ). It is organized by pipeline and then by analysis type-date. If you are running these pipelines (including nil-ril-nf ) it is important that you move your analysis folder here once complete (and out of your personal folder) so that everyone has access to the results. data \u00b6 This directory contains all the lab data -- split by species. The goal is to create the same file structure across all species. This not only makes it easy to find the file you are looking for, but also makes it easy to script file locations. A general example of a species' file structure is below: \u251c\u2500\u2500 genomes \u2502 \u251c\u2500\u2500 genetic_map \u2502 \u2502 \u2514\u2500\u2500 {chr}.map \u2502 \u251c\u2500\u2500 {project_ID} \u2502 \u2502 \u2514\u2500\u2500 {WS_build} \u2502 \u2502 \u251c\u2500\u2500 *.genome.fa* \u2502 \u2502 \u251c\u2500\u2500 csq \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Length.tsv \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Scores.tsv \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.csq.gff3.gz \u2502 \u2502 \u251c\u2500\u2500 lcr \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.dust.bed.gz \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.repeat_masker.bed.gz \u2502 \u2502 \u2514\u2500\u2500 snpeff \u2502 \u2502 \u251c\u2500\u2500 {species}.{project}.{ws_build} \u2502 \u2502 \u2502 \u251c\u2500\u2500 genes.gtf.gz \u2502 \u2502 \u2502 \u251c\u2500\u2500 sequences.fa \u2502 \u2502 \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin \u2502 \u2502 \u2514\u2500\u2500 snpEff.config \u2502 \u2514\u2500\u2500 WI_PacBio_assemblies \u251c\u2500\u2500 RIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 NIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 WI \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u251c\u2500\u2500 _bam_not_for_cendr \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 concordance \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 divergent_regions \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u251c\u2500\u2500 dna \u2502 \u2502 \u2502 \u251c\u2500\u2500 _fastq_not_for_cendr \u2502 \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2502 \u2514\u2500\u2500 rna \u2502 \u2502 \u2514\u2500\u2500 {project} \u2502 \u251c\u2500\u2500 haplotype \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tree \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tracks \u2502 \u2514\u2500\u2500 vcf \u2502 \u251c\u2500\u2500 strain_vcf \u2502 \u2502 \u2514\u2500\u2500 {strain}.{date}.vcf.gz \u2502 \u2514\u2500\u2500 WI.{date}.*.vcf.gz \u2514\u2500\u2500 {other - i.e. BSA, MUTANT, HiC, PacBio} \u2514\u2500\u2500 *currently could look like anything in here - organized by project* Note This restructure is still not 100% complete and things might continue to change as we expand our genomes and data types. If you see something that is not organized well, let Erik or Katie know. docs \u00b6 This directory should probably be removed and the docs inside stored elsewhere. Currently, shows the process for fastq SRA submission from 2019 and then again from 2021. modules \u00b6 I think this directory can maybe be deleted? projects \u00b6 Projects is where most people will spend most of their time. Each user with access to quest should create a user-specific folder (named with their name) in the projects directory. Inside your folder, you can do all your work: analyzing data, writing new scripts, temporary storage, etc. Note Please be aware of how much space you are using in your personal folder. Of course some projects might require more space than others, and some projects require a lot of temporary space that can be deleted once completed. However, if you find that you have > 500-1000 GB of used space in your folder, please take a look if there is any unused data or scripts. Either way, it is good practice to clean out your folder every few months to avoid storage pile-up. You can check how much space you are using with du -hs * or ask Katie. software \u00b6 General This is a great location to download any software tools or packages that you need to use that you cannot access with Quest or create a conda environment for. Especially important if they are software packages that other people in the lab might also use, as it is a shared space. Conda environments Inside the directory conda_envs you can find all the shared lab conda environments necessary for running certain nextflow pipelines. If you create a shared conda environment, it is important that you update the README.md with the code you used to create the environment for reproducible purposes . You can create a conda environment in this directory with: conda create -p /projects/b1059/software/conda_envs/<name_of_env> You can also update your ~/.condarc file to point to this directory so that you can easily load conda environments just by the name (i.e. source activate nf20_env instead of source activate /projects/b1059/software/conda_envs/nf20_env ): channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /projects/b1059/software/conda_envs/ Important It is very important that you do not update any packages or software while running a shared conda environment. This is especially an issue with Nextflow and the nf20_env environment. Updating nextflow whilie running this environment will update the version in the environment -- and it needs to be v20.0 for many of the pipelines to run successfully If you want to see what all is loaded in a particular conda environment, or re-create an environment, you can use: # make an exact copy conda create --clone py35 --name py35-2 # list all packages and versions conda list --explicit > bio-env.txt # list a history of revisions conda list --revisions # go back to a previous revision conda install --revision 2 # create environment from file conda env create --file bio-env.txt Also, check out this cheat sheet or our conda page for more. Note Most of the nextflow pipelines are written using module load python/anaconda3.6; source activate nf20_env , so if you are having trouble running with conda, try loading the right environment first. R libraries Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. One way we have gotten around this is by installing the proper versions of R packages to a shared location ( /projects/b1059/software/R_lib_3.6.0 ). Check out the Using R on Quest section of the R page to learn more about installing a package to this folder and using it in a script. Important It is very important that you do not update any packages in this location unless absolutely necessary and proceed with extreme caution (especially if it is tidyverse !!!). Updating packages could break pipelines that rely on them. Docker images This is not available yet, but might be a good idea to store shared docker images here. Note: can change the singularity_cache parameter in nextflow scripts to store images here in the future . to_be_deleted \u00b6 This directory is a temporary holding place for large files/data that can be deleted. Think of it as a soft-trash bin. If, after a few months of living in to_be_deleted , it is likely the data in fact can be deleted without being missed. This was a temporary solution for the large restructure in 2021 and can likely be removed in the future. workflows \u00b6 This directory is being phased out. Workflows will now be hosted on GitHub and any lab members who wish to run a shared workflow should run remotely (if nextflow script) or clone the repo into their personal folders.","title":"Data on b1059"},{"location":"b1059/#data_storage_on_b1059","text":"Data storage on b1059 analysis data docs modules projects software to_be_deleted workflows In 2021, the lab underwent a large restructuring of the data storage on b1059. The goal of this restructure was 1) to make it easy and clear to find data of a specific type and 2) to allow the expansion from mostly C. elegans data to other species, particularly C. briggsae and C. tropicalis . b1059 is broken down into several main parts, here I will describe them all:","title":"Data storage on b1059"},{"location":"b1059/#analysis","text":"This directory contains non-data output and analyses from general lab pipelines (i.e. wi-gatk or alignment-nf ). It is organized by pipeline and then by analysis type-date. If you are running these pipelines (including nil-ril-nf ) it is important that you move your analysis folder here once complete (and out of your personal folder) so that everyone has access to the results.","title":"analysis"},{"location":"b1059/#data","text":"This directory contains all the lab data -- split by species. The goal is to create the same file structure across all species. This not only makes it easy to find the file you are looking for, but also makes it easy to script file locations. A general example of a species' file structure is below: \u251c\u2500\u2500 genomes \u2502 \u251c\u2500\u2500 genetic_map \u2502 \u2502 \u2514\u2500\u2500 {chr}.map \u2502 \u251c\u2500\u2500 {project_ID} \u2502 \u2502 \u2514\u2500\u2500 {WS_build} \u2502 \u2502 \u251c\u2500\u2500 *.genome.fa* \u2502 \u2502 \u251c\u2500\u2500 csq \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Length.tsv \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.AA_Scores.tsv \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.csq.gff3.gz \u2502 \u2502 \u251c\u2500\u2500 lcr \u2502 \u2502 \u2502 \u251c\u2500\u2500 *.dust.bed.gz \u2502 \u2502 \u2502 \u2514\u2500\u2500 *.repeat_masker.bed.gz \u2502 \u2502 \u2514\u2500\u2500 snpeff \u2502 \u2502 \u251c\u2500\u2500 {species}.{project}.{ws_build} \u2502 \u2502 \u2502 \u251c\u2500\u2500 genes.gtf.gz \u2502 \u2502 \u2502 \u251c\u2500\u2500 sequences.fa \u2502 \u2502 \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin \u2502 \u2502 \u2514\u2500\u2500 snpEff.config \u2502 \u2514\u2500\u2500 WI_PacBio_assemblies \u251c\u2500\u2500 RIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 NIL \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {project_analysis} \u251c\u2500\u2500 WI \u2502 \u251c\u2500\u2500 alignments \u2502 \u2502 \u251c\u2500\u2500 _bam_not_for_cendr \u2502 \u2502 \u2514\u2500\u2500 {strain}.bam \u2502 \u251c\u2500\u2500 concordance \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 divergent_regions \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 fastq \u2502 \u2502 \u251c\u2500\u2500 dna \u2502 \u2502 \u2502 \u251c\u2500\u2500 _fastq_not_for_cendr \u2502 \u2502 \u2502 \u2514\u2500\u2500 {strain}.*.fastq.gz \u2502 \u2502 \u2514\u2500\u2500 rna \u2502 \u2502 \u2514\u2500\u2500 {project} \u2502 \u251c\u2500\u2500 haplotype \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tree \u2502 \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u2514\u2500\u2500 variation \u2502 \u2514\u2500\u2500 {cendr_release_date} \u2502 \u251c\u2500\u2500 tracks \u2502 \u2514\u2500\u2500 vcf \u2502 \u251c\u2500\u2500 strain_vcf \u2502 \u2502 \u2514\u2500\u2500 {strain}.{date}.vcf.gz \u2502 \u2514\u2500\u2500 WI.{date}.*.vcf.gz \u2514\u2500\u2500 {other - i.e. BSA, MUTANT, HiC, PacBio} \u2514\u2500\u2500 *currently could look like anything in here - organized by project* Note This restructure is still not 100% complete and things might continue to change as we expand our genomes and data types. If you see something that is not organized well, let Erik or Katie know.","title":"data"},{"location":"b1059/#docs","text":"This directory should probably be removed and the docs inside stored elsewhere. Currently, shows the process for fastq SRA submission from 2019 and then again from 2021.","title":"docs"},{"location":"b1059/#modules","text":"I think this directory can maybe be deleted?","title":"modules"},{"location":"b1059/#projects","text":"Projects is where most people will spend most of their time. Each user with access to quest should create a user-specific folder (named with their name) in the projects directory. Inside your folder, you can do all your work: analyzing data, writing new scripts, temporary storage, etc. Note Please be aware of how much space you are using in your personal folder. Of course some projects might require more space than others, and some projects require a lot of temporary space that can be deleted once completed. However, if you find that you have > 500-1000 GB of used space in your folder, please take a look if there is any unused data or scripts. Either way, it is good practice to clean out your folder every few months to avoid storage pile-up. You can check how much space you are using with du -hs * or ask Katie.","title":"projects"},{"location":"b1059/#software","text":"General This is a great location to download any software tools or packages that you need to use that you cannot access with Quest or create a conda environment for. Especially important if they are software packages that other people in the lab might also use, as it is a shared space. Conda environments Inside the directory conda_envs you can find all the shared lab conda environments necessary for running certain nextflow pipelines. If you create a shared conda environment, it is important that you update the README.md with the code you used to create the environment for reproducible purposes . You can create a conda environment in this directory with: conda create -p /projects/b1059/software/conda_envs/<name_of_env> You can also update your ~/.condarc file to point to this directory so that you can easily load conda environments just by the name (i.e. source activate nf20_env instead of source activate /projects/b1059/software/conda_envs/nf20_env ): channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /projects/b1059/software/conda_envs/ Important It is very important that you do not update any packages or software while running a shared conda environment. This is especially an issue with Nextflow and the nf20_env environment. Updating nextflow whilie running this environment will update the version in the environment -- and it needs to be v20.0 for many of the pipelines to run successfully If you want to see what all is loaded in a particular conda environment, or re-create an environment, you can use: # make an exact copy conda create --clone py35 --name py35-2 # list all packages and versions conda list --explicit > bio-env.txt # list a history of revisions conda list --revisions # go back to a previous revision conda install --revision 2 # create environment from file conda env create --file bio-env.txt Also, check out this cheat sheet or our conda page for more. Note Most of the nextflow pipelines are written using module load python/anaconda3.6; source activate nf20_env , so if you are having trouble running with conda, try loading the right environment first. R libraries Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. One way we have gotten around this is by installing the proper versions of R packages to a shared location ( /projects/b1059/software/R_lib_3.6.0 ). Check out the Using R on Quest section of the R page to learn more about installing a package to this folder and using it in a script. Important It is very important that you do not update any packages in this location unless absolutely necessary and proceed with extreme caution (especially if it is tidyverse !!!). Updating packages could break pipelines that rely on them. Docker images This is not available yet, but might be a good idea to store shared docker images here. Note: can change the singularity_cache parameter in nextflow scripts to store images here in the future .","title":"software"},{"location":"b1059/#to_be_deleted","text":"This directory is a temporary holding place for large files/data that can be deleted. Think of it as a soft-trash bin. If, after a few months of living in to_be_deleted , it is likely the data in fact can be deleted without being missed. This was a temporary solution for the large restructure in 2021 and can likely be removed in the future.","title":"to_be_deleted"},{"location":"b1059/#workflows","text":"This directory is being phased out. Workflows will now be hosted on GitHub and any lab members who wish to run a shared workflow should run remotely (if nextflow script) or clone the repo into their personal folders.","title":"workflows"},{"location":"backup/","text":"Backup data \u00b6 Backup files to google cloud \u00b6 1. Download and setup google cloud SDK - only need to do once # download from google curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-325.0.0-linux-x86_64.tar.gz # un-tar tar -xf google-cloud-sdk-325.0.0-linux-x86_64.tar.gz # add to path ./google-cloud-sdk/install.sh # initialize and authenticate gcloud init # follow prompts 2. Add files from QUEST folder to google cloud bucket You can see the structure of the google buckets by going here . Most things are currently in the elegansvariation.org bucket. # copy all files in current directory to specified bucket gsutil cp * gs://<YOUR_BUCKET_NAME> # copy all files in parallel (good for multiple files) gsutil -m cp * gs://<YOUR_BUCKET_NAME> # view list of files in bucket gsutil ls gs://<YOUR_BUCKET_NAME> # example - move all vcf files to the variation folder under 20210121 release gsutil -m cp * gs://elegansvarition.org/releases/20210121/variation Note We store .bam and .vcf files on google because they are used by CeNDR. The .bam files are not separated by release but the .vcf files (and accompanying files for a CeNDR release) are. Local Backup \u00b6 We also store all .fastq files locally in duplicate. If anything ever happens to any downstream data we can always recreate it with the original FASTQ files. This step is extremely important . A list of all data and which hardrive it is backed up on can be found here . The main copies of all data can be found on Pangolin, Armadillo, Raven, Karasu, and Turkey Choose a hardrive that has enough space for the data you need to back up and plug it in to your computer Change directories on your local computer into the hardrive cd /Volumes/{name_of_harddrive}/{path} Sync data from QUEST with Rsync -avh <netid>@quest.northwestern.edu:<path_to_folder_quest> . . Don't forget the '.'!!! When finished, check that the total file size is the same on the hard drive and on QUEST, you can always re-run the Rsync command to verify it is complete. Repeat with the second hard drive Complete the data backup google sheet with your new backup. Adding FASTQ to SRA project \u00b6 Checkout this guide for how to upload data to SRA. This should be done with wild isolate FASTQ files after each new CeNDR release. The SRA submission ID might need to be cited in publications.","title":"Backup"},{"location":"backup/#backup_data","text":"","title":"Backup data"},{"location":"backup/#backup_files_to_google_cloud","text":"1. Download and setup google cloud SDK - only need to do once # download from google curl -O https://dl.google.com/dl/cloudsdk/channels/rapid/downloads/google-cloud-sdk-325.0.0-linux-x86_64.tar.gz # un-tar tar -xf google-cloud-sdk-325.0.0-linux-x86_64.tar.gz # add to path ./google-cloud-sdk/install.sh # initialize and authenticate gcloud init # follow prompts 2. Add files from QUEST folder to google cloud bucket You can see the structure of the google buckets by going here . Most things are currently in the elegansvariation.org bucket. # copy all files in current directory to specified bucket gsutil cp * gs://<YOUR_BUCKET_NAME> # copy all files in parallel (good for multiple files) gsutil -m cp * gs://<YOUR_BUCKET_NAME> # view list of files in bucket gsutil ls gs://<YOUR_BUCKET_NAME> # example - move all vcf files to the variation folder under 20210121 release gsutil -m cp * gs://elegansvarition.org/releases/20210121/variation Note We store .bam and .vcf files on google because they are used by CeNDR. The .bam files are not separated by release but the .vcf files (and accompanying files for a CeNDR release) are.","title":"Backup files to google cloud"},{"location":"backup/#local_backup","text":"We also store all .fastq files locally in duplicate. If anything ever happens to any downstream data we can always recreate it with the original FASTQ files. This step is extremely important . A list of all data and which hardrive it is backed up on can be found here . The main copies of all data can be found on Pangolin, Armadillo, Raven, Karasu, and Turkey Choose a hardrive that has enough space for the data you need to back up and plug it in to your computer Change directories on your local computer into the hardrive cd /Volumes/{name_of_harddrive}/{path} Sync data from QUEST with Rsync -avh <netid>@quest.northwestern.edu:<path_to_folder_quest> . . Don't forget the '.'!!! When finished, check that the total file size is the same on the hard drive and on QUEST, you can always re-run the Rsync command to verify it is complete. Repeat with the second hard drive Complete the data backup google sheet with your new backup.","title":"Local Backup"},{"location":"backup/#adding_fastq_to_sra_project","text":"Checkout this guide for how to upload data to SRA. This should be done with wild isolate FASTQ files after each new CeNDR release. The SRA submission ID might need to be cited in publications.","title":"Adding FASTQ to SRA project"},{"location":"bash/","text":"Command line \u00b6 Command line Basic Commands More Advanced Good Guides grep awk Rearranging columns Filtering based on criteria bcftools Screen Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with this introduction to bash . Also check out this cheatsheet Basic Commands \u00b6 You should familiarize yourself with the following commands. alias - create a shortcut for a command cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted. More Advanced \u00b6 You should learn these once you have the basics down. git - version control awk - file manipulation; Filtering; Rearranging columns sed - quick find/replace Good Guides \u00b6 Below are some good guides for various bash utilities. grep \u00b6 using grep with regular expressions another regex grep guide awk \u00b6 awk guide awk by example - hundreds of examples Rearranging columns \u00b6 cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column. Filtering based on criteria \u00b6 Print only lines that start with a comment (#) character cat example.tsv awk '$0 ~ \"^#\" { print }' bcftools \u00b6 bcftools view bcftools view - view VCF bcftools view -h - view only header of VCF bcftools view -H - view VCF without header bcftools view -s CB4856,XZ1516,ECA701 - subset vcf for only these three samples bcftools view -S sample_file.txt - subset vcf for only samples listed in sample_file.txt bcftools view -r III:1-800000 - subset vcf for a region of interest can also just use -r III to get entire chromosome bcftools view -R regions.txt - subset vcf for a region(s) of interest in the regions.txt file bcftools query bcftools query -l - print out list of samples in vcf Print out contents of vcf in specified format (i.e. tsv): bcftools query -f '%CHROM\\t%POS\\t%REF\\t%ALT[\\t%SAMPLE=%GT]\\n' <vcf> > out.tsv Output of above line of code: bcftools query -i GT==\"alt\" - keep rows that include a tag (like a filter) bcftools query -e GT==\"ref\" - remove rows that include a tag Note bcftools query -i/e are not necessarily opposites. For example, if you have three genotype options (REF, ALT, or NA), including only ALT calls is different than exluding only REF calls... For more, check out the bcftools manual and this cheatsheet Screen \u00b6 Screen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running nextflow because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection. Screen basics","title":"Command Line"},{"location":"bash/#command_line","text":"Command line Basic Commands More Advanced Good Guides grep awk Rearranging columns Filtering based on criteria bcftools Screen Bash is the default unix shell on Mac OS and most Linux operating systems. Many bioinformatic programs are run using the command line, so becoming familiar with Bash is important. Start with this introduction to bash . Also check out this cheatsheet","title":"Command line"},{"location":"bash/#basic_commands","text":"You should familiarize yourself with the following commands. alias - create a shortcut for a command cat - concatenate files zcat - concatenate zipped files cd - change directories curl - download files echo - print strings export - Add a variable to the global environment so that they get passed on to child processes. grep - filter by pattern egrep - filter by regex rm - delete files sudo - run as an administrator sort - sorts files source - runs a file ssh - connect to servers which - locate files on your PATH uniq - get unique lines. File must be sorted.","title":"Basic Commands"},{"location":"bash/#more_advanced","text":"You should learn these once you have the basics down. git - version control awk - file manipulation; Filtering; Rearranging columns sed - quick find/replace","title":"More Advanced"},{"location":"bash/#good_guides","text":"Below are some good guides for various bash utilities.","title":"Good Guides"},{"location":"bash/#grep","text":"using grep with regular expressions another regex grep guide","title":"grep"},{"location":"bash/#awk","text":"awk guide awk by example - hundreds of examples","title":"awk"},{"location":"bash/#rearranging_columns","text":"cat example.tsv | awk -f OFS=\"\\t\" '{ print $2, $3, $1 }' The line above will print the second column, the third column and finally the first column.","title":"Rearranging columns"},{"location":"bash/#filtering_based_on_criteria","text":"Print only lines that start with a comment (#) character cat example.tsv awk '$0 ~ \"^#\" { print }'","title":"Filtering based on criteria"},{"location":"bash/#bcftools","text":"bcftools view bcftools view - view VCF bcftools view -h - view only header of VCF bcftools view -H - view VCF without header bcftools view -s CB4856,XZ1516,ECA701 - subset vcf for only these three samples bcftools view -S sample_file.txt - subset vcf for only samples listed in sample_file.txt bcftools view -r III:1-800000 - subset vcf for a region of interest can also just use -r III to get entire chromosome bcftools view -R regions.txt - subset vcf for a region(s) of interest in the regions.txt file bcftools query bcftools query -l - print out list of samples in vcf Print out contents of vcf in specified format (i.e. tsv): bcftools query -f '%CHROM\\t%POS\\t%REF\\t%ALT[\\t%SAMPLE=%GT]\\n' <vcf> > out.tsv Output of above line of code: bcftools query -i GT==\"alt\" - keep rows that include a tag (like a filter) bcftools query -e GT==\"ref\" - remove rows that include a tag Note bcftools query -i/e are not necessarily opposites. For example, if you have three genotype options (REF, ALT, or NA), including only ALT calls is different than exluding only REF calls... For more, check out the bcftools manual and this cheatsheet","title":"bcftools"},{"location":"bash/#screen","text":"Screen can be used to run things in the background. It is extremely useful if you need to run things on quest without worry that they will be terminated if you log out or get kicked off. This is essential when running nextflow because pipelines can sometimes run for many hours and its likely you will be kicked off in that time or lose your connection. Screen basics","title":"Screen"},{"location":"best_practices/","text":"Andersen Lab Coding Best Practices \u00b6 These best practices are just a few of the important coding tips and tricks for reproducible research. If you have more ideas, contact Katie! General \u00b6 You should be doing most (if not all) of your analyses in ~/Dropbox/AndersenLab/LabFolders/YourName (except for QUEST, see below) This is (1) to make sure the data is backed up/saved with version history and (2) to allow other lab members to access your code/scripts when necessary Do NOT use spaces in names of files or folders. Try not to use spaces in column names too (although sometimes it is necessary for a final table output) Computers often have a hard time reading spaces and code used to ignore spaces can vary from program to program Instead, you can use _ or . or - or capitalization ( fileName.txt ) NEVER replace raw data!!!!! You should save your raw data in the rawest format, write a script to analyze it, then if you wish, save the processed data for further use. This is important because it always allows you to go back to the original raw data in case something happens Some suggested project folder structure might look like something below: Include a README.md (or README.txt ) file in each project folder to explain where the data and scripts can be found for certain analyses. Trust me, after a few years you will definitely forget\u2026 And don\u2019t forget to update the README regularly, an old README doesn\u2019t do anyone good! Either use full path names in scripts or be explicit about where the working directory is This is important to allow other people to run your code (or might even be helpful for you if you ever reorganize folders one day) Date your files, especially when you update an existing file. Write dates in YYYYMMDD format As much as possible, ensure that your processed data is \u201ctidy\u201d (see below). This doesn\u2019t work for all complex data types, but it should be a general norm to follow. Each variable must have its own column Each observation must have its own row Each value must have its own cell No color or highlighting No empty cells (fill with NA if necessary) Save data as plain text files ( .csv , .tsv , .txt etc. -- NOT .xls !!!) R \u00b6 ALWAYS use namespaces before functions from packages (i.e. dplyr::filter() instead of filter() ) This includes ggplot2 and especially dplyr !!! Some packages have functions with the same name, so adding a namespace is crucial for reproducibility. Also, this helps other people read your code by knowing which functions came from which packages When piping with Tidyverse ( %>% ), press <Enter> to go to the next line after a pipe This makes your code more readable In fact, general practices state no more than 80-100 characters per line of code EVER to increase readability QUEST \u00b6 You should be doing most (if not all) of your analyses in /projects/b1059/projects/yourName (not your home directory (i.e. /home/netid )) Important Main exception: Nextflow temporary working directories should NOT be on b1059 (it will fill us up!) but rather in the scratch space b1042 (files get automatically deleted here every 30 days). A correctly designed nextflow.config file will take care of this. Python \u00b6 [ Needs filling in from someone who uses python :) ]","title":"Best Practices"},{"location":"best_practices/#andersen_lab_coding_best_practices","text":"These best practices are just a few of the important coding tips and tricks for reproducible research. If you have more ideas, contact Katie!","title":"Andersen Lab Coding Best Practices"},{"location":"best_practices/#general","text":"You should be doing most (if not all) of your analyses in ~/Dropbox/AndersenLab/LabFolders/YourName (except for QUEST, see below) This is (1) to make sure the data is backed up/saved with version history and (2) to allow other lab members to access your code/scripts when necessary Do NOT use spaces in names of files or folders. Try not to use spaces in column names too (although sometimes it is necessary for a final table output) Computers often have a hard time reading spaces and code used to ignore spaces can vary from program to program Instead, you can use _ or . or - or capitalization ( fileName.txt ) NEVER replace raw data!!!!! You should save your raw data in the rawest format, write a script to analyze it, then if you wish, save the processed data for further use. This is important because it always allows you to go back to the original raw data in case something happens Some suggested project folder structure might look like something below: Include a README.md (or README.txt ) file in each project folder to explain where the data and scripts can be found for certain analyses. Trust me, after a few years you will definitely forget\u2026 And don\u2019t forget to update the README regularly, an old README doesn\u2019t do anyone good! Either use full path names in scripts or be explicit about where the working directory is This is important to allow other people to run your code (or might even be helpful for you if you ever reorganize folders one day) Date your files, especially when you update an existing file. Write dates in YYYYMMDD format As much as possible, ensure that your processed data is \u201ctidy\u201d (see below). This doesn\u2019t work for all complex data types, but it should be a general norm to follow. Each variable must have its own column Each observation must have its own row Each value must have its own cell No color or highlighting No empty cells (fill with NA if necessary) Save data as plain text files ( .csv , .tsv , .txt etc. -- NOT .xls !!!)","title":"General"},{"location":"best_practices/#r","text":"ALWAYS use namespaces before functions from packages (i.e. dplyr::filter() instead of filter() ) This includes ggplot2 and especially dplyr !!! Some packages have functions with the same name, so adding a namespace is crucial for reproducibility. Also, this helps other people read your code by knowing which functions came from which packages When piping with Tidyverse ( %>% ), press <Enter> to go to the next line after a pipe This makes your code more readable In fact, general practices state no more than 80-100 characters per line of code EVER to increase readability","title":"R"},{"location":"best_practices/#quest","text":"You should be doing most (if not all) of your analyses in /projects/b1059/projects/yourName (not your home directory (i.e. /home/netid )) Important Main exception: Nextflow temporary working directories should NOT be on b1059 (it will fill us up!) but rather in the scratch space b1042 (files get automatically deleted here every 30 days). A correctly designed nextflow.config file will take care of this.","title":"QUEST"},{"location":"best_practices/#python","text":"[ Needs filling in from someone who uses python :) ]","title":"Python"},{"location":"cendr/","text":"CeNDR \u00b6 CeNDR Setting up CeNDR Clone the repo Setup a python environment Download and install the gcloud-sdk Create a cendr gcloud configuration Install direnv Test flask Setup Credentials Load the database Test the site Creating a new release Uploading BAMs Uploading Release Data Bump the CeNDR version number and change-log Adding the release to the CeNDR website Adding isotype images Update the current variant datasets. Updating Publications Setting up CeNDR \u00b6 Clone the repo \u00b6 Clone the repo first. git clone http://www.github.com/andersenlab/cendr Switch to the development branch git checkout --track origin/development Setup a python environment \u00b6 Use miniconda as it will make your life much easier. The conda environment has been specified in the env.yaml file, and can be installed using: conda env create -f env.yaml Download and install the gcloud-sdk \u00b6 Install the gcloud-sdk Create a cendr gcloud configuration \u00b6 gcloud config configurations create cendr Install direnv \u00b6 direnv allows you to load a configuration file when you enter the development directory. Please read about how it works. CeNDR uses a .envrc file within the repo to set up the appropriate environmental variables. Once direnv is installed you can run direnv allow within the CeNDR repo: direnv allow Test flask \u00b6 With direnv enabled, you are nearly able to run the site locally. Run flask , and you should see the following: > flask Usage: flask [OPTIONS] COMMAND [ARGS]... A general utility script for Flask applications. Provides commands from Flask, extensions, and the application. Loads the application defined in the FLASK_APP environment variable, or from a wsgi.py file. Setting the FLASK_ENV environment variable to 'development' will enable debug mode. $ export FLASK_APP=hello.py $ export FLASK_ENV=development $ flask run Options: --version Show the flask version --help Show this message and exit. Commands: decrypt_credentials Decrypt credentials download_db Download the database (used in docker... initdb Initialize the database routes Show the routes for the app. run Run a development server. shell Runs a shell in the app context. update_credentials Update credentials update_strains Updates the strain table of the database If you do not see the full set of commands there - something is broken. Setup Credentials \u00b6 Authenticate with gcloud. Run the following command: mkdir -p env_config flask decrypt_credentials This will create a directory with the site credentials ( env_config ). Keep these secret. Important DO NOT COMMIT THESE CREDENTIALS TO GITHUB !!! Load the database \u00b6 The site uses an SQLite database that can be setup by running: flask download_db This will update the SQLite database used by CeNDR ( base/cendr.db ). The tables are: homologs - A table of homologs+orthologs. strain - Strain info pulled from the google spreadsheet C. elegans WI Strain Info . wormbase_gene - Summarizes gene information; Broken into component parts (e.g. exons, introns etc.). wormbase_gene_summary - Summarizes gene information. One line per gene. metadata - tracks how data was obtained. When. Where. etc. Test the site \u00b6 You can at this point test the site locally by running: flask run Be sure you have direnv. Otherwise you should source the .envrc file prior to running: source .envrc flask run Creating a new release \u00b6 Before a new release is possible, you must have first completed the following tasks: See Add new sequence data for further details . Add new wild isolate sequence data, and process with the trimmomatic-nf pipeline. Identified new isotypes using the concordance-nf Updated the C. elegans WI Strain Info spreadsheet, adding in new isotypes. Update the release column to reflect the release data in the C. elegans WI Strain Info spreadsheet Run and process sequence data with the wi-nf pipeline. Pushing a new release requires a series of steps described below. Uploading BAMs \u00b6 You will need AWS credentials to upload BAMs to Amazon S3. These are available in the secret credentials location. pip install aws-shell aws configure # Use s3 user credentials Once configured, navigate to the BAM location on b1059. cd /projects/b1059/data/alignments/WI/isotype # CD to bams folder... aws s3 sync . s3://elegansvariation.org/bam Run this command in screen to ensure that it completes (it's going to take a while) Uploading Release Data \u00b6 When you run the wi-nf pipeline it will create a folder with the format WI-YYYYMMDD . These data are output in a format that CeNDR can read as a release. You must upload the WI-YYYYMMDD folder to google storage with a command that looks like this: # First cd to the path of the results folder (WI-YYYYMMDD) from the `wi-nf` pipeline. gsutil rsync . gs://elegansvariation.org/releases/YYYYMMDD/ Important Use rsync to copy the files up to google storage. Note that the WI- prefix has been dropped from the YYYYMMDD declaration. Bump the CeNDR version number and change-log \u00b6 Because we are creating a new data release, we need to \"bump\" or move up the CeNDR version. The CeNDR version number is specified in a file at the base of the repo: travis.yml . Modify this line: - export VERSION_NUM=1-2-8 And increase the version number by 1 (e.g. 1-2-9). You should also update the change log and/or add a news item. The change-log is a markdown file located at base/static/content/help/Change-Log.md ; News items are located at base/static/news/ . Look at existing content to get an idea of how to add new items. It is fairly straightforward. You should be able to see changes on the test site. Adding the release to the CeNDR website \u00b6 After the site is loaded, the BAMs and release data are up, and the database is updated, you need to modify the file base/constants.py to add the new release. The date must match the date of the release that was uploaded. Add your release with the appropriate date and the annotation database used (e.g. (\"YYYYMMDD\", \"Annotation Database\") ). RELEASES = [(\"20180413\", \"WS263\"), (\"20170531\", \"WS258\"), (\"20160408\", \"WS245\")] Commit your changes to the development branch of CeNDR and push them to github. Once pushed, travis-ci will build the app and deploy it to a test branch. Use the google app engine interface to identify and test the app. If everything looks good open a pull request bringing the changes on the development branch to the master branch. Again, travis-ci will launch the new site. Important You need to shut down development instances and older versions of the site on the google-app engine interface once you are done testing/deploying new instances to prevent us from incurring charges for those running instances. Adding isotype images \u00b6 Isolation photos are initially prepared on dropbox and are located in the folder here: ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans Each file should be named using the isotype name and the strain name strain name in the following format: <isotype>_<strain>.jpg Then you will use imagemagick (a commandline-based utility) to scale the images down to 1000 pixels (width) and generate a 150px thumbnail. for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; Once you have generated the images you can upload them to google storage. They should be uploaded to the following location: gs://elegansvariation.org/photos/isolation You can drag/drop the photos using the web-based browser or use gsutil : # First cd to the appropriate directory # cd ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation The script for processing files is located in the dropbox folder and is called 'process_images.sh'. It's also here: for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; # Copy using rsync; Skip .DS_Store files. gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation Update the current variant datasets. \u00b6 The current folder located in gs://elegansvariation.org/releases contains the latest variant datasets and is used by WormBase to display natural variation data. Once you've completed a new release, update the files in this folder gs://elegansvariation.org/releases/current folder. Updating Publications \u00b6 The publications page ( /about/publications ) is generated using a google spreadsheet. The spreadsheet can be accessed here . You can request access to edit the spreadsheet by visiting that link. The last row of the spreadsheet contains a function that can fetch publication data from Pubmed using its API. Simply fill in column A with the PMID (Pubmed Identifier), and the publication data will be fetched. Once you have retrieved the latest pubmed data, create a new row and copy/paste the values for any new publications so they are not fetched from the Pubmed API. Alternatively, you can fill in the details for a publication manually. In either case, any details added should be double checked. Changes should be instant, but there may be some dely on the CeNDR website.","title":"CeNDR"},{"location":"cendr/#cendr","text":"CeNDR Setting up CeNDR Clone the repo Setup a python environment Download and install the gcloud-sdk Create a cendr gcloud configuration Install direnv Test flask Setup Credentials Load the database Test the site Creating a new release Uploading BAMs Uploading Release Data Bump the CeNDR version number and change-log Adding the release to the CeNDR website Adding isotype images Update the current variant datasets. Updating Publications","title":"CeNDR"},{"location":"cendr/#setting_up_cendr","text":"","title":"Setting up CeNDR"},{"location":"cendr/#clone_the_repo","text":"Clone the repo first. git clone http://www.github.com/andersenlab/cendr Switch to the development branch git checkout --track origin/development","title":"Clone the repo"},{"location":"cendr/#setup_a_python_environment","text":"Use miniconda as it will make your life much easier. The conda environment has been specified in the env.yaml file, and can be installed using: conda env create -f env.yaml","title":"Setup a python environment"},{"location":"cendr/#download_and_install_the_gcloud-sdk","text":"Install the gcloud-sdk","title":"Download and install the gcloud-sdk"},{"location":"cendr/#create_a_cendr_gcloud_configuration","text":"gcloud config configurations create cendr","title":"Create a cendr gcloud configuration"},{"location":"cendr/#install_direnv","text":"direnv allows you to load a configuration file when you enter the development directory. Please read about how it works. CeNDR uses a .envrc file within the repo to set up the appropriate environmental variables. Once direnv is installed you can run direnv allow within the CeNDR repo: direnv allow","title":"Install direnv"},{"location":"cendr/#test_flask","text":"With direnv enabled, you are nearly able to run the site locally. Run flask , and you should see the following: > flask Usage: flask [OPTIONS] COMMAND [ARGS]... A general utility script for Flask applications. Provides commands from Flask, extensions, and the application. Loads the application defined in the FLASK_APP environment variable, or from a wsgi.py file. Setting the FLASK_ENV environment variable to 'development' will enable debug mode. $ export FLASK_APP=hello.py $ export FLASK_ENV=development $ flask run Options: --version Show the flask version --help Show this message and exit. Commands: decrypt_credentials Decrypt credentials download_db Download the database (used in docker... initdb Initialize the database routes Show the routes for the app. run Run a development server. shell Runs a shell in the app context. update_credentials Update credentials update_strains Updates the strain table of the database If you do not see the full set of commands there - something is broken.","title":"Test flask"},{"location":"cendr/#setup_credentials","text":"Authenticate with gcloud. Run the following command: mkdir -p env_config flask decrypt_credentials This will create a directory with the site credentials ( env_config ). Keep these secret. Important DO NOT COMMIT THESE CREDENTIALS TO GITHUB !!!","title":"Setup Credentials"},{"location":"cendr/#load_the_database","text":"The site uses an SQLite database that can be setup by running: flask download_db This will update the SQLite database used by CeNDR ( base/cendr.db ). The tables are: homologs - A table of homologs+orthologs. strain - Strain info pulled from the google spreadsheet C. elegans WI Strain Info . wormbase_gene - Summarizes gene information; Broken into component parts (e.g. exons, introns etc.). wormbase_gene_summary - Summarizes gene information. One line per gene. metadata - tracks how data was obtained. When. Where. etc.","title":"Load the database"},{"location":"cendr/#test_the_site","text":"You can at this point test the site locally by running: flask run Be sure you have direnv. Otherwise you should source the .envrc file prior to running: source .envrc flask run","title":"Test the site"},{"location":"cendr/#creating_a_new_release","text":"Before a new release is possible, you must have first completed the following tasks: See Add new sequence data for further details . Add new wild isolate sequence data, and process with the trimmomatic-nf pipeline. Identified new isotypes using the concordance-nf Updated the C. elegans WI Strain Info spreadsheet, adding in new isotypes. Update the release column to reflect the release data in the C. elegans WI Strain Info spreadsheet Run and process sequence data with the wi-nf pipeline. Pushing a new release requires a series of steps described below.","title":"Creating a new release"},{"location":"cendr/#uploading_bams","text":"You will need AWS credentials to upload BAMs to Amazon S3. These are available in the secret credentials location. pip install aws-shell aws configure # Use s3 user credentials Once configured, navigate to the BAM location on b1059. cd /projects/b1059/data/alignments/WI/isotype # CD to bams folder... aws s3 sync . s3://elegansvariation.org/bam Run this command in screen to ensure that it completes (it's going to take a while)","title":"Uploading BAMs"},{"location":"cendr/#uploading_release_data","text":"When you run the wi-nf pipeline it will create a folder with the format WI-YYYYMMDD . These data are output in a format that CeNDR can read as a release. You must upload the WI-YYYYMMDD folder to google storage with a command that looks like this: # First cd to the path of the results folder (WI-YYYYMMDD) from the `wi-nf` pipeline. gsutil rsync . gs://elegansvariation.org/releases/YYYYMMDD/ Important Use rsync to copy the files up to google storage. Note that the WI- prefix has been dropped from the YYYYMMDD declaration.","title":"Uploading Release Data"},{"location":"cendr/#bump_the_cendr_version_number_and_change-log","text":"Because we are creating a new data release, we need to \"bump\" or move up the CeNDR version. The CeNDR version number is specified in a file at the base of the repo: travis.yml . Modify this line: - export VERSION_NUM=1-2-8 And increase the version number by 1 (e.g. 1-2-9). You should also update the change log and/or add a news item. The change-log is a markdown file located at base/static/content/help/Change-Log.md ; News items are located at base/static/news/ . Look at existing content to get an idea of how to add new items. It is fairly straightforward. You should be able to see changes on the test site.","title":"Bump the CeNDR version number and change-log"},{"location":"cendr/#adding_the_release_to_the_cendr_website","text":"After the site is loaded, the BAMs and release data are up, and the database is updated, you need to modify the file base/constants.py to add the new release. The date must match the date of the release that was uploaded. Add your release with the appropriate date and the annotation database used (e.g. (\"YYYYMMDD\", \"Annotation Database\") ). RELEASES = [(\"20180413\", \"WS263\"), (\"20170531\", \"WS258\"), (\"20160408\", \"WS245\")] Commit your changes to the development branch of CeNDR and push them to github. Once pushed, travis-ci will build the app and deploy it to a test branch. Use the google app engine interface to identify and test the app. If everything looks good open a pull request bringing the changes on the development branch to the master branch. Again, travis-ci will launch the new site. Important You need to shut down development instances and older versions of the site on the google-app engine interface once you are done testing/deploying new instances to prevent us from incurring charges for those running instances.","title":"Adding the release to the CeNDR website"},{"location":"cendr/#adding_isotype_images","text":"Isolation photos are initially prepared on dropbox and are located in the folder here: ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans Each file should be named using the isotype name and the strain name strain name in the following format: <isotype>_<strain>.jpg Then you will use imagemagick (a commandline-based utility) to scale the images down to 1000 pixels (width) and generate a 150px thumbnail. for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; Once you have generated the images you can upload them to google storage. They should be uploaded to the following location: gs://elegansvariation.org/photos/isolation You can drag/drop the photos using the web-based browser or use gsutil : # First cd to the appropriate directory # cd ~/Dropbox/Andersenlab/Reagents/WormReagents/isolation_photos/c_elegans gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation The script for processing files is located in the dropbox folder and is called 'process_images.sh'. It's also here: for img in `ls *.jpg`; do convert ${img} -density 300 -resize 1000 ${img} convert ${img} -density 300 -resize 150 ${img/.jpg/}.thumb.jpg done; # Copy using rsync; Skip .DS_Store files. gsutil rsync -x \".DS_Store\" . gs://elegansvariation.org/photos/isolation","title":"Adding isotype images"},{"location":"cendr/#update_the_current_variant_datasets","text":"The current folder located in gs://elegansvariation.org/releases contains the latest variant datasets and is used by WormBase to display natural variation data. Once you've completed a new release, update the files in this folder gs://elegansvariation.org/releases/current folder.","title":"Update the current variant datasets."},{"location":"cendr/#updating_publications","text":"The publications page ( /about/publications ) is generated using a google spreadsheet. The spreadsheet can be accessed here . You can request access to edit the spreadsheet by visiting that link. The last row of the spreadsheet contains a function that can fetch publication data from Pubmed using its API. Simply fill in column A with the PMID (Pubmed Identifier), and the publication data will be fetched. Once you have retrieved the latest pubmed data, create a new row and copy/paste the values for any new publications so they are not fetched from the Pubmed API. Alternatively, you can fill in the details for a publication manually. In either case, any details added should be double checked. Changes should be instant, but there may be some dely on the CeNDR website.","title":"Updating Publications"},{"location":"cloud/","text":"AndersenLab cloud resources \u00b6 AndersenLab cloud resources Google Domains Google Cloud Google Cloud Storage Buckets elegansvariation.org andersenlab.org Other buckets Secret Bucket cegwas (deprecated) Google datastore App engine Error Reporting BigQuery AWS S3 Fargate For full documentation visit mkdocs.org . Google Domains \u00b6 Any domain names the lab uses should be registered with Google Domains. The two ones currently are: andersenlab.org elegansvariation.org Google domains can be used to forward domain-specific email addresses if necessary. For example, example@andersenlab.org could be created and forwarded to an email address. Google Cloud \u00b6 Google cloud is used for a variety of services that the lab uses. Google Cloud Storage \u00b6 Google cloud storage is used to store and distribute files on CeNDR, for cegwas, and for some files on the lab website. Buckets \u00b6 Files are grouped into 'buckets' on google storage. We use the following buckets: elegansvariation.org \u00b6 This bucket contains all the data associated with elegansvariation.org. It is broken down into six primary directories. browser_tracks - for genome-browser tracks that rarely if ever change. db - Storage/access to the SQLite database. photos - sample collection photos. releases - dataset releases. For more detail, see post-gatk-nf . reports - images and data files within reports. static - static assets used by the site. bam - stores all BAM files at the strain level andersenlab.org \u00b6 In some cases the data associated with a publication is too large to put on github. We store those data here, along with a couple other odds and ends. Other buckets \u00b6 Google Cloud creates a bunch of other buckets too. Most of these should be ignored as they are part of the Secret Bucket \u00b6 There is one other secret bucket. Ask Erik about it. cegwas (deprecated) \u00b6 Currently this bucket has a SQLite database used by cegwas. This will be replaced to use the same database used by CeNDR in the db/ folder. Once the new version of cegwas is developed - this section should be removed, and that bucket should be deleted. Google datastore \u00b6 Google datastore used as the database for CeNDR. It stores information on mappings, traits, and more. App engine \u00b6 App engine is the platform CeNDR runs on. Error Reporting \u00b6 Google cloud contains a really nice error reporting interface. Error reports are generated whenever something goes wrong on CeNDR. A github issue can be created for these errors and they can be addressed. BigQuery \u00b6 We have used bigquery in the past for large query jobs. We are not actively using it as of late. AWS \u00b6 S3 \u00b6 In the past we stored BAMs on AWS at elegansvariation.org , however these data have now been migrated to GCP as of the 20210121 CeNDR release. Fargate \u00b6 Amazon Fargate was used to run the mapping pipeline on CeNDR in the past, but this is now moved to GCP as of 2021","title":"Cloud"},{"location":"cloud/#andersenlab_cloud_resources","text":"AndersenLab cloud resources Google Domains Google Cloud Google Cloud Storage Buckets elegansvariation.org andersenlab.org Other buckets Secret Bucket cegwas (deprecated) Google datastore App engine Error Reporting BigQuery AWS S3 Fargate For full documentation visit mkdocs.org .","title":"AndersenLab cloud resources"},{"location":"cloud/#google_domains","text":"Any domain names the lab uses should be registered with Google Domains. The two ones currently are: andersenlab.org elegansvariation.org Google domains can be used to forward domain-specific email addresses if necessary. For example, example@andersenlab.org could be created and forwarded to an email address.","title":"Google Domains"},{"location":"cloud/#google_cloud","text":"Google cloud is used for a variety of services that the lab uses.","title":"Google Cloud"},{"location":"cloud/#google_cloud_storage","text":"Google cloud storage is used to store and distribute files on CeNDR, for cegwas, and for some files on the lab website.","title":"Google Cloud Storage"},{"location":"cloud/#buckets","text":"Files are grouped into 'buckets' on google storage. We use the following buckets:","title":"Buckets"},{"location":"cloud/#elegansvariationorg","text":"This bucket contains all the data associated with elegansvariation.org. It is broken down into six primary directories. browser_tracks - for genome-browser tracks that rarely if ever change. db - Storage/access to the SQLite database. photos - sample collection photos. releases - dataset releases. For more detail, see post-gatk-nf . reports - images and data files within reports. static - static assets used by the site. bam - stores all BAM files at the strain level","title":"elegansvariation.org"},{"location":"cloud/#andersenlaborg","text":"In some cases the data associated with a publication is too large to put on github. We store those data here, along with a couple other odds and ends.","title":"andersenlab.org"},{"location":"cloud/#other_buckets","text":"Google Cloud creates a bunch of other buckets too. Most of these should be ignored as they are part of the","title":"Other buckets"},{"location":"cloud/#secret_bucket","text":"There is one other secret bucket. Ask Erik about it.","title":"Secret Bucket"},{"location":"cloud/#cegwas_deprecated","text":"Currently this bucket has a SQLite database used by cegwas. This will be replaced to use the same database used by CeNDR in the db/ folder. Once the new version of cegwas is developed - this section should be removed, and that bucket should be deleted.","title":"cegwas (deprecated)"},{"location":"cloud/#google_datastore","text":"Google datastore used as the database for CeNDR. It stores information on mappings, traits, and more.","title":"Google datastore"},{"location":"cloud/#app_engine","text":"App engine is the platform CeNDR runs on.","title":"App engine"},{"location":"cloud/#error_reporting","text":"Google cloud contains a really nice error reporting interface. Error reports are generated whenever something goes wrong on CeNDR. A github issue can be created for these errors and they can be addressed.","title":"Error Reporting"},{"location":"cloud/#bigquery","text":"We have used bigquery in the past for large query jobs. We are not actively using it as of late.","title":"BigQuery"},{"location":"cloud/#aws","text":"","title":"AWS"},{"location":"cloud/#s3","text":"In the past we stored BAMs on AWS at elegansvariation.org , however these data have now been migrated to GCP as of the 20210121 CeNDR release.","title":"S3"},{"location":"cloud/#fargate","text":"Amazon Fargate was used to run the mapping pipeline on CeNDR in the past, but this is now moved to GCP as of 2021","title":"Fargate"},{"location":"github/","text":"Git and Github \u00b6 Git and Github Introduction Using Git and Github The GitHub Flow 1. Clone/pull 2. Branch 3. Edit 4. Commit 5. Push 6. Pull request 7. Inspect 8. Merge Command Line git commands GitHub Flow Best Practices Resources Introduction \u00b6 Git is a version control software package similar to traccked changes and saving documents as \"final1.pdf\" and \"final2.pdf\" Github is a 3rd party web-based graphical interface that has a copy of the project that you and/or other people can push and pull from to work on the same code simultaneously. Keep in mind, its not like google docs, it doesn\u2019t update automatically, requires you to push changes and pull changes to the new computer. Note You must install git to use and create an account on Github. Check out this intro guide here Using Git and Github \u00b6 The Andersen Lab Github can be found here . As of the writing of this page, we have 165 \"repositories\" (or projects). Notice some projects are code like NemaScan and others are personal projects ( abamectin ) or manuscripts ( mol_eco_manuscript ). Anything can be a repo! There are two main ways to use Git: (1) on the command line (aka Terminal on Macs) or with a GUI (graphical user interface). Both are good and neither are \"wrong\". For new users, it is usually easier to start with a GUI like Github Desktop . However, only a few basic commands are really necessary to get started using git on the command line, so don't be nervous! Important Git GUI like Github Desktop cannot be used with repositories on QUEST. If you are building a pipeline on QUEST, it is essential to get comfortable with using Git on the command line The GitHub Flow \u00b6 There are several different Git branching strategies, but the most popular for our lab is the \"GitHub Flow\". This 8 step process can help keep our pipelines flowing, functional, and organized. New to the GitHub flow? I highly recommend you try out this amazing tutorial to practice all the steps from beginning to end. Then start putting it in use for your own pipelines! The following 8 steps can be done on the command line or with a GUI. Below I will show the basic git commands for managing a repo on the command line, for more help you can find the slides from Code club at ~/Dropbox/AndersenLab/LabMeetings/CodeClub/20210326_KSE/20210326_slides.key Note When you are maintaining a project repo that only you are updating, it is less important to follow the GitHub Flow with creating short-lived branches. However, if you are developing/maintaining code that other people will use and/or working collaboratively this is an essential skill to master. 1. Clone/pull \u00b6 # Cloning - new repo cd < directory you want repo stored > git clone https://github.com/AndersenLab/code_club.git cd code_club # pulling - already cloned repo you want to get newest version of cd < directory of repo > git pull 2. Branch \u00b6 # create a new branch AND move to it git checkout -b <branch_name> # list all available branches git branch # move to a branch git checkout < name of branch > 3. Edit \u00b6 No code here... make any edits to the repo. 4. Commit \u00b6 # first step - add changed files to staging area git add <changed file> # OR add ALL files to staging area git add . # commit files in staging area git commit -m \"<some message about what changes you made>\" 5. Push \u00b6 # push changes to remote git push # when it is your first time pushing a new branch, it might prompt you to set an upstream branch: git push --set-upstream origin new_branch 6. Pull request \u00b6 I generally like to do this step online at github.com because I think it is useful to visually see the changes I made go to the repo site click the green \"compare and pull request\" button check the branches are right at the top: which branch is merging into which branch optional: assing reviewer and/or assignees on the right hand side. This is often useful when coding collaboratively update the title/comment for the pull request to let yourself and others know what changes were made and why 7. Inspect \u00b6 I generally like to do this step online at github.com because I think it is useful to visually see the changes I made. If you scroll down you should be able to see which files were changed and what exact changes were made. If there are merge conflicts, github will walk you through fixing them. 8. Merge \u00b6 When you are satisfied with your merge, click the green \"merge pull request\" button. Also make sure the delete the old branch when you are done as part of keeping the repo clean and clutter-free Note Good practice is to make a new branch to implement a new feature, then delete the branch once it has been merged. To start a new feature, open a NEW branch. Not as important on self-projects, but very important for collaboration Command Line git commands \u00b6 Basic git clone - clone remote repository git pull - pull most recent version from remote git add - add local files to be staged for remote git commit - stage/commit local changes git push - push local commits to remote Intermediate git branch - list all available branches git checkout - move to new branch git status - checks which branch you are on and if you have any unsaved changes git log - shows log of previous commits on current branch git diff - shows details of changes made For more, check out this tutorial, and others. GitHub Flow Best Practices \u00b6 Any code in the main branch should be deployable Create new descriptively-named branches off the main branch for new work such as feature/add-new-plot Commit new work to your local branches and regularly push work to the remote To request feedback or help, or when you think your work is ready to merge into the main branch, open a pull request After your work or feature has been reviewed and approved, it can be merged into the main branch Delete stale branches! Once your work has been merged into the main branch, it should be deployed immediately Note GitHub Flow is not the only branching strategy out there! This was a great article about the three most common strategies with pros and cons for why you might use each one. I challenge you to think aobut which strategy might be best for our lab moving forward and let's start a discussion about it! Resources \u00b6 This blog on the differences between git and github \"Git started\" using Git on the command line here Overview of top Git GUI from 2021 here Great intro video to the GitHub Flow HIGHLY RECOMMENDED introduction tutorial to GitHub Flow Amazing article on different git branch strategies here","title":"Git and Github"},{"location":"github/#git_and_github","text":"Git and Github Introduction Using Git and Github The GitHub Flow 1. Clone/pull 2. Branch 3. Edit 4. Commit 5. Push 6. Pull request 7. Inspect 8. Merge Command Line git commands GitHub Flow Best Practices Resources","title":"Git and Github"},{"location":"github/#introduction","text":"Git is a version control software package similar to traccked changes and saving documents as \"final1.pdf\" and \"final2.pdf\" Github is a 3rd party web-based graphical interface that has a copy of the project that you and/or other people can push and pull from to work on the same code simultaneously. Keep in mind, its not like google docs, it doesn\u2019t update automatically, requires you to push changes and pull changes to the new computer. Note You must install git to use and create an account on Github. Check out this intro guide here","title":"Introduction"},{"location":"github/#using_git_and_github","text":"The Andersen Lab Github can be found here . As of the writing of this page, we have 165 \"repositories\" (or projects). Notice some projects are code like NemaScan and others are personal projects ( abamectin ) or manuscripts ( mol_eco_manuscript ). Anything can be a repo! There are two main ways to use Git: (1) on the command line (aka Terminal on Macs) or with a GUI (graphical user interface). Both are good and neither are \"wrong\". For new users, it is usually easier to start with a GUI like Github Desktop . However, only a few basic commands are really necessary to get started using git on the command line, so don't be nervous! Important Git GUI like Github Desktop cannot be used with repositories on QUEST. If you are building a pipeline on QUEST, it is essential to get comfortable with using Git on the command line","title":"Using Git and Github"},{"location":"github/#the_github_flow","text":"There are several different Git branching strategies, but the most popular for our lab is the \"GitHub Flow\". This 8 step process can help keep our pipelines flowing, functional, and organized. New to the GitHub flow? I highly recommend you try out this amazing tutorial to practice all the steps from beginning to end. Then start putting it in use for your own pipelines! The following 8 steps can be done on the command line or with a GUI. Below I will show the basic git commands for managing a repo on the command line, for more help you can find the slides from Code club at ~/Dropbox/AndersenLab/LabMeetings/CodeClub/20210326_KSE/20210326_slides.key Note When you are maintaining a project repo that only you are updating, it is less important to follow the GitHub Flow with creating short-lived branches. However, if you are developing/maintaining code that other people will use and/or working collaboratively this is an essential skill to master.","title":"The GitHub Flow"},{"location":"github/#1_clonepull","text":"# Cloning - new repo cd < directory you want repo stored > git clone https://github.com/AndersenLab/code_club.git cd code_club # pulling - already cloned repo you want to get newest version of cd < directory of repo > git pull","title":"1. Clone/pull"},{"location":"github/#2_branch","text":"# create a new branch AND move to it git checkout -b <branch_name> # list all available branches git branch # move to a branch git checkout < name of branch >","title":"2. Branch"},{"location":"github/#3_edit","text":"No code here... make any edits to the repo.","title":"3. Edit"},{"location":"github/#4_commit","text":"# first step - add changed files to staging area git add <changed file> # OR add ALL files to staging area git add . # commit files in staging area git commit -m \"<some message about what changes you made>\"","title":"4. Commit"},{"location":"github/#5_push","text":"# push changes to remote git push # when it is your first time pushing a new branch, it might prompt you to set an upstream branch: git push --set-upstream origin new_branch","title":"5. Push"},{"location":"github/#6_pull_request","text":"I generally like to do this step online at github.com because I think it is useful to visually see the changes I made go to the repo site click the green \"compare and pull request\" button check the branches are right at the top: which branch is merging into which branch optional: assing reviewer and/or assignees on the right hand side. This is often useful when coding collaboratively update the title/comment for the pull request to let yourself and others know what changes were made and why","title":"6. Pull request"},{"location":"github/#7_inspect","text":"I generally like to do this step online at github.com because I think it is useful to visually see the changes I made. If you scroll down you should be able to see which files were changed and what exact changes were made. If there are merge conflicts, github will walk you through fixing them.","title":"7. Inspect"},{"location":"github/#8_merge","text":"When you are satisfied with your merge, click the green \"merge pull request\" button. Also make sure the delete the old branch when you are done as part of keeping the repo clean and clutter-free Note Good practice is to make a new branch to implement a new feature, then delete the branch once it has been merged. To start a new feature, open a NEW branch. Not as important on self-projects, but very important for collaboration","title":"8. Merge"},{"location":"github/#command_line_git_commands","text":"Basic git clone - clone remote repository git pull - pull most recent version from remote git add - add local files to be staged for remote git commit - stage/commit local changes git push - push local commits to remote Intermediate git branch - list all available branches git checkout - move to new branch git status - checks which branch you are on and if you have any unsaved changes git log - shows log of previous commits on current branch git diff - shows details of changes made For more, check out this tutorial, and others.","title":"Command Line git commands"},{"location":"github/#github_flow_best_practices","text":"Any code in the main branch should be deployable Create new descriptively-named branches off the main branch for new work such as feature/add-new-plot Commit new work to your local branches and regularly push work to the remote To request feedback or help, or when you think your work is ready to merge into the main branch, open a pull request After your work or feature has been reviewed and approved, it can be merged into the main branch Delete stale branches! Once your work has been merged into the main branch, it should be deployed immediately Note GitHub Flow is not the only branching strategy out there! This was a great article about the three most common strategies with pros and cons for why you might use each one. I challenge you to think aobut which strategy might be best for our lab moving forward and let's start a discussion about it!","title":"GitHub Flow Best Practices"},{"location":"github/#resources","text":"This blog on the differences between git and github \"Git started\" using Git on the command line here Overview of top Git GUI from 2021 here Great intro video to the GitHub Flow HIGHLY RECOMMENDED introduction tutorial to GitHub Flow Amazing article on different git branch strategies here","title":"Resources"},{"location":"labsite/","text":"Andersenlab.org \u00b6 Andersenlab.org Getting Started Software-Dependencies Cloning the repo Updating the site andersenlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software Getting Started \u00b6 The Andersen Lab website was built using jekyll and runs using the Github Pages service. Software-Dependencies \u00b6 Several software packages are required for editing/maintaining the Andersen Lab site. They can be installed using Homebrew : brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml . Cloning the repo \u00b6 To get started editing, clone the repo: git clone https://github.com/andersenlab/andersenlab.github.io This repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url. Updating the site \u00b6 In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git. andersenlab.github.io \u00b6 The structure of the Andersen Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with Announcements \u00b6 Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication. General Announcements \u00b6 To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more. Publication Post \u00b6 New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Katie's paper accepted at <em>G3</em>!\" subtitle: \"Correlations of geneotype with climate parameters suggest <em>Caenorhabditis elegans</em> niche adaptations\" layout: post tags: news published: true PMID: 27866149 --- Congratulations to Katie for her paper accepted at G3! Lab members \u00b6 Adding new lab members: \u00b6 (1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page. Set Status to Former \u00b6 Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Mostafa pub_names: - Zamanian M last_name: Zamanian description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2015-2017 photo: Mostafa2014.jpg former: true github: mzamanian email: zamanian@northwestern.edu current_status: Assistant Professor at UW Madison -- <a href='http://www.zamanianlab.org/'>Zamanian Lab Website</a> Remove lab members \u00b6 Remove the persons information from _data/people.yaml ; Optionally delete their photo. Funding \u00b6 Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: nigms: https://www.nigms.nih.gov/Pages/default.aspx acs: http://www.cancer.org/ pew: http://www.pewtrusts.org/en niaid: https://www.niaid.nih.gov/ aws: https://aws.amazon.com/ weinberg: http://www.weinberg.northwestern.edu/ mod: http://www.marchofdimes.org/ cbc: http://www.chicagobiomedicalconsortium.org/ Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here. Protocols \u00b6 Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>C. elegans</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Brood Assay file: SemiQuantitativeBroodAssay.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Pseudomonas aeruginosa</em> Fast-killing assay</a> file: FKAprotocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Staphylococcus aureus</em> killing assay</a> file: Staphaureus_Protocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Bacillus thuringiensis</em> toxin assay on plates</a> file: Bacillus-thuringiensis-toxin-plate-assay.pdf group: <em>C. elegans</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines. Research \u00b6 The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: High-throughput approaches to understand conserved drug responses image: worms_drugs2.jpg order: 1 --- Because of the efforts of a number of highly dedicated scientists and citizen volunteers... To this end, we deep sequenced all of these strains... The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000). Publications \u00b6 Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the Caenorhabditis elegans species You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Caenorhabditis elegans</em> Final result: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Date_Published: 2017 May 09 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the <em>Caenorhabditis elegans</em> species PDF: 2017Laricchia (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2017Laricchia has an associated folder in publications/ where supplemental data and figures are stored: publications/2017Laricchia/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2017Laricchia : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page. Photo Albums \u00b6 Photo albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2017-08-05-Hawaii Trip . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the andersenlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push Software \u00b6 If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Andersen Labsite"},{"location":"labsite/#andersenlaborg","text":"Andersenlab.org Getting Started Software-Dependencies Cloning the repo Updating the site andersenlab.github.io Announcements General Announcements Publication Post Lab members Adding new lab members: Set Status to Former Remove lab members Funding Protocols Research Publications Photo Albums Software","title":"Andersenlab.org"},{"location":"labsite/#getting_started","text":"The Andersen Lab website was built using jekyll and runs using the Github Pages service.","title":"Getting Started"},{"location":"labsite/#software-dependencies","text":"Several software packages are required for editing/maintaining the Andersen Lab site. They can be installed using Homebrew : brew install ruby imagemagick exiftool python ghostscript brew upgrade ruby # If Ruby has not been updated in a while, you may need to do this. sudo gem install jekyll -v 3.6.0 # If you get an error when trying to run pip, try: # brew link --overwrite python pip install metapub pyyaml Ruby - Is used to run jekyll, which is the software that builds the site. Jekyll - As stated earlier, jekyll builds the static site and is written in Ruby. Imagemagick - Handles thumbnail generation and scaling photos. Imagemagick is used in the build.sh script. exiftool Extract data about photos as part of the build.sh script for use in scaling images. Python Retrieves information about publications and updates _data/pubs_data.yaml .","title":"Software-Dependencies"},{"location":"labsite/#cloning_the_repo","text":"To get started editing, clone the repo: git clone https://github.com/andersenlab/andersenlab.github.io This repo contains documents that get compiled into the Andersen Lab website. When you make changes to this repo and push them to GitHub, Github will trigger a 'build' of the labsite and update it. This usually takes less than a minute. Instructions for changing various aspects of the site are listed below. You can also use Github Desktop to manage changes to the site. If you want to edit the site locally and preview changes, run the following in the root directory of the git repo: jekyll serve The site should become available at localhost:4000 and any changes you make will be reflected at that local url.","title":"Cloning the repo"},{"location":"labsite/#updating_the_site","text":"In order for any change to become visible, you need to use git. Any subsequent directions that suggest modifying, adding, or removing files assumes you will be committing these changes to the repo and pushing the commit to GitHub.com. See Git-SCM for a basic introduction to git.","title":"Updating the site"},{"location":"labsite/#andersenlabgithubio","text":"The structure of the Andersen Lab repo looks like this: CNAME LICENSE README.md build.sh index.html _config.yml _data/ _includes/ _layouts/ _posts/ _site/ assets/ feeds/ files/ pages/ people/ publications/ scripts/ protocols/ funding/ The folders prefixed with","title":"andersenlab.github.io"},{"location":"labsite/#announcements","text":"Announcements are stored in the _posts folder. Posts are organized into folders by year. There is also a _photo_albums folder that you can ignore (more on this below). Two types of announcements can be made. A 'general' announcement regarding anything, or a new publication.","title":"Announcements"},{"location":"labsite/#general_announcements","text":"To add a new post create a new text file with the following naming scheme: YYYY-MM-DD-title.md For example: 2017-09-24-A new post.md The contents of the file should correspond to the following structure: --- title: \"The title of the post\" layout: post tags: news published: true --- The post content goes here! The top part surrounded by --- is known as the header and has to define a number of variables: layout: post , tags: news , and published: true should always be set and should not change. The only thing you will change is the title . Set a title, and add content below. Because we used a *.md extension when naming the file, we can use markdown in the post to create headings, links, images, and more.","title":"General Announcements"},{"location":"labsite/#publication_post","text":"New publication posts can be created. These posts embed a publication summary identical to what you see on the publication page. They follow the same paradigm as above except they require two additional lines in the header: subtitle: - Usually the title of the paper; Appears on homepage. PMID: - The pubmed identifier Example : --- title: \"Katie's paper accepted at <em>G3</em>!\" subtitle: \"Correlations of geneotype with climate parameters suggest <em>Caenorhabditis elegans</em> niche adaptations\" layout: post tags: news published: true PMID: 27866149 --- Congratulations to Katie for her paper accepted at G3!","title":"Publication Post"},{"location":"labsite/#lab_members","text":"","title":"Lab members"},{"location":"labsite/#adding_new_lab_members","text":"(1) - Add a photo of the individual to the people/ folder. (2) - Edit the _data/people.yaml file, and add the information about that individual. Each individual should have - at a minimum, the following: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> photo: <filename of the photo located in the people/ directory> Additional fields can also be added: - first_name: <first name> last_name: <last name> title: <The job title of the individual; e.g. Graduate Student; Research Associate; Undergrad; Postdoctoral Researcher; Lab technician> pub_names: [\"<an array>\", \"<of possible>\", \"<publication>\", \"<names>\"] photo: <base filename of the photo located in the people/ directory; e.g. 'dan.jpg'> website: <website> description: <a description of research> email: <email> github: <github username> Note pub_names is a list of possible ways an individual is referenced in the author list of a publication. This creates links from the publications page back to lab members on the people page.","title":"Adding new lab members:"},{"location":"labsite/#set_status_to_former","text":"Lab members can be moved to the bottom of the people page under the 'former member' area. To do this add a former: true line for that individual and a current_status: line indicating what they are up to. For example: - first_name: Mostafa pub_names: - Zamanian M last_name: Zamanian description: My research broadly spans \"neglected disease\" genomics and drug discovery. I am currently working to uncover new genetic determinants of anthelmintic resistance and to develop genome editing technology for human pathogenic helminths. title: Postdoctoral Researcher, 2015-2017 photo: Mostafa2014.jpg former: true github: mzamanian email: zamanian@northwestern.edu current_status: Assistant Professor at UW Madison -- <a href='http://www.zamanianlab.org/'>Zamanian Lab Website</a>","title":"Set Status to Former"},{"location":"labsite/#remove_lab_members","text":"Remove the persons information from _data/people.yaml ; Optionally delete their photo.","title":"Remove lab members"},{"location":"labsite/#funding","text":"Funding is managed using the funding/ folder in the root directory and the data file _data/funding_links.yaml . The funding/ folder has two subfolders: past/ and current/ for past funding and current funding. Rename the logo file to be lowercase and simple. To update funding simply place the logo of the institution providing funding in one of these folders and it will appear on the funding page under the heading corresponding to the subfolder in which it was placed. If you would like to add a link for the funding of that organization you can edit the _data/funding_links.yaml file. This file is structured as a set of basename: url pairs: nigms: https://www.nigms.nih.gov/Pages/default.aspx acs: http://www.cancer.org/ pew: http://www.pewtrusts.org/en niaid: https://www.niaid.nih.gov/ aws: https://aws.amazon.com/ weinberg: http://www.weinberg.northwestern.edu/ mod: http://www.marchofdimes.org/ cbc: http://www.chicagobiomedicalconsortium.org/ Each acronym above corresponds with an image file in the current/ or past/ folder. Notice that the extension (e.g. jpeg/png, etc) does not matter. Just use the basename of the file and its associated link here.","title":"Funding"},{"location":"labsite/#protocols","text":"Protocols are stored in the protocols/ folder and their titles and pdfs are managed in _data/protocols.yaml . To add a new protocol, add the PDF to the protocols/ folder. Then add these lines to the _data/protocols.yaml file: - Name: Title of Protocol file: filename_of_protocol_in_protocols_folder.pdf group: <em>C. elegans</em> Phenotyping methods name - The name of the protocol file - The filename of the protocol within the protocols/ folder. group - The grouping of the protocol; It will be nested under this grouping on the protocols page. - name: Semi-Quantitative Brood Assay file: SemiQuantitativeBroodAssay.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Pseudomonas aeruginosa</em> Fast-killing assay</a> file: FKAprotocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Staphylococcus aureus</em> killing assay</a> file: Staphaureus_Protocol.pdf group: <em>C. elegans</em> Phenotyping Methods - name: <em>Bacillus thuringiensis</em> toxin assay on plates</a> file: Bacillus-thuringiensis-toxin-plate-assay.pdf group: <em>C. elegans</em> Phenotyping Methods To remove a protocol, delete the pdf and remove the corresponding lines.","title":"Protocols"},{"location":"labsite/#research","text":"The research portion of the site is structured as a set of sections - each devoted to a project/area. Navigate to /pages/research and you will see a set of files: research.html - This page controls the content at the top of the research page. It's an overview of research in the Andersen lab. You can edit the top portion between the <p>[content]</p> tags freely to modify the top of the research page. research-*.md - These are the individual projects. These files look like this: --- title: High-throughput approaches to understand conserved drug responses image: worms_drugs2.jpg order: 1 --- Because of the efforts of a number of highly dedicated scientists and citizen volunteers... To this end, we deep sequenced all of these strains... The page includes a header (the items located between --- ) which includes a number of important items. title - the title to display for the research area. image - An image for that research area/project. This is the base name of the image placed in /assets/img/research/ order - A number indicating the order you would like the page ot appear in. Order is descending and any set of numbers can be used to determine sort order (e.g. 1, 2, 5, 8, 1000).","title":"Research"},{"location":"labsite/#publications","text":"Elements used to construct the publications page of the website are stored in two places: _data/pubs_data.yaml - The publications data stores authors, pub date, journal, etc. publications/ - The publications folder for PDFs, thumbnails, and supplementary files. (1) Download a PDF of the publication You will want to remove any additional PDF pages (e.g. cover sheets) if there are any present in the PDF. See this guide for information on removing pages from a PDF. Save the PDF to /publications/[year][tag] Where tag is a unique identifier for the publication. In general, these have been the first author or the journal or a combination of both. (Optional) PMID Known If the PubMed Identifier (PMID) is known for the publication, you can add it to the file publications/publications_list.txt . (2) Run build.sh The build.sh script does a variety of tasks for the website. For publications - it will generate thumbnails. It will also fetch information for publications and add it to the _data/pubs_data.yaml file if a PMID has been provided. If you did not add a PMID, you will have to manually add authors, journal, etc. to the _data/pubs_data.yaml file. (3) Edit _data/pubs_data.yaml The publication should now be added either manually or automatically to _data/pubs_data.yaml and should look something like this: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the Caenorhabditis elegans species You will need to add a few things: - Add a PDF: line to associate the publication with the correct PDF and its thumbnail. This is the same tag you used above. - If there is no Date_Published: line you will want to add that. The format is YYYY-month_abbr-DD (e.g. 2017 Aug 17 ). - Add <em> tags around items you want to italicize: <em>Caenorhabditis elegans</em> Final result: - Authors: [Laricchia KM, Zdraljevic S, Cook DE, Andersen EC] Citations: 0 DOI: 10.1093/molbev/msx155 Date_Published: 2017 May 09 Journal: Molecular Biology and Evolution PMID: 28486636 Title: Natural variation in the distribution and abundance of transposable elements across the <em>Caenorhabditis elegans</em> species PDF: 2017Laricchia (4) Add supplementary data Supplemental data and figures are stored in publications/[pdf_name] . For example, 2017Laricchia has an associated folder in publications/ where supplemental data and figures are stored: publications/2017Laricchia/<supplemental files> Once you have added supplemental files, you'll need to add some information to _data/pubs_data.yaml to describe them. These are the lines that were added for 2017Laricchia : pub_data: files: Supplemental_Figures.pdf: {title: Supplemental Figures} Supplemental_Files.zip: {title: Supplemental Files} Supplemental_Tables.zip: {title: Supplemental Tables}\\ ... <name of file>: {title: <title to display>} The last line above illustrates the format. The name of the file must match exactly what is in the publications/[pdf_name] folder. Resulting supplemental data will be listed under publications and on the data page.","title":"Publications"},{"location":"labsite/#photo_albums","text":"Photo albums can be added to the Andersen Labsite. Adding albums requires two utilities to be installed on your computer: (a) Image Magick (b) exiftool These can easily be installed with homebrew . should have been installed during Setup (above), but if not you can install them using the following: brew install imagemagick brew install exiftool (1) Place images in a folder and name it according to the following schema: YYYY-MM-DD-title For example, 2017-08-05-Hawaii Trip . (2) Move that folder to /people/albums/ (3) Run the build.sh script in the root of the andersenlab.github.io repo. The build.sh script will do the following: (a) Construct pages for the album being published. (b) Decrease the size of the images in the album (max width=1200). Note The build.sh script also performs other maintenance-related tasks. It is fine to run this script at anytime. You can run the script using: bash build.sh (4) Add the images using git and push to GitHub You can easily add all images using: git add *.jpg (5) Push changes to github git push","title":"Photo Albums"},{"location":"labsite/#software","text":"If you can write software you should be able to figure out how to update this section. It's markdown/html and not overly complicated.","title":"Software"},{"location":"pipeline-GCPconfig/","text":"Running Nextflow pipeline on GCP \u00b6 Running Nextflow pipeline on GCP Enable API Create service account Generate credential for the service account Nextflow version and mode Configure Nextflow for GCP Google genomic API allows auto scale for computational resources by creating and closing VMs automatically. We have a dedicated google project caendr which using Google genomic APT for all the nextflow pipelines in our lab. To access it, you should provide your gmail accout to Erik and ask Erik give you a project owner role for caendr . I already preset the project to enable running Nextflow pipelines using Google genomic API . See below for more details. Enable API \u00b6 Go the the main page of google cloud platform . In the Produck & Services menu, click APIs & Services , and then click Enable APIs and Services . The following APIs should be enabled to run nextflow pipeline on GCP. Genomics API Cloud Life Sciences API Compute Engine API Google Container Registry API Create service account \u00b6 Go to the IAM & admin , find the Service accounts . Click CREATE SERVICE ACCOUNT to create a new service accounts. Note this service accounts must have a project owner role to run Nextflow pipelines. The service account I created here is called nextflowRUN . You don't need to do the above processes when you use GCP. But you have to do all the following processes to make sure you have the right permissions to caendr . Generate credential for the service account \u00b6 After you get the access to caendr , go to the API & Services . Click the Create credentials button, select Service account key . And choose nextflowRUN to generate a JSON file, which is a privite key file for using nextflowRUN . Download the file and save it in a safe place. Finally, define the GOOGLE_APPLICATION_CREDENTIALS variable in .bash_profile with the directory of the JSON file. Which should looks like the example. export GOOGLE_APPLICATION_CREDENTIALS=$HOME/google_creds/caendr-2cae6210c8d1.json Nextflow version and mode \u00b6 Only the version of 19.07.0 or higher of Nextflow are compatible with GCP. And also, the Nextflow should have a google mode. You can define the version and mode in .bash_profile . export NXF_VER=19.07.0 export NXF_MODE=google Then, run the following code to update or install Nextflow. curl https://get.nextflow.io | bash Configure Nextflow for GCP \u00b6 To run Nextflow pipelines on GCP, you need to build docker images for them. Check the docker file repo of our lab for more information. The google genomic API has its own executor called google-pipelines , you need to define the executor variable with google-pipelines in the nextflow.config file. Here is the example for concordance-nf . docker { enabled = true } process { executor = 'google-pipelines' withLabel: bam_coverage { container = 'faithman/bam_toolbox:latest' } container = 'faithman/concordance:latest' machineType = 'n1-standard-4' } google { project = 'caendr' zone = 'us-central1-a' } cloud { preemptible = true } executor { queueSize = 500 } Important The file system of google buckets is not like S3 that can read/write directly by most softwares. You have to use gsutil tool to interact with google buckets to read/write files in most situations. Nextflow has built-in functions to interact with google buckets, but you still can not read/write files directly in your script. All the files have to be read and write via channels in Nextflow!","title":"GCP"},{"location":"pipeline-GCPconfig/#running_nextflow_pipeline_on_gcp","text":"Running Nextflow pipeline on GCP Enable API Create service account Generate credential for the service account Nextflow version and mode Configure Nextflow for GCP Google genomic API allows auto scale for computational resources by creating and closing VMs automatically. We have a dedicated google project caendr which using Google genomic APT for all the nextflow pipelines in our lab. To access it, you should provide your gmail accout to Erik and ask Erik give you a project owner role for caendr . I already preset the project to enable running Nextflow pipelines using Google genomic API . See below for more details.","title":"Running Nextflow pipeline on GCP"},{"location":"pipeline-GCPconfig/#enable_api","text":"Go the the main page of google cloud platform . In the Produck & Services menu, click APIs & Services , and then click Enable APIs and Services . The following APIs should be enabled to run nextflow pipeline on GCP. Genomics API Cloud Life Sciences API Compute Engine API Google Container Registry API","title":"Enable API"},{"location":"pipeline-GCPconfig/#create_service_account","text":"Go to the IAM & admin , find the Service accounts . Click CREATE SERVICE ACCOUNT to create a new service accounts. Note this service accounts must have a project owner role to run Nextflow pipelines. The service account I created here is called nextflowRUN . You don't need to do the above processes when you use GCP. But you have to do all the following processes to make sure you have the right permissions to caendr .","title":"Create service account"},{"location":"pipeline-GCPconfig/#generate_credential_for_the_service_account","text":"After you get the access to caendr , go to the API & Services . Click the Create credentials button, select Service account key . And choose nextflowRUN to generate a JSON file, which is a privite key file for using nextflowRUN . Download the file and save it in a safe place. Finally, define the GOOGLE_APPLICATION_CREDENTIALS variable in .bash_profile with the directory of the JSON file. Which should looks like the example. export GOOGLE_APPLICATION_CREDENTIALS=$HOME/google_creds/caendr-2cae6210c8d1.json","title":"Generate credential for the service account"},{"location":"pipeline-GCPconfig/#nextflow_version_and_mode","text":"Only the version of 19.07.0 or higher of Nextflow are compatible with GCP. And also, the Nextflow should have a google mode. You can define the version and mode in .bash_profile . export NXF_VER=19.07.0 export NXF_MODE=google Then, run the following code to update or install Nextflow. curl https://get.nextflow.io | bash","title":"Nextflow version and mode"},{"location":"pipeline-GCPconfig/#configure_nextflow_for_gcp","text":"To run Nextflow pipelines on GCP, you need to build docker images for them. Check the docker file repo of our lab for more information. The google genomic API has its own executor called google-pipelines , you need to define the executor variable with google-pipelines in the nextflow.config file. Here is the example for concordance-nf . docker { enabled = true } process { executor = 'google-pipelines' withLabel: bam_coverage { container = 'faithman/bam_toolbox:latest' } container = 'faithman/concordance:latest' machineType = 'n1-standard-4' } google { project = 'caendr' zone = 'us-central1-a' } cloud { preemptible = true } executor { queueSize = 500 } Important The file system of google buckets is not like S3 that can read/write directly by most softwares. You have to use gsutil tool to interact with google buckets to read/write files in most situations. Nextflow has built-in functions to interact with google buckets, but you still can not read/write files directly in your script. All the files have to be read and write via channels in Nextflow!","title":"Configure Nextflow for GCP"},{"location":"pipeline-alignment/","text":"alignment-nf \u00b6 alignment-nf Pipeline overview Software requirements Usage Testing on Quest Running on Quest Parameters -profile --sample_sheet --debug (optional) --species (optional) --fq_prefix (optional) --kmers (optional) --reference (optional) --ncbi (optional) --output (optional) Output Data storage Cleanup Archive construct_sample_sheet.sh Adding new sequencing datasets The alignment-nf pipeline performs alignment for wild isolate sequence data at the strain level , and outputs BAMs and related information. Those BAMs can be used for downstream analysis including variant calling, concordance analysis , wi-gatk-nf (variant calling) and other analyses. This page details how to run the pipeline and how to add new wild isolate sequencing data. Note Historically, sequence processing was performed at the isotype level. We are still interested in filtering strains used in analysis at the isotype level, but alignment and variant calling are now performed at the strain level rather than at the isotype level. Pipeline overview \u00b6 \u2597\u2596 \u259d\u259c \u259d \u2597 \u2597\u2596 \u2596\u2597\u2584\u2584\u2596 \u2590\u258c \u2590 \u2597\u2584 \u2584\u2584 \u2597\u2597\u2596 \u2597\u2584\u2584 \u2584\u2596 \u2597\u2597\u2596 \u2597\u259f\u2584 \u2590\u259a \u258c\u2590 \u258c\u2590 \u2590 \u2590 \u2590\u2598\u259c \u2590\u2598\u2590 \u2590\u2590\u2590 \u2590\u2598\u2590 \u2590\u2598\u2590 \u2590 \u2590\u2590\u2596\u258c\u2590\u2584\u2584\u2596 \u2599\u259f \u2590 \u2590 \u2590 \u2590 \u2590 \u2590 \u2590\u2590\u2590 \u2590\u2580\u2580 \u2590 \u2590 \u2590 \u2580\u2598 \u2590 \u258c\u258c\u2590 \u2590 \u258c \u259d\u2584 \u2597\u259f\u2584 \u259d\u2599\u259c \u2590 \u2590 \u2590\u2590\u2590 \u259d\u2599\u259e \u2590 \u2590 \u259d\u2584 \u2590 \u2590\u258c\u2590 \u2596\u2590 \u259d\u2598 parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --sample_sheet See test_data/sample_sheet for example null --species Species to map: 'ce', 'cb' or 'ct' null --fq_prefix Path to fastq if not in sample_sheet /projects/b1059/data/{species}/WI/fastq/dna/ --kmers Whether to count kmers false --reference genome.fasta.gz to use in place of default defaults for c.e, c.b, and c.t --output Output folder name. alignment-{date} HELP: http://andersenlab.org/dry-guide/pipeline-alignment/ Software requirements \u00b6 Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Currently only runs on Quest with conda environments installed at /projects/b1059/software/conda_envs/ Note mosdepth is used to calculate coverage. mosdepth is available on Linux machines, but not on Mac OSX. That is why the conda environment for the coverage process is specified as conda { System.properties['os.name'] != \"Mac OS X\" ? 'bioconda::mosdepth=0.2.6' : \"\" } . This snippet allows mosdepth to run off the executable present in the bin folder locally on Mac OSX, or use the conda-based installation when on Linux. Usage \u00b6 Testing on Quest \u00b6 This command uses a test dataset nextflow run andersenlab/alignment-nf --debug -profile quest Running on Quest \u00b6 You should run this in a screen session. Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. nextflow run andersenlab/alignment-nf --sample_sheet <path_to_sample_sheet> --species c_elegans -profile quest -resume Parameters \u00b6 -profile \u00b6 There are three configuration profiles for this pipeline. local - Used for local development. quest - Used for running on Quest. gcp - For running on Google Cloud (not currently active?). --sample_sheet \u00b6 The sample sheet for alignment is the output from the trim-fq-nf pipeline. The sample sheet must be tsv formatted , is the full path to the sample sheet (even if it is in your current directory), and has the following columns: strain - the name of the strain. Multiple sequencing runs of the same strain are merged together. id - A unique ID for each sequencing run. This must be unique for every single pair of FASTQs. lb - A library ID. This should uniquely identify a DNA sequencing library. fq1 - The path to FASTQ1 fq2 - The path to FASTQ2 Note Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . The library column is a useful tool for identifying errors by variant callers. For example, if the same library is sequenced twice, and a variant is only observed in one sequencing run then that variant may be excluded as a technical / PCR artifact depending on the variant caller being used. Important The alignment pipeline will merge multiple sequencing runs of the same strain into a single bam. However, summary output is provided at both the strain and id level. In this way, if there is a poor sequencing run it can be identified and removed from a collection of sequencing runs belonging to a strain. For this reason, it is important that each id be unique and not just the strain name Note The sample sheet is a critical tool. It allows us to associated metadata with each sequencing run (e.g. isotype, reference strain, id, library). It also allows us to quickly verify that all results have been output. It is much easier than working with a list of files! --debug (optional) \u00b6 You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder) using your specified configuration profile (e.g. local / quest / gcp). For example: nextflow run andersenlab/alignment-nf -profile quest --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv --species (optional) \u00b6 Defaults to \"c_elegans\", change to \"c_briggsae\" or \"c_tropicalis\" to select correct reference file. --fq_prefix (optional) \u00b6 Within a sample sheet you may specify the locations of FASTQs using an absolute directory or a relative directory. If you want to use a relative directory, you should use the --fq_prefix to set the path that should be prefixed to each FASTQ. Note Previously, this option was --fqs_file_prefix --kmers (optional) \u00b6 default = false Toggles kmer-analysis --reference (optional) \u00b6 A fasta reference indexed with BWA. WS245 is packaged with the pipeline for convenience when testing or running locally. On Quest, the default references are here: c_elegans: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz c_briggsae: /projects/b1059/data/c_briggsae/genomes/QX1410_nanopore/Feb2020/c_briggsae.QX1410_nanopore.Feb2020.genome.fa.gz c_tropicalis: /projects/b1059/data/c_tropicalis/genomes/NIC58_nanopore/June2021/c_tropicalis.NIC58_nanopore.June2021.genome.fa.gz Warning The current C. briggsae genome does not contain the Mitochondria DNA. This needs to be addressed later. Note A different --project and --wsbuild can be used with the --species parameter to generate the path to other reference genomes such as: nextflow run andersenlab/alignment-nf --species c_elegans --project PRJNA13758 --wsbuild WS280 --ncbi (optional) \u00b6 Default - /projects/b1059/data/other/ncbi_blast_db/ Path to the NCBI blast database used for blobtool analysis. Should not need to change. --output (optional) \u00b6 Default - alignment-YYYYMMDD A directory in which to output results. If you have set --debug true , the default output directory will be alignment-YYYYMMDD-debug . Output \u00b6 \u251c\u2500\u2500 _aggregate \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 multiqc \u2502 \u251c\u2500\u2500 strain_data/ \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-dist-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-per-contig_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-plot-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_picard_deduplication_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Normalised_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools_alignment_plot_1.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc.log \u2502 \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u2502 \u251c\u2500\u2500 multiqc_general_stats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_picard_dups.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_qualimap_bamqc_genome_results.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_flagstat.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_sources.txt \u2502 \u251c\u2500\u2500 strain_multiqc_report.html \u2502 \u251c\u2500\u2500 id_data/ \u2502 \u2502 \u2514\u2500\u2500... (same as strain_data/) \u2502 \u2514\u2500\u2500 id_multiqc_report.html \u251c\u2500\u2500 bam \u2502 \u251c\u2500\u2500 [strain].bam \u2502 \u2514\u2500\u2500 [strain].bam.bai \u251c\u2500\u2500 blobtools \u2502 \u251c\u2500\u2500 {strain}.*.blobplot.bam0.png \u2502 \u251c\u2500\u2500 {strain}.*.blobplot.read_cov.bam0.png \u2502 \u2514\u2500\u2500 {strain}.*.blobplot.stats.txt \u251c\u2500\u2500 software_versions.txt \u251c\u2500\u2500 sample_sheet.tsv \u251c\u2500\u2500 strain_summary.tsv \u251c\u2500\u2500 stats_strain_all.tsv \u251c\u2500\u2500 stats_strains_with_low_values.tsv \u251c\u2500\u2500 sample_sheet_for_seq_sheet.tsv \u251c\u2500\u2500 sample_sheet_for_seq_sheet_ALL.tsv \u251c\u2500\u2500 low_map_cov_for_seq_sheet.Rmd \u251c\u2500\u2500 low_map_cov_for_seq_sheet.html \u2514\u2500\u2500 summary.txt Most files should be obvious. A few are detailed below. software_versions.txt - Outputs the software versions used for every process (step) of the pipeline. summary.txt - Outputs a summary of the parameters used. sample_sheet.tsv - The sample sheet (input file) that was used to produce the alignment directory. strain_summary.tsv - A summary of all strains and bams in the alignment directory. aggregate - Stores data that has been aggregated across all strains or sequencing IDs. coverage - Contains coverage data at the strain or id level, presented in a variety of ways. low_map_cov_for_seq_sheet.(Rmd/html) - Report showing low coverage or problematic strains to remove. stats_strain_all.tsv - contains stats for all strains, with all replicates combined stats_strains_with_low_values.tsv - contains stats for strains with either (1) low number of reads, (2) low mapping rate, and/or (3) low coverage sample_sheet_for_seq_sheet.tsv - sample sheet to be added to google sheet, filtered to remove low coverage strains sample_sheet_for_seq_sheet_ALL.tsv - sample sheet to be added to google sheet, contains all strains (use this one) blobplot/ - contains plots for low coverage strains to see if they show contamination issues and if they should be resequenced. Data storage \u00b6 Cleanup \u00b6 Once the alignment-nf pipeline has completed successfully and you have removed low coverage strains (see pipeline overview ), all BAM files can be moved to /projects/b1059/data/{species}/WI/alignments/ prior to variant calling. Note Low coverage or otherwise problematic BAM files can be moved to /projects/b1059/data/{species}/WI/alignments/_bam_not_for_cendr/ . Make sure to update the _README.md file in this folder with the reason each BAM was moved here. This will help remind people which files might be used again in the future. Archive \u00b6 The following sections have been integrated into other code that no longer needs to be run manually, but I am keeping the documentation here in case we need to go back to it. It is important to always check that the sample sheet is generated appropriately. If there are errors in teh sample sheet, one can be constructed manually using the following code: construct_sample_sheet.sh \u00b6 The scripts/construct_sample_sheet.sh script generates the WI_sample_sheet.tsv file. Warning The WI_sample_sheet.tsv file should never be generated and/or edited by hand. It should only be generated using the scripts/construct_sample_sheet.tsv script. The construct_sample_sheet.sh script does a few things. (1) Parses FASTQ Filenames Unfortunately, no two sequencing centers are alike and they use different formats for naming sequencing files. For example: ECA768_RET-S11_S79_L001_2P.fq.gz [strain]_[lib_lib#]_[sample_#]_[lane]_[read].fq.gz XZ1734_S573_L007_2P.fq.gz [strain]_[sample_#]_[lane]_[read].fq.gz In some cases they even changed formats over time! The script parses the FASTQ filenames from different sequencing centers, extracting the strain name, and a unique ID. Note that the library and unique sequencing run ID ( id ) are named somewhat arbitrarily. The most imporant aspect of these columns is that any DNA library that has been sequenced multiple times possess the same library , and that every pair of FASTQs possess a unique sequencing ID. Consider the following (fake) example: strain isotype reference_strain id library AB1 AB1 TRUE BGI2-RET2-AB1 RET2 AB1 AB1 TRUE BGI2-RET3-AB1 RET3 AB4 CB4858 FALSE BGI1-RET2-AB4 RET2 AB4 CB4858 FALSE BGI2-RET2-AB4 RET2 AB1 was sequenced twice, however two different DNA libraries were produced for each sequencing run ( RET2 and RET3 ). AB4 was also sequenced twice, but both sequencing runs were of the same DNA library (called RET2 ). Note that the id column is always unique across all sequencing runs. If you look at the WI_sample_sheet.tsv in more detail you will observe that the id and library columns are not consistantly named. This is not ideal, but it works. The inconsistancy does not affect analysis, and exists because the filenames are not consistant, but unique library and sequencing run IDs must be derived from them. (2) Clean up strain names The second thing the construct_sample_sheet.sh script does is that it replaces shorthand strain names or innapropriately named strains with the 3-letter system. For example, N2Baer is renamed to ECA254 . (3) Integrate metadata The C. elegans WI Strain Info google spreadsheet is a master spreadlist containing every strain, reference_strain, and isotype for C. elegans wild isolates. The script downloads this dataset and uses it to integrate the isotype and reference strain into the sample sheet. Adding new sequencing datasets \u00b6 Sequencing data should be added to QUEST and processed through the trimming pipeline before being added to WI_sample_sheet.tsv . Before proceeding, be sure to read pipeline-trimming To add new sequencing datasets you will need to devise a strategy for extracting the strain name, a unique ID, and sequencing library from the FASTQ filenames. This may be the same as a past dataset, in which case you can append the sequencing run folder name to the list with that format. Alternatively, you may need to create a custom set of bash commands for generating the rows corresponding to each FASTQ pair. Here is an example from the construct_sample_sheet.sh script. #===================================# # BGI-20161012-ECA23 # #===================================# out=`mktemp` seq_folder=BGI-20161012-ECA23 >&2 echo ${seq_folder} prefix=${fastq_dir}/WI/dna/processed/$seq_folder for i in `ls -1 $prefix/*1P.fq.gz`; do bname=`basename ${i}`; barcode=`zcat ${i} | grep '@' | cut -f 10 -d ':' | sed 's/_//g' | head -n 100 | uniq -c | sort -k 1,1n | cut -c 9-100 | tail -n 1` echo -e \"${bname}\\t${i}\\t${barcode}\" >> ${out} done; cat ${out} |\\ awk -v prefix=${prefix} -v seq_folder=${seq_folder} '{ fq1 = $1; fq2 = $1; LB = $3; gsub(\"N\", \"\", LB); gsub(\"1P.fq.gz\", \"2P.fq.gz\", fq2); ID = $1; gsub(\"_1P.fq.gz\", \"\", ID); split(ID, a, \"[-_]\") SM=a[2]; print SM \"\\t\" ID \"\\t\" LB \"\\t\" prefix \"/\" fq1 \"\\t\" prefix \"/\" fq2 \"\\t\" seq_folder; }' >> ${fq_sheet} Notes on this snippet: SM = strain , LB = library , and ID = id in the final output file. The sequencing run is listed in the comment box at the top. Barcodes are extracted from each FASTQ in the first forloop. These are used to define the library . The id is defined using the basename of the file. A final column corresponding to the seq_folder is always added.","title":"alignment-nf"},{"location":"pipeline-alignment/#alignment-nf","text":"alignment-nf Pipeline overview Software requirements Usage Testing on Quest Running on Quest Parameters -profile --sample_sheet --debug (optional) --species (optional) --fq_prefix (optional) --kmers (optional) --reference (optional) --ncbi (optional) --output (optional) Output Data storage Cleanup Archive construct_sample_sheet.sh Adding new sequencing datasets The alignment-nf pipeline performs alignment for wild isolate sequence data at the strain level , and outputs BAMs and related information. Those BAMs can be used for downstream analysis including variant calling, concordance analysis , wi-gatk-nf (variant calling) and other analyses. This page details how to run the pipeline and how to add new wild isolate sequencing data. Note Historically, sequence processing was performed at the isotype level. We are still interested in filtering strains used in analysis at the isotype level, but alignment and variant calling are now performed at the strain level rather than at the isotype level.","title":"alignment-nf"},{"location":"pipeline-alignment/#pipeline_overview","text":"\u2597\u2596 \u259d\u259c \u259d \u2597 \u2597\u2596 \u2596\u2597\u2584\u2584\u2596 \u2590\u258c \u2590 \u2597\u2584 \u2584\u2584 \u2597\u2597\u2596 \u2597\u2584\u2584 \u2584\u2596 \u2597\u2597\u2596 \u2597\u259f\u2584 \u2590\u259a \u258c\u2590 \u258c\u2590 \u2590 \u2590 \u2590\u2598\u259c \u2590\u2598\u2590 \u2590\u2590\u2590 \u2590\u2598\u2590 \u2590\u2598\u2590 \u2590 \u2590\u2590\u2596\u258c\u2590\u2584\u2584\u2596 \u2599\u259f \u2590 \u2590 \u2590 \u2590 \u2590 \u2590 \u2590\u2590\u2590 \u2590\u2580\u2580 \u2590 \u2590 \u2590 \u2580\u2598 \u2590 \u258c\u258c\u2590 \u2590 \u258c \u259d\u2584 \u2597\u259f\u2584 \u259d\u2599\u259c \u2590 \u2590 \u2590\u2590\u2590 \u259d\u2599\u259e \u2590 \u2590 \u259d\u2584 \u2590 \u2590\u258c\u2590 \u2596\u2590 \u259d\u2598 parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --sample_sheet See test_data/sample_sheet for example null --species Species to map: 'ce', 'cb' or 'ct' null --fq_prefix Path to fastq if not in sample_sheet /projects/b1059/data/{species}/WI/fastq/dna/ --kmers Whether to count kmers false --reference genome.fasta.gz to use in place of default defaults for c.e, c.b, and c.t --output Output folder name. alignment-{date} HELP: http://andersenlab.org/dry-guide/pipeline-alignment/","title":"Pipeline overview"},{"location":"pipeline-alignment/#software_requirements","text":"Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Currently only runs on Quest with conda environments installed at /projects/b1059/software/conda_envs/ Note mosdepth is used to calculate coverage. mosdepth is available on Linux machines, but not on Mac OSX. That is why the conda environment for the coverage process is specified as conda { System.properties['os.name'] != \"Mac OS X\" ? 'bioconda::mosdepth=0.2.6' : \"\" } . This snippet allows mosdepth to run off the executable present in the bin folder locally on Mac OSX, or use the conda-based installation when on Linux.","title":"Software requirements"},{"location":"pipeline-alignment/#usage","text":"","title":"Usage"},{"location":"pipeline-alignment/#testing_on_quest","text":"This command uses a test dataset nextflow run andersenlab/alignment-nf --debug -profile quest","title":"Testing on Quest"},{"location":"pipeline-alignment/#running_on_quest","text":"You should run this in a screen session. Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. nextflow run andersenlab/alignment-nf --sample_sheet <path_to_sample_sheet> --species c_elegans -profile quest -resume","title":"Running on Quest"},{"location":"pipeline-alignment/#parameters","text":"","title":"Parameters"},{"location":"pipeline-alignment/#-profile","text":"There are three configuration profiles for this pipeline. local - Used for local development. quest - Used for running on Quest. gcp - For running on Google Cloud (not currently active?).","title":"-profile"},{"location":"pipeline-alignment/#--sample_sheet","text":"The sample sheet for alignment is the output from the trim-fq-nf pipeline. The sample sheet must be tsv formatted , is the full path to the sample sheet (even if it is in your current directory), and has the following columns: strain - the name of the strain. Multiple sequencing runs of the same strain are merged together. id - A unique ID for each sequencing run. This must be unique for every single pair of FASTQs. lb - A library ID. This should uniquely identify a DNA sequencing library. fq1 - The path to FASTQ1 fq2 - The path to FASTQ2 Note Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . The library column is a useful tool for identifying errors by variant callers. For example, if the same library is sequenced twice, and a variant is only observed in one sequencing run then that variant may be excluded as a technical / PCR artifact depending on the variant caller being used. Important The alignment pipeline will merge multiple sequencing runs of the same strain into a single bam. However, summary output is provided at both the strain and id level. In this way, if there is a poor sequencing run it can be identified and removed from a collection of sequencing runs belonging to a strain. For this reason, it is important that each id be unique and not just the strain name Note The sample sheet is a critical tool. It allows us to associated metadata with each sequencing run (e.g. isotype, reference strain, id, library). It also allows us to quickly verify that all results have been output. It is much easier than working with a list of files!","title":"--sample_sheet"},{"location":"pipeline-alignment/#--debug_optional","text":"You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder) using your specified configuration profile (e.g. local / quest / gcp). For example: nextflow run andersenlab/alignment-nf -profile quest --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv","title":"--debug (optional)"},{"location":"pipeline-alignment/#--species_optional","text":"Defaults to \"c_elegans\", change to \"c_briggsae\" or \"c_tropicalis\" to select correct reference file.","title":"--species (optional)"},{"location":"pipeline-alignment/#--fq_prefix_optional","text":"Within a sample sheet you may specify the locations of FASTQs using an absolute directory or a relative directory. If you want to use a relative directory, you should use the --fq_prefix to set the path that should be prefixed to each FASTQ. Note Previously, this option was --fqs_file_prefix","title":"--fq_prefix (optional)"},{"location":"pipeline-alignment/#--kmers_optional","text":"default = false Toggles kmer-analysis","title":"--kmers (optional)"},{"location":"pipeline-alignment/#--reference_optional","text":"A fasta reference indexed with BWA. WS245 is packaged with the pipeline for convenience when testing or running locally. On Quest, the default references are here: c_elegans: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz c_briggsae: /projects/b1059/data/c_briggsae/genomes/QX1410_nanopore/Feb2020/c_briggsae.QX1410_nanopore.Feb2020.genome.fa.gz c_tropicalis: /projects/b1059/data/c_tropicalis/genomes/NIC58_nanopore/June2021/c_tropicalis.NIC58_nanopore.June2021.genome.fa.gz Warning The current C. briggsae genome does not contain the Mitochondria DNA. This needs to be addressed later. Note A different --project and --wsbuild can be used with the --species parameter to generate the path to other reference genomes such as: nextflow run andersenlab/alignment-nf --species c_elegans --project PRJNA13758 --wsbuild WS280","title":"--reference (optional)"},{"location":"pipeline-alignment/#--ncbi_optional","text":"Default - /projects/b1059/data/other/ncbi_blast_db/ Path to the NCBI blast database used for blobtool analysis. Should not need to change.","title":"--ncbi (optional)"},{"location":"pipeline-alignment/#--output_optional","text":"Default - alignment-YYYYMMDD A directory in which to output results. If you have set --debug true , the default output directory will be alignment-YYYYMMDD-debug .","title":"--output (optional)"},{"location":"pipeline-alignment/#output","text":"\u251c\u2500\u2500 _aggregate \u2502 \u251c\u2500\u2500 kmers.tsv \u2502 \u2514\u2500\u2500 multiqc \u2502 \u251c\u2500\u2500 strain_data/ \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-dist-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-per-contig_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_mosdepth-coverage-plot-id_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_picard_deduplication_1.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools-idxstats-mapped-reads-plot_Normalised_Counts.txt \u2502 \u2502 \u251c\u2500\u2500 mqc_samtools_alignment_plot_1.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc.log \u2502 \u2502 \u251c\u2500\u2500 multiqc_data.json \u2502 \u2502 \u251c\u2500\u2500 multiqc_general_stats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_picard_dups.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_qualimap_bamqc_genome_results.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_flagstat.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_idxstats.txt \u2502 \u2502 \u251c\u2500\u2500 multiqc_samtools_stats.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_sources.txt \u2502 \u251c\u2500\u2500 strain_multiqc_report.html \u2502 \u251c\u2500\u2500 id_data/ \u2502 \u2502 \u2514\u2500\u2500... (same as strain_data/) \u2502 \u2514\u2500\u2500 id_multiqc_report.html \u251c\u2500\u2500 bam \u2502 \u251c\u2500\u2500 [strain].bam \u2502 \u2514\u2500\u2500 [strain].bam.bai \u251c\u2500\u2500 blobtools \u2502 \u251c\u2500\u2500 {strain}.*.blobplot.bam0.png \u2502 \u251c\u2500\u2500 {strain}.*.blobplot.read_cov.bam0.png \u2502 \u2514\u2500\u2500 {strain}.*.blobplot.stats.txt \u251c\u2500\u2500 software_versions.txt \u251c\u2500\u2500 sample_sheet.tsv \u251c\u2500\u2500 strain_summary.tsv \u251c\u2500\u2500 stats_strain_all.tsv \u251c\u2500\u2500 stats_strains_with_low_values.tsv \u251c\u2500\u2500 sample_sheet_for_seq_sheet.tsv \u251c\u2500\u2500 sample_sheet_for_seq_sheet_ALL.tsv \u251c\u2500\u2500 low_map_cov_for_seq_sheet.Rmd \u251c\u2500\u2500 low_map_cov_for_seq_sheet.html \u2514\u2500\u2500 summary.txt Most files should be obvious. A few are detailed below. software_versions.txt - Outputs the software versions used for every process (step) of the pipeline. summary.txt - Outputs a summary of the parameters used. sample_sheet.tsv - The sample sheet (input file) that was used to produce the alignment directory. strain_summary.tsv - A summary of all strains and bams in the alignment directory. aggregate - Stores data that has been aggregated across all strains or sequencing IDs. coverage - Contains coverage data at the strain or id level, presented in a variety of ways. low_map_cov_for_seq_sheet.(Rmd/html) - Report showing low coverage or problematic strains to remove. stats_strain_all.tsv - contains stats for all strains, with all replicates combined stats_strains_with_low_values.tsv - contains stats for strains with either (1) low number of reads, (2) low mapping rate, and/or (3) low coverage sample_sheet_for_seq_sheet.tsv - sample sheet to be added to google sheet, filtered to remove low coverage strains sample_sheet_for_seq_sheet_ALL.tsv - sample sheet to be added to google sheet, contains all strains (use this one) blobplot/ - contains plots for low coverage strains to see if they show contamination issues and if they should be resequenced.","title":"Output"},{"location":"pipeline-alignment/#data_storage","text":"","title":"Data storage"},{"location":"pipeline-alignment/#cleanup","text":"Once the alignment-nf pipeline has completed successfully and you have removed low coverage strains (see pipeline overview ), all BAM files can be moved to /projects/b1059/data/{species}/WI/alignments/ prior to variant calling. Note Low coverage or otherwise problematic BAM files can be moved to /projects/b1059/data/{species}/WI/alignments/_bam_not_for_cendr/ . Make sure to update the _README.md file in this folder with the reason each BAM was moved here. This will help remind people which files might be used again in the future.","title":"Cleanup"},{"location":"pipeline-alignment/#archive","text":"The following sections have been integrated into other code that no longer needs to be run manually, but I am keeping the documentation here in case we need to go back to it. It is important to always check that the sample sheet is generated appropriately. If there are errors in teh sample sheet, one can be constructed manually using the following code:","title":"Archive"},{"location":"pipeline-alignment/#construct_sample_sheetsh","text":"The scripts/construct_sample_sheet.sh script generates the WI_sample_sheet.tsv file. Warning The WI_sample_sheet.tsv file should never be generated and/or edited by hand. It should only be generated using the scripts/construct_sample_sheet.tsv script. The construct_sample_sheet.sh script does a few things. (1) Parses FASTQ Filenames Unfortunately, no two sequencing centers are alike and they use different formats for naming sequencing files. For example: ECA768_RET-S11_S79_L001_2P.fq.gz [strain]_[lib_lib#]_[sample_#]_[lane]_[read].fq.gz XZ1734_S573_L007_2P.fq.gz [strain]_[sample_#]_[lane]_[read].fq.gz In some cases they even changed formats over time! The script parses the FASTQ filenames from different sequencing centers, extracting the strain name, and a unique ID. Note that the library and unique sequencing run ID ( id ) are named somewhat arbitrarily. The most imporant aspect of these columns is that any DNA library that has been sequenced multiple times possess the same library , and that every pair of FASTQs possess a unique sequencing ID. Consider the following (fake) example: strain isotype reference_strain id library AB1 AB1 TRUE BGI2-RET2-AB1 RET2 AB1 AB1 TRUE BGI2-RET3-AB1 RET3 AB4 CB4858 FALSE BGI1-RET2-AB4 RET2 AB4 CB4858 FALSE BGI2-RET2-AB4 RET2 AB1 was sequenced twice, however two different DNA libraries were produced for each sequencing run ( RET2 and RET3 ). AB4 was also sequenced twice, but both sequencing runs were of the same DNA library (called RET2 ). Note that the id column is always unique across all sequencing runs. If you look at the WI_sample_sheet.tsv in more detail you will observe that the id and library columns are not consistantly named. This is not ideal, but it works. The inconsistancy does not affect analysis, and exists because the filenames are not consistant, but unique library and sequencing run IDs must be derived from them. (2) Clean up strain names The second thing the construct_sample_sheet.sh script does is that it replaces shorthand strain names or innapropriately named strains with the 3-letter system. For example, N2Baer is renamed to ECA254 . (3) Integrate metadata The C. elegans WI Strain Info google spreadsheet is a master spreadlist containing every strain, reference_strain, and isotype for C. elegans wild isolates. The script downloads this dataset and uses it to integrate the isotype and reference strain into the sample sheet.","title":"construct_sample_sheet.sh"},{"location":"pipeline-alignment/#adding_new_sequencing_datasets","text":"Sequencing data should be added to QUEST and processed through the trimming pipeline before being added to WI_sample_sheet.tsv . Before proceeding, be sure to read pipeline-trimming To add new sequencing datasets you will need to devise a strategy for extracting the strain name, a unique ID, and sequencing library from the FASTQ filenames. This may be the same as a past dataset, in which case you can append the sequencing run folder name to the list with that format. Alternatively, you may need to create a custom set of bash commands for generating the rows corresponding to each FASTQ pair. Here is an example from the construct_sample_sheet.sh script. #===================================# # BGI-20161012-ECA23 # #===================================# out=`mktemp` seq_folder=BGI-20161012-ECA23 >&2 echo ${seq_folder} prefix=${fastq_dir}/WI/dna/processed/$seq_folder for i in `ls -1 $prefix/*1P.fq.gz`; do bname=`basename ${i}`; barcode=`zcat ${i} | grep '@' | cut -f 10 -d ':' | sed 's/_//g' | head -n 100 | uniq -c | sort -k 1,1n | cut -c 9-100 | tail -n 1` echo -e \"${bname}\\t${i}\\t${barcode}\" >> ${out} done; cat ${out} |\\ awk -v prefix=${prefix} -v seq_folder=${seq_folder} '{ fq1 = $1; fq2 = $1; LB = $3; gsub(\"N\", \"\", LB); gsub(\"1P.fq.gz\", \"2P.fq.gz\", fq2); ID = $1; gsub(\"_1P.fq.gz\", \"\", ID); split(ID, a, \"[-_]\") SM=a[2]; print SM \"\\t\" ID \"\\t\" LB \"\\t\" prefix \"/\" fq1 \"\\t\" prefix \"/\" fq2 \"\\t\" seq_folder; }' >> ${fq_sheet} Notes on this snippet: SM = strain , LB = library , and ID = id in the final output file. The sequencing run is listed in the comment box at the top. Barcodes are extracted from each FASTQ in the first forloop. These are used to define the library . The id is defined using the basename of the file. A final column corresponding to the seq_folder is always added.","title":"Adding new sequencing datasets"},{"location":"pipeline-annotation-nf/","text":"annotation-nf \u00b6 annotation-nf Pipeline overview Software Requirements Usage Testing on Quest Running on Quest Parameters --debug --vcf --species --divergent_regions --reference, --project, --ws_build (optional) --ncsq_param (optional) Output Data storage Cleanup Updating CeNDR Updating NemaScan The annotation-nf pipeline performs variant annotation for the VCF at the isotype level using both SnpEff and BCFtools/csq (BCSQ) . Note Before running, make sure to check out the genomes-nf pipeline to ensure that the reference genome and annotation databases are set up properly. Pipeline overview \u00b6 ------------- ANNOTATION-NF ------------- nextflow andersenlab/annotation-nf --debug nextflow andersenlab/annotation-nf --vcf=hard-filtered.vcf --species=c_elegans --divergent_regions=divergent_regions_strain.bed parameters description Set/Default ========== =========== ======================== --debug Set to 'true' to test ${params.debug} --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' ${params.species} --vcf hard filtered vcf to calculate variant density ${params.vcf} --divergent_regions (Optional) Divergent region bed file ${params.divergent_regions} --reference Reference used based on species and project ${params.reference} --output (Optional) output folder name ${params.output} username ${\"whoami\".execute().in.text} HELP: http://andersenlab.org/dry-guide/pipeline-annotation-nf Software Requirements \u00b6 The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20_env conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update Important This pipeline currently only supports analysis on Quest, cannot be run locally Usage \u00b6 Note For more info about running Nextflow pipelines in the Andersen Lab, check out this page Testing on Quest \u00b6 This command uses a test dataset nextflow run andersenlab/annotation-nf --debug Running on Quest \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. You should run this in a screen session. nextflow run andersenlab/annotation-nf --vcf <path_to_vcf> --species <species> --divergent_regions <path_to_file> Parameters \u00b6 --debug \u00b6 You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run andersenlab/annotation-nf --debug --vcf \u00b6 Path to the hard-filter, isotype VCF (output from post-gatk-nf ) --species \u00b6 Choose from c_elegans, c_briggsae, or c_tropicalis. Species will specifiy a default reference genome. You can select a different one if you prefer (see below) --divergent_regions \u00b6 This is the divergent_regions_strain.bed file output from the post-gatk-nf pipeline. This file is used to add a column to the flat file if the variant is within a divergent region. Currently, C. elegans is the only species with divergent regions, if running for another species, do not provide a divergent_regions file and the pipeline will ignore it. --reference, --project, --ws_build (optional) \u00b6 By default, the reference genome is set by the species parameter. If you don't want to use the default, you could change the project and/or ws_build. As long as the genome is in the proper location on quest (for more, see the genomes-nf pipeline), this will work. Alternatively, you could provide the path to a reference of your choice. Defaults: - C. elegans - /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz - C. briggsae - /projects/b1059/data/c_briggsae/genomes/QX1410_nanopore/Feb2020/c_briggsae.QX1410_nanopore.Feb2020.genome.fa.gz - C. tropicalis - /projects/b1059/data/c_tropicalis/genomes/NIC58_nanopore/June2021/c_tropicalis.NIC58_nanopore.June2021.genome.fa.gz --ncsq_param (optional) \u00b6 This parameter is necessary for correct annotation using BCSQ for variants with many different annotations (like found in divergent regions). In 20210121 we found that the default value of 224 was sufficient, but as more strains are added this number might need to increase. If there is an issue, you should see a warning error from BCFtools and they should suggest what to change this parameter to. Output \u00b6 \u251c\u2500\u2500 strain_vcf \u2502 \u251c\u2500\u2500 {strain}.{date}.vcf.gz \u2502 \u2514\u2500\u2500 {strain}.{date}.vcf.gz.tbi \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz.tbi \u251c\u2500\u2500 snpeff.stats.csv \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.stats.txt \u2514\u2500\u2500 WI.{date}.strain-annotation.bcsq.tsv Data storage \u00b6 Cleanup \u00b6 Once the pipeline has complete successfully and you are satisfied with the results, the final data can be moved to their final storage place on Quest accordingly: Both the strain_vcf and the variation folders can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf If applicable, all snpeff .bed files (HIGH, LOW, MODERATE, etc.) can be moved to /projects/b1059/data/{species}/WI/variation/{date}/tracks/ ( As of 20210901 this is no longer being produced for CeNDR ) Updating CeNDR \u00b6 Check out the CeNDR page for more information about updating a new data release for CeNDR. Updating NemaScan \u00b6 Once a new CeNDR release is ready, it is important to also update the genome-wide association mapping packages to ensure users can appropriately analyze data from new strains as well as old strains. Here is a list of things that need to be updated: The default vcf should be changed to the newest release date (i.e. from 20200815 to 20210121). Users will still have the option to use an earlier vcf. Ensure strain annotation flat file is stored in the proper file location to be accessed, both on QUEST and on GCP for CeNDR/local users.","title":"annotation-nf"},{"location":"pipeline-annotation-nf/#annotation-nf","text":"annotation-nf Pipeline overview Software Requirements Usage Testing on Quest Running on Quest Parameters --debug --vcf --species --divergent_regions --reference, --project, --ws_build (optional) --ncsq_param (optional) Output Data storage Cleanup Updating CeNDR Updating NemaScan The annotation-nf pipeline performs variant annotation for the VCF at the isotype level using both SnpEff and BCFtools/csq (BCSQ) . Note Before running, make sure to check out the genomes-nf pipeline to ensure that the reference genome and annotation databases are set up properly.","title":"annotation-nf"},{"location":"pipeline-annotation-nf/#pipeline_overview","text":"------------- ANNOTATION-NF ------------- nextflow andersenlab/annotation-nf --debug nextflow andersenlab/annotation-nf --vcf=hard-filtered.vcf --species=c_elegans --divergent_regions=divergent_regions_strain.bed parameters description Set/Default ========== =========== ======================== --debug Set to 'true' to test ${params.debug} --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' ${params.species} --vcf hard filtered vcf to calculate variant density ${params.vcf} --divergent_regions (Optional) Divergent region bed file ${params.divergent_regions} --reference Reference used based on species and project ${params.reference} --output (Optional) output folder name ${params.output} username ${\"whoami\".execute().in.text} HELP: http://andersenlab.org/dry-guide/pipeline-annotation-nf","title":"Pipeline overview"},{"location":"pipeline-annotation-nf/#software_requirements","text":"The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20_env conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update Important This pipeline currently only supports analysis on Quest, cannot be run locally","title":"Software Requirements"},{"location":"pipeline-annotation-nf/#usage","text":"Note For more info about running Nextflow pipelines in the Andersen Lab, check out this page","title":"Usage"},{"location":"pipeline-annotation-nf/#testing_on_quest","text":"This command uses a test dataset nextflow run andersenlab/annotation-nf --debug","title":"Testing on Quest"},{"location":"pipeline-annotation-nf/#running_on_quest","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. You should run this in a screen session. nextflow run andersenlab/annotation-nf --vcf <path_to_vcf> --species <species> --divergent_regions <path_to_file>","title":"Running on Quest"},{"location":"pipeline-annotation-nf/#parameters","text":"","title":"Parameters"},{"location":"pipeline-annotation-nf/#--debug","text":"You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run andersenlab/annotation-nf --debug","title":"--debug"},{"location":"pipeline-annotation-nf/#--vcf","text":"Path to the hard-filter, isotype VCF (output from post-gatk-nf )","title":"--vcf"},{"location":"pipeline-annotation-nf/#--species","text":"Choose from c_elegans, c_briggsae, or c_tropicalis. Species will specifiy a default reference genome. You can select a different one if you prefer (see below)","title":"--species"},{"location":"pipeline-annotation-nf/#--divergent_regions","text":"This is the divergent_regions_strain.bed file output from the post-gatk-nf pipeline. This file is used to add a column to the flat file if the variant is within a divergent region. Currently, C. elegans is the only species with divergent regions, if running for another species, do not provide a divergent_regions file and the pipeline will ignore it.","title":"--divergent_regions"},{"location":"pipeline-annotation-nf/#--reference_--project_--ws_build_optional","text":"By default, the reference genome is set by the species parameter. If you don't want to use the default, you could change the project and/or ws_build. As long as the genome is in the proper location on quest (for more, see the genomes-nf pipeline), this will work. Alternatively, you could provide the path to a reference of your choice. Defaults: - C. elegans - /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz - C. briggsae - /projects/b1059/data/c_briggsae/genomes/QX1410_nanopore/Feb2020/c_briggsae.QX1410_nanopore.Feb2020.genome.fa.gz - C. tropicalis - /projects/b1059/data/c_tropicalis/genomes/NIC58_nanopore/June2021/c_tropicalis.NIC58_nanopore.June2021.genome.fa.gz","title":"--reference, --project, --ws_build (optional)"},{"location":"pipeline-annotation-nf/#--ncsq_param_optional","text":"This parameter is necessary for correct annotation using BCSQ for variants with many different annotations (like found in divergent regions). In 20210121 we found that the default value of 224 was sufficient, but as more strains are added this number might need to increase. If there is an issue, you should see a warning error from BCFtools and they should suggest what to change this parameter to.","title":"--ncsq_param (optional)"},{"location":"pipeline-annotation-nf/#output","text":"\u251c\u2500\u2500 strain_vcf \u2502 \u251c\u2500\u2500 {strain}.{date}.vcf.gz \u2502 \u2514\u2500\u2500 {strain}.{date}.vcf.gz.tbi \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.snpeff.vcf.gz.tbi \u251c\u2500\u2500 snpeff.stats.csv \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.bcsq.vcf.gz.stats.txt \u2514\u2500\u2500 WI.{date}.strain-annotation.bcsq.tsv","title":"Output"},{"location":"pipeline-annotation-nf/#data_storage","text":"","title":"Data storage"},{"location":"pipeline-annotation-nf/#cleanup","text":"Once the pipeline has complete successfully and you are satisfied with the results, the final data can be moved to their final storage place on Quest accordingly: Both the strain_vcf and the variation folders can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf If applicable, all snpeff .bed files (HIGH, LOW, MODERATE, etc.) can be moved to /projects/b1059/data/{species}/WI/variation/{date}/tracks/ ( As of 20210901 this is no longer being produced for CeNDR )","title":"Cleanup"},{"location":"pipeline-annotation-nf/#updating_cendr","text":"Check out the CeNDR page for more information about updating a new data release for CeNDR.","title":"Updating CeNDR"},{"location":"pipeline-annotation-nf/#updating_nemascan","text":"Once a new CeNDR release is ready, it is important to also update the genome-wide association mapping packages to ensure users can appropriately analyze data from new strains as well as old strains. Here is a list of things that need to be updated: The default vcf should be changed to the newest release date (i.e. from 20200815 to 20210121). Users will still have the option to use an earlier vcf. Ensure strain annotation flat file is stored in the proper file location to be accessed, both on QUEST and on GCP for CeNDR/local users.","title":"Updating NemaScan"},{"location":"pipeline-cegwas/","text":"cegwas2-nf \u00b6 Warning As of 2021, cegwas2-nf is being phased out for NemaScan . If you are looking to run a genome-wide association mapping, please check out NemaScan as all new updates will be there. cegwas2-nf Overview of the workflow Required software for running on QUEST Required software for running outside of QUEST Required data for running outside of QUEST Testing pipeline using Nextflow Execution of pipeline using Nextflow Profiles Parameters Optional parameters R scripts Output Folder Structure Phenotypes folder Genotype_Matrix folder Mappings folder Data Plots Fine_Mappings folder Data Plots BURDEN folder (Contains two subfolders VT/SKAT with the same structure) Data Plots GWA mapping with C. elegans Overview of the workflow \u00b6 Required software for running on QUEST \u00b6 nextflow-v20.0+ Users can either update Nextflow to the newest version to run OR load a conda environment for Nextflow v20 using the following commands: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Required software for running outside of QUEST \u00b6 These packages should be in the user's PATH R-v3.6.0 nextflow-v20.0+ BCFtools-v1.9 plink-v1.9 bedtools-2.29.2 R-cegwas2 R-tidyverse-v1.2.1 R-coop-0.6-2 R-rrBLUP-v4.6 R-plotly-4.9.2 R-DT-0.12 R-data.table-1.12.8 R-Rcpp-1.0.1 R-genetics-1.3.8.1.2 R-sommer-4.0.4 R-RSpectra-v0.13-1 pandoc=2.12 R-knitr-1.28 R-rmarkdown-2.1 R-cowplot-1.0.0 R-ggbeeswarm-v0.6 Required data for running outside of QUEST \u00b6 VCF(s) - A hard-filtered vcf containing phenotyped samples for mapping - A tabix-generated index hard-filtered vcf (.tbi) - An imputed vcf Testing pipeline using Nextflow \u00b6 Running debug mode is a good way to quickly test if your environment is set up correctly. Entire debug run should take 2-3 minutes. git clone https://github.com/AndersenLab/cegwas2-nf.git cd cegwas2-nf nextflow main.nf --debug Execution of pipeline using Nextflow \u00b6 git clone https://github.com/AndersenLab/cegwas2-nf.git cd cegwas2-nf nextflow main.nf --traitfile <path to traitfile> --annotation bcsq [optional parameters, see below] Profiles \u00b6 Users can select from a number of profiles that each run different processes for the analysis: -profile standard - This is the default profile if no profile is included. This profile runs the GWA mapping, fine mapping, burden mapping, and generates plots and dataframes in the output folder. -profile manplot_only - This profile only runs the GWA mapping and generates manhattan and phenotype-by-genotype plots for each trait. No fine mapping or burden mapping is performed. This profile is good for users with hundreds of traits. -profile reports - This profile runs all the same processes as the standard profile with an additional analysis for haplotypes and divergent regions in the QTL. In addition to the standard outputs, this profile also outputs an html markdown report for each trait with the markdown file that can be run independently on a local computer to reproduce the output files and figures. This profile works best with <30 traits. Parameters \u00b6 --traitfile - is a tab-delimited formatted (.tsv) file that contains trait information. Each phenotype file should be in the following format (replace trait_name with the phenotype of interest): strain trait_name_1 trait_name_2 JU258 32.73 19.34 ECA640 34.065378 12.32 ... ... ... ECA250 34.096 23.1 --annotation - Users can choose between \"snpeff\" annotation or \"bcsq\" annotation. Currently, annotation is only set up with the 20210121 release. nextflow main.nf --help - will display the help message Optional parameters \u00b6 --vcf - CeNDR release date for the VCF file with variant data. Default is \"20210121\". Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. --sthresh - This determines the signficance threshold required for performing post-mapping analysis of a QTL. BF corresponds to Bonferroni correction (DEFAULT), EIGEN corresponds to correcting for the number of independent markers in your data set, and user-specified corresponds to a user-defined threshold, where you replace user-specified with a number. For example --sthresh=4 will set the threshold to a -log10(p) value of 4. We recommend using the strict BF correction as a first pass to see what the resulting data looks like. If the pipeline stops at the summarize_maps process, no significant QTL were discovered with the input threshold. You might want to consider lowering the threshold if this occurs. --p3d - This determines what type of kinship correction to perform prior to mapping. TRUE corresponds to the EMMAx method and FALSE corresponds to the slower EMMA method. We recommend running with --p3d=TRUE to make sure all files of the required files are present and in the proper format, then run with --p3d=FALSE (DEFAULT) for a more exact mapping. --freqUpper - Upper bound for variant allele frequency for a variant to be considered for burden mapping. Default = 0.5 --minburden - The number of strains that must share a variant for that variant to be considered for burden mapping. Default = 2 --refflat - Genomic locations for genes used for burden mapping. A default generated from WS245 is provided in the repositories bin. --genes - Genomic locations for genes formatted for plotting purposes. A default generated from WS245 is provided in the repositories bin. R scripts \u00b6 Get_GenoMatrix_Eigen.R - Takes a genotype matrix and chromosome name as input and identifies the number significant eigenvalues. Fix_Isotype_names_bulk.R - Take sample names present in phenotype data and changes them to isotype names found on CeNDR . Run_Mappings.R - Performs GWA mapping using the rrBLUP R package and the EMMA or EMMAx algorithm for kinship correction. Generates manhattan plot and phenotype by genotype plot for peak positions. Summarize_Mappings.R - Generates plot of all QTL identified in nextflow pipeline. Finemap_QTL_Intervals.R - Run EMMA/EMMAx on QTL region of interest. Generates fine map plot, colored by LD with peak QTL SNV found from genome-wide scan plot_genes.R - Runs SnpEff and generates gene plot. makeped.R - Converts trait .tsv files to .ped format for burden mapping. rvtest - Executable to run burden mapping, can be found at the RVtests homepage plot_burden.R - Plots the results from burden mapping. Output Folder Structure \u00b6 Analysis_Results-{Date} | \u251c\u2500\u2500cegwas2_report_traitname_main.html \u251c\u2500\u2500cegwas2_report_traitname_main.Rmd | \u251c\u2500\u2500Phenotypes \u251c\u2500\u2500 strain_issues.txt \u251c\u2500\u2500 pr_traitname.tsv \u251c\u2500\u2500Genotype_Matrix \u251c\u2500\u2500 Genotype_Matrix.tsv \u251c\u2500\u2500 total_independent_tests.txt \u251c\u2500\u2500Mappings \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname_processed_mapping.tsv \u251c\u2500\u2500 QTL_peaks.tsv \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_manplot.pdf \u251c\u2500\u2500 traitname_pxgplot.pdf \u251c\u2500\u2500 Summarized_mappings.pdf \u251c\u2500\u2500Fine_Mappings \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname_snpeff_genes.tsv \u251c\u2500\u2500 traitname_qtlinterval_prLD_df.tsv \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_qtlinterval_finemap_plot.pdf \u251c\u2500\u2500 traitname_qtlinterval_gene_plot.pdf \u251c\u2500\u2500Divergent_and_haplotype \u251c\u2500\u2500all_QTL_bins.bed \u251c\u2500\u2500all_QTL_div.bed \u251c\u2500\u2500div_isotype_list.txt \u251c\u2500\u2500haplotype_in_QTL_region.txt \u251c\u2500\u2500BURDEN \u251c\u2500\u2500 VT \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname.VariableThresholdPrice.assoc \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_VTprice.pdf \u251c\u2500\u2500 SKAT \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname.Skat.assoc \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_SKAT.pdf Phenotypes folder \u00b6 strain_issues.txt - Output of any strain names that were changed to match vcf (i.e. isotypes that are not reference strains) pr_traitname.tsv - Processed phenotype file for each trait. This is the file that goes into the mapping Genotype_Matrix folder \u00b6 Genotype_Matrix.tsv - pruned LD-pruned genotype matrix used for GWAS and construction of kinship matrix total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix Mappings folder \u00b6 Data \u00b6 traitname_processed_mapping.tsv - Processed mapping data frame for each trait mapped QTL_peaks.tsv - List of signifcant QTL identified across all traits Plots \u00b6 traitname_manplot.pdf - Manhattan plot for each trait that was analyzed. Two significance threshold lines are present, one for the Bonferronit corrected threshold, and another for the spectral decomposition threshold. traitname_pxgplot.pdf - Phenotype by genotype split at peak QTL positions for every significant QTL identified Summarized_mappings.pdf - A summary plot of all QTL identified Fine_Mappings folder \u00b6 Data \u00b6 traitname_snpeff_genes.tsv - Fine-mapping data frame for all significant QTL Plots \u00b6 traitname_qtlinterval_finemap_plot.pdf - Fine map plot of QTL interval, colored by marker LD with the peak QTL identified from the genome-wide scan traitname_qtlinterval_gene_plot.pdf - variant annotation plot overlaid with gene CDS for QTL interval BURDEN folder (Contains two subfolders VT/SKAT with the same structure) \u00b6 Data \u00b6 traitname.VariableThresholdPrice.assoc - Genome-wide burden mapping result using VT price, see RVtests homepage traitname.Skat.assoc - Genome-wide burden mapping result using Skat, see RVtests homepage Plots \u00b6 traitname_VTprice.pdf - Genome-wide burden mapping manhattan plot for VTprice traitname_SKAT.pdf - Genome-wide burden mapping manhattan plot for Skat","title":"cegwas2-nf"},{"location":"pipeline-cegwas/#cegwas2-nf","text":"Warning As of 2021, cegwas2-nf is being phased out for NemaScan . If you are looking to run a genome-wide association mapping, please check out NemaScan as all new updates will be there. cegwas2-nf Overview of the workflow Required software for running on QUEST Required software for running outside of QUEST Required data for running outside of QUEST Testing pipeline using Nextflow Execution of pipeline using Nextflow Profiles Parameters Optional parameters R scripts Output Folder Structure Phenotypes folder Genotype_Matrix folder Mappings folder Data Plots Fine_Mappings folder Data Plots BURDEN folder (Contains two subfolders VT/SKAT with the same structure) Data Plots GWA mapping with C. elegans","title":"cegwas2-nf"},{"location":"pipeline-cegwas/#overview_of_the_workflow","text":"","title":"Overview of the workflow"},{"location":"pipeline-cegwas/#required_software_for_running_on_quest","text":"nextflow-v20.0+ Users can either update Nextflow to the newest version to run OR load a conda environment for Nextflow v20 using the following commands: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env","title":"Required software for running on QUEST"},{"location":"pipeline-cegwas/#required_software_for_running_outside_of_quest","text":"These packages should be in the user's PATH R-v3.6.0 nextflow-v20.0+ BCFtools-v1.9 plink-v1.9 bedtools-2.29.2 R-cegwas2 R-tidyverse-v1.2.1 R-coop-0.6-2 R-rrBLUP-v4.6 R-plotly-4.9.2 R-DT-0.12 R-data.table-1.12.8 R-Rcpp-1.0.1 R-genetics-1.3.8.1.2 R-sommer-4.0.4 R-RSpectra-v0.13-1 pandoc=2.12 R-knitr-1.28 R-rmarkdown-2.1 R-cowplot-1.0.0 R-ggbeeswarm-v0.6","title":"Required software for running outside of QUEST"},{"location":"pipeline-cegwas/#required_data_for_running_outside_of_quest","text":"VCF(s) - A hard-filtered vcf containing phenotyped samples for mapping - A tabix-generated index hard-filtered vcf (.tbi) - An imputed vcf","title":"Required data for running outside of QUEST"},{"location":"pipeline-cegwas/#testing_pipeline_using_nextflow","text":"Running debug mode is a good way to quickly test if your environment is set up correctly. Entire debug run should take 2-3 minutes. git clone https://github.com/AndersenLab/cegwas2-nf.git cd cegwas2-nf nextflow main.nf --debug","title":"Testing pipeline using Nextflow"},{"location":"pipeline-cegwas/#execution_of_pipeline_using_nextflow","text":"git clone https://github.com/AndersenLab/cegwas2-nf.git cd cegwas2-nf nextflow main.nf --traitfile <path to traitfile> --annotation bcsq [optional parameters, see below]","title":"Execution of pipeline using Nextflow"},{"location":"pipeline-cegwas/#profiles","text":"Users can select from a number of profiles that each run different processes for the analysis: -profile standard - This is the default profile if no profile is included. This profile runs the GWA mapping, fine mapping, burden mapping, and generates plots and dataframes in the output folder. -profile manplot_only - This profile only runs the GWA mapping and generates manhattan and phenotype-by-genotype plots for each trait. No fine mapping or burden mapping is performed. This profile is good for users with hundreds of traits. -profile reports - This profile runs all the same processes as the standard profile with an additional analysis for haplotypes and divergent regions in the QTL. In addition to the standard outputs, this profile also outputs an html markdown report for each trait with the markdown file that can be run independently on a local computer to reproduce the output files and figures. This profile works best with <30 traits.","title":"Profiles"},{"location":"pipeline-cegwas/#parameters","text":"--traitfile - is a tab-delimited formatted (.tsv) file that contains trait information. Each phenotype file should be in the following format (replace trait_name with the phenotype of interest): strain trait_name_1 trait_name_2 JU258 32.73 19.34 ECA640 34.065378 12.32 ... ... ... ECA250 34.096 23.1 --annotation - Users can choose between \"snpeff\" annotation or \"bcsq\" annotation. Currently, annotation is only set up with the 20210121 release. nextflow main.nf --help - will display the help message","title":"Parameters"},{"location":"pipeline-cegwas/#optional_parameters","text":"--vcf - CeNDR release date for the VCF file with variant data. Default is \"20210121\". Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. --sthresh - This determines the signficance threshold required for performing post-mapping analysis of a QTL. BF corresponds to Bonferroni correction (DEFAULT), EIGEN corresponds to correcting for the number of independent markers in your data set, and user-specified corresponds to a user-defined threshold, where you replace user-specified with a number. For example --sthresh=4 will set the threshold to a -log10(p) value of 4. We recommend using the strict BF correction as a first pass to see what the resulting data looks like. If the pipeline stops at the summarize_maps process, no significant QTL were discovered with the input threshold. You might want to consider lowering the threshold if this occurs. --p3d - This determines what type of kinship correction to perform prior to mapping. TRUE corresponds to the EMMAx method and FALSE corresponds to the slower EMMA method. We recommend running with --p3d=TRUE to make sure all files of the required files are present and in the proper format, then run with --p3d=FALSE (DEFAULT) for a more exact mapping. --freqUpper - Upper bound for variant allele frequency for a variant to be considered for burden mapping. Default = 0.5 --minburden - The number of strains that must share a variant for that variant to be considered for burden mapping. Default = 2 --refflat - Genomic locations for genes used for burden mapping. A default generated from WS245 is provided in the repositories bin. --genes - Genomic locations for genes formatted for plotting purposes. A default generated from WS245 is provided in the repositories bin.","title":"Optional parameters"},{"location":"pipeline-cegwas/#r_scripts","text":"Get_GenoMatrix_Eigen.R - Takes a genotype matrix and chromosome name as input and identifies the number significant eigenvalues. Fix_Isotype_names_bulk.R - Take sample names present in phenotype data and changes them to isotype names found on CeNDR . Run_Mappings.R - Performs GWA mapping using the rrBLUP R package and the EMMA or EMMAx algorithm for kinship correction. Generates manhattan plot and phenotype by genotype plot for peak positions. Summarize_Mappings.R - Generates plot of all QTL identified in nextflow pipeline. Finemap_QTL_Intervals.R - Run EMMA/EMMAx on QTL region of interest. Generates fine map plot, colored by LD with peak QTL SNV found from genome-wide scan plot_genes.R - Runs SnpEff and generates gene plot. makeped.R - Converts trait .tsv files to .ped format for burden mapping. rvtest - Executable to run burden mapping, can be found at the RVtests homepage plot_burden.R - Plots the results from burden mapping.","title":"R scripts"},{"location":"pipeline-cegwas/#output_folder_structure","text":"Analysis_Results-{Date} | \u251c\u2500\u2500cegwas2_report_traitname_main.html \u251c\u2500\u2500cegwas2_report_traitname_main.Rmd | \u251c\u2500\u2500Phenotypes \u251c\u2500\u2500 strain_issues.txt \u251c\u2500\u2500 pr_traitname.tsv \u251c\u2500\u2500Genotype_Matrix \u251c\u2500\u2500 Genotype_Matrix.tsv \u251c\u2500\u2500 total_independent_tests.txt \u251c\u2500\u2500Mappings \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname_processed_mapping.tsv \u251c\u2500\u2500 QTL_peaks.tsv \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_manplot.pdf \u251c\u2500\u2500 traitname_pxgplot.pdf \u251c\u2500\u2500 Summarized_mappings.pdf \u251c\u2500\u2500Fine_Mappings \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname_snpeff_genes.tsv \u251c\u2500\u2500 traitname_qtlinterval_prLD_df.tsv \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_qtlinterval_finemap_plot.pdf \u251c\u2500\u2500 traitname_qtlinterval_gene_plot.pdf \u251c\u2500\u2500Divergent_and_haplotype \u251c\u2500\u2500all_QTL_bins.bed \u251c\u2500\u2500all_QTL_div.bed \u251c\u2500\u2500div_isotype_list.txt \u251c\u2500\u2500haplotype_in_QTL_region.txt \u251c\u2500\u2500BURDEN \u251c\u2500\u2500 VT \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname.VariableThresholdPrice.assoc \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_VTprice.pdf \u251c\u2500\u2500 SKAT \u251c\u2500\u2500 Data \u251c\u2500\u2500 traitname.Skat.assoc \u251c\u2500\u2500 Plots \u251c\u2500\u2500 traitname_SKAT.pdf","title":"Output Folder Structure"},{"location":"pipeline-cegwas/#phenotypes_folder","text":"strain_issues.txt - Output of any strain names that were changed to match vcf (i.e. isotypes that are not reference strains) pr_traitname.tsv - Processed phenotype file for each trait. This is the file that goes into the mapping","title":"Phenotypes folder"},{"location":"pipeline-cegwas/#genotype_matrix_folder","text":"Genotype_Matrix.tsv - pruned LD-pruned genotype matrix used for GWAS and construction of kinship matrix total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix","title":"Genotype_Matrix folder"},{"location":"pipeline-cegwas/#mappings_folder","text":"","title":"Mappings folder"},{"location":"pipeline-cegwas/#data","text":"traitname_processed_mapping.tsv - Processed mapping data frame for each trait mapped QTL_peaks.tsv - List of signifcant QTL identified across all traits","title":"Data"},{"location":"pipeline-cegwas/#plots","text":"traitname_manplot.pdf - Manhattan plot for each trait that was analyzed. Two significance threshold lines are present, one for the Bonferronit corrected threshold, and another for the spectral decomposition threshold. traitname_pxgplot.pdf - Phenotype by genotype split at peak QTL positions for every significant QTL identified Summarized_mappings.pdf - A summary plot of all QTL identified","title":"Plots"},{"location":"pipeline-cegwas/#fine_mappings_folder","text":"","title":"Fine_Mappings folder"},{"location":"pipeline-cegwas/#data_1","text":"traitname_snpeff_genes.tsv - Fine-mapping data frame for all significant QTL","title":"Data"},{"location":"pipeline-cegwas/#plots_1","text":"traitname_qtlinterval_finemap_plot.pdf - Fine map plot of QTL interval, colored by marker LD with the peak QTL identified from the genome-wide scan traitname_qtlinterval_gene_plot.pdf - variant annotation plot overlaid with gene CDS for QTL interval","title":"Plots"},{"location":"pipeline-cegwas/#burden_folder_contains_two_subfolders_vtskat_with_the_same_structure","text":"","title":"BURDEN folder (Contains two subfolders VT/SKAT with the same structure)"},{"location":"pipeline-cegwas/#data_2","text":"traitname.VariableThresholdPrice.assoc - Genome-wide burden mapping result using VT price, see RVtests homepage traitname.Skat.assoc - Genome-wide burden mapping result using Skat, see RVtests homepage","title":"Data"},{"location":"pipeline-cegwas/#plots_2","text":"traitname_VTprice.pdf - Genome-wide burden mapping manhattan plot for VTprice traitname_SKAT.pdf - Genome-wide burden mapping manhattan plot for Skat","title":"Plots"},{"location":"pipeline-cellprofiler/","text":"Andersen Lab Image Analysis Pipeline \u00b6 Implemented using CellProfiler \u00b6 CellProfiler Directory Structure (With Example Files) \u00b6 CellProfiler \u251c\u2500\u2500 batch_files \u251c\u2500\u2500 20191119_example_batch_20201018.h5 \u251c\u2500\u2500 metadata \u251c\u2500\u2500 20191119_example_metadata_20201018.csv \u251c\u2500\u2500 pipelines \u251c\u2500\u2500 20191119_example.cpproj \u251c\u2500\u2500 sample_pipelines \u251c\u2500\u2500 projects \u251c\u2500\u2500 20191119_example \u251c\u2500\u2500 raw_images \u251c\u2500\u2500 output_data \u251c\u2500\u2500 20191119_example_data_1603047856 \u251c\u2500\u2500 CellProfiler-Analysis_20191119_example_data_1603047856run1 \u251c\u2500\u2500 Logs \u251c\u2500\u2500 ProcessedImages \u251c\u2500\u2500 OverlappingWorms_Data \u251c\u2500\u2500 NonOverlappingWorms_Data \u251c\u2500\u2500 scripts \u251c\u2500\u2500 generate_metadata.R \u251c\u2500\u2500 run_cellprofiler.sh \u251c\u2500\u2500 cellprofiler_parallel.sh \u251c\u2500\u2500 check_run_cellprofiler.sh \u251c\u2500\u2500 aggregate_cellprofiler_results.R \u251c\u2500\u2500 well_masks \u251c\u2500\u2500 wellmask_98.png \u251c\u2500\u2500 worm_models \u251c\u2500\u2500 Adult_N2_HB101_100w.xml \u251c\u2500\u2500 L1_N2_HB101_100w.xml \u251c\u2500\u2500 L2L3_N2_HB101_100w.xml \u251c\u2500\u2500 L4_N2_HB101_100w.xml \u251c\u2500\u2500 WM_FBZ_control.xml \u251c\u2500\u2500 WM_FBZ_dose.xml \u251c\u2500\u2500 high_dose_worm_model.xml To recreate CellProfiler (CP) on QUEST: \u00b6 1) Navigate to the directory where you wish to clone CP: ssh -X user@quest.it.northwestern.edu cd [desired directory] 2) Clone CP repository git clone https://github.com/AndersenLab/CellProfiler.git 3) Download CP Docker images and convert to Singularity images within CP directory: cd CellProfiler module load singularity singularity pull docker://cellprofiler/cellprofiler:3.1.9 singularity pull docker://cellprofiler/cellprofiler:4.0.3 To execute the CP pipeline on QUEST: \u00b6 Full instructions here : \u00b6 1) Navigate to the CP Directory: cd CellProfiler 2) Generate metadata CP file: Rscript scripts/generate_metadata.R 20191119_example 3) Download metadata file and create CellProfiler pipeline locally . 4) Upload properly named batch file and CellProfiler pipeline to appropriate QUEST directories. (see file structure above) 5) Collect measurements using CellProfiler: bash scripts/run_cellprofiler.sh projects/20191119_example batch_files/20191119_example_batch_[date].h5 bash scripts/check_run_cellprofiler.sh projects/20191119_example batch_files/20191119_example_batch_[date].h5 [timestamp] 6) Aggregate measurement data: Rscript scripts/aggregate_cellprofiler_results.R 20191119_example_data_[timestamp] 20191119_example_metadata_[date].csv [output_info] At this point, a summarized .RData file will be available for download, containing measurement outputs corresponding to each worm model used in the pipeline: CellProfiler \u251c\u2500\u2500 projects \u251c\u2500\u2500 20191119_example \u251c\u2500\u2500 raw_images \u251c\u2500\u2500 output_data \u251c\u2500\u2500 20191119_example_summary_data \u251c\u2500\u2500 CellProfiler-Analysis_20191119_example_data_[output_info]_[timestamp].RData \u251c\u2500\u2500 Logs \u251c\u2500\u2500 ProcessedImages \u251c\u2500\u2500 OverlappingWorms_Data \u251c\u2500\u2500 NonOverlappingWorms_Data These data can be analyzed using the R/easyXpress package","title":"Image Analysis"},{"location":"pipeline-cellprofiler/#andersen_lab_image_analysis_pipeline","text":"","title":"Andersen Lab Image Analysis Pipeline"},{"location":"pipeline-cellprofiler/#implemented_using_cellprofiler","text":"","title":"Implemented using CellProfiler"},{"location":"pipeline-cellprofiler/#cellprofiler_directory_structure_with_example_files","text":"CellProfiler \u251c\u2500\u2500 batch_files \u251c\u2500\u2500 20191119_example_batch_20201018.h5 \u251c\u2500\u2500 metadata \u251c\u2500\u2500 20191119_example_metadata_20201018.csv \u251c\u2500\u2500 pipelines \u251c\u2500\u2500 20191119_example.cpproj \u251c\u2500\u2500 sample_pipelines \u251c\u2500\u2500 projects \u251c\u2500\u2500 20191119_example \u251c\u2500\u2500 raw_images \u251c\u2500\u2500 output_data \u251c\u2500\u2500 20191119_example_data_1603047856 \u251c\u2500\u2500 CellProfiler-Analysis_20191119_example_data_1603047856run1 \u251c\u2500\u2500 Logs \u251c\u2500\u2500 ProcessedImages \u251c\u2500\u2500 OverlappingWorms_Data \u251c\u2500\u2500 NonOverlappingWorms_Data \u251c\u2500\u2500 scripts \u251c\u2500\u2500 generate_metadata.R \u251c\u2500\u2500 run_cellprofiler.sh \u251c\u2500\u2500 cellprofiler_parallel.sh \u251c\u2500\u2500 check_run_cellprofiler.sh \u251c\u2500\u2500 aggregate_cellprofiler_results.R \u251c\u2500\u2500 well_masks \u251c\u2500\u2500 wellmask_98.png \u251c\u2500\u2500 worm_models \u251c\u2500\u2500 Adult_N2_HB101_100w.xml \u251c\u2500\u2500 L1_N2_HB101_100w.xml \u251c\u2500\u2500 L2L3_N2_HB101_100w.xml \u251c\u2500\u2500 L4_N2_HB101_100w.xml \u251c\u2500\u2500 WM_FBZ_control.xml \u251c\u2500\u2500 WM_FBZ_dose.xml \u251c\u2500\u2500 high_dose_worm_model.xml","title":"CellProfiler Directory Structure (With Example Files)"},{"location":"pipeline-cellprofiler/#to_recreate_cellprofiler_cp_on_quest","text":"1) Navigate to the directory where you wish to clone CP: ssh -X user@quest.it.northwestern.edu cd [desired directory] 2) Clone CP repository git clone https://github.com/AndersenLab/CellProfiler.git 3) Download CP Docker images and convert to Singularity images within CP directory: cd CellProfiler module load singularity singularity pull docker://cellprofiler/cellprofiler:3.1.9 singularity pull docker://cellprofiler/cellprofiler:4.0.3","title":"To recreate CellProfiler (CP) on QUEST:"},{"location":"pipeline-cellprofiler/#to_execute_the_cp_pipeline_on_quest","text":"","title":"To execute the CP pipeline on QUEST:"},{"location":"pipeline-cellprofiler/#full_instructions_here","text":"1) Navigate to the CP Directory: cd CellProfiler 2) Generate metadata CP file: Rscript scripts/generate_metadata.R 20191119_example 3) Download metadata file and create CellProfiler pipeline locally . 4) Upload properly named batch file and CellProfiler pipeline to appropriate QUEST directories. (see file structure above) 5) Collect measurements using CellProfiler: bash scripts/run_cellprofiler.sh projects/20191119_example batch_files/20191119_example_batch_[date].h5 bash scripts/check_run_cellprofiler.sh projects/20191119_example batch_files/20191119_example_batch_[date].h5 [timestamp] 6) Aggregate measurement data: Rscript scripts/aggregate_cellprofiler_results.R 20191119_example_data_[timestamp] 20191119_example_metadata_[date].csv [output_info] At this point, a summarized .RData file will be available for download, containing measurement outputs corresponding to each worm model used in the pipeline: CellProfiler \u251c\u2500\u2500 projects \u251c\u2500\u2500 20191119_example \u251c\u2500\u2500 raw_images \u251c\u2500\u2500 output_data \u251c\u2500\u2500 20191119_example_summary_data \u251c\u2500\u2500 CellProfiler-Analysis_20191119_example_data_[output_info]_[timestamp].RData \u251c\u2500\u2500 Logs \u251c\u2500\u2500 ProcessedImages \u251c\u2500\u2500 OverlappingWorms_Data \u251c\u2500\u2500 NonOverlappingWorms_Data These data can be analyzed using the R/easyXpress package","title":"Full instructions here:"},{"location":"pipeline-concordance/","text":"concordance-nf \u00b6 concordance-nf Pipeline overview Software Requirements Usage Profiles Debugging the pipeline on Quest Running the pipeline on Quest Parameters --bam_coverage --vcf --info_sheet --species (optional) --concordance_cutoff (optional) --cores (optional) --out (optional) Output The concordance-nf pipeline is used to detect sample swaps and determine which wild isolate strains should be grouped together as an isotype. To determine which strains belong to the same isotype we use two criteria. First we look at the strains that group together with a concordance threshold of 99.95%. Generally this will group most isotypes without issue. However, it is possible that you will run into cases where the grouping is not clean. For example, strain A groups with B, B groups with C, but C does not group with A. In these cases you must examine the data closely to identify why strains are incompletely grouping. Our second criteria we use to group isotypes may address these types of groupings. The second criteria that we use to group isotypes regards looking for regional differences among strains. If two strains are similar but possess a region of their genome (binned at 1 Mb) that differs by more than 2% then we will separate them out into their own isotypes. The process of grouping isotypes is very hand-on. This pipeline will help process the data but you must carefully review the output and investigate closely. Pipeline overview \u00b6 \u250c\u2500\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u2500\u2510\u250c\u252c\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510 \u250c\u2510\u250c\u250c\u2500\u2510 \u2502 \u2502 \u2502\u2502\u2502\u2502\u2502 \u2502 \u2502\u251c\u252c\u2518 \u2502\u2502\u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u251c\u2524\u2500\u2500\u2500\u2502\u2502\u2502\u251c\u2524 \u2514\u2500\u2518\u2514\u2500\u2518\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518\u2534\u2514\u2500\u2500\u2534\u2518\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518 \u2518\u2514\u2518\u2514 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Regular job cores 4 --out Directory to output results concordance-{date} --vcf Hard filtered vcf null --bam_coverage Table with \"strain\" and \"coverage\" as header null --info_sheet Strain sheet containing exisiting isotype assignment null --species 'c_elegans' will check for npr1. All other values will skip this null --concordance_cutoff Cutoff of concordance value to count two strains as same isotype 0.9995 Software Requirements \u00b6 The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update Usage \u00b6 Profiles \u00b6 The nextflow.config file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. Note If you forget to add a -profile , the quest profile will be chosen as default Debugging the pipeline on Quest \u00b6 When running on Quest, you should first run the quest debug profile. The Quest debug profile will use the test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run andersenlab/concordance-nf -profile quest_debug -resume Running the pipeline on Quest \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. The pipeline can be run on Quest using the following command: nextflow run andersenlab/concordance-nf -profile quest --bam_coverage <path_to_file> --vcf <path_to_file> --species c_elegans --info_sheet <path_to_file> -resume Parameters \u00b6 The nextflow profiles configured in nextflow.config are designed to make it so that you don't need to change the parameters. However, the pipeline offers this flexibility if it is ever called for. --bam_coverage \u00b6 The sample sheet to use. This is generally the same sample sheet used for wi-gatk . The sample sheet should look like this: Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced. --vcf \u00b6 The hard-filtered VCF output from wi-gatk . --info_sheet \u00b6 Strain sheet containing existing isotype assignment. You can download the current wild isolate species master sheet for this input. --species (optional) \u00b6 Common options include 'c_elegans', 'c_briggsae', and 'c_tropicalis'. If species == c_elegans, pipeline will check for npr-1 variant, otherwise this step will be skipped. Default = 'c_elegans' --concordance_cutoff (optional) \u00b6 Cutoff to use to determine isotype groups. Default is 0.9995. --cores (optional) \u00b6 The number of cores to use during alignments and variant calling. --out (optional) \u00b6 A directory in which to output results. By default it will be concordance-YYYYMMDD where YYYYMMDD is todays date. Output \u00b6 \u251c\u2500\u2500 concordance \u251c\u2500\u2500 gtcheck.tsv \u251c\u2500\u2500 isotype_count.txt \u251c\u2500\u2500 isotype_groups.tsv \u251c\u2500\u2500 npr1_allele_strain.tsv \u251c\u2500\u2500 problem_strains.tsv \u251c\u2500\u2500 WI_metadata.tsv \u251c\u2500\u2500 concordance.pdf/png \u251c\u2500\u2500 xconcordance.pdf/png \u2514\u2500\u2500 pairwise \u2514\u2500\u2500 within_group \u2514\u2500\u2500 {isotype_group}.{isotype}.{strain1}_{strain2}.png concordance.png/pdf - An image showing the distribution of pairwise concordances across all strains. The cutoff is at 99.9% above which pairs are considered to be in the same isotype unless issues arise. xconcordance.png/pdf - A close up view of the concordances showing more detail. isotype_groups.tsv - This is the one of the most important output files . It illustrates the isotypes identified for each strain and identifies potential issues. A file with the following structure: group - A number used to group strains (in each row) into an isotype automatically. This number should be unique with the isotype column (e.g. 1--> AB1, 112 --> CB4858, BRC20067 --> 175). The number can change between analyses. strain - the strain isotype - the currently assigned isotype for a strain taken from the WI Strain Info spreadsheet. When new strains are added this is blank. latitude longitude coverage - Depth of coverage for strain. unique_isotypes_per_group - Number of unique isotypes when grouping by the group column. This should be 1. If it is more than 1, it indicates that multiple isotypes were assigned to a grouping and that a previously assigned isotype is now being called into question. unique_groups_per_isotype Number of unique groups assigned to an isotype. This should be 1. If it is higher than 1, it indicates that a strain is concordant with strains in two different isotypes (including blanks). If it is equal to 2 and contains blanks in the isotype column it likely means that an isotype should be assigned to that strain. strain_in_multiple_isotypes - Indicates that a strain is falling into multiple isotypes (a problem!). location_issue - Indicates a location issue. This occurs when strains fall into an isotype but are located far away from one another. Some are known issues and can be ignored. strain_conflict - TRUE if any issue is present that should be investigated. Note This file might change as you manually adjust the concordance cutoff for each run gtcheck.tsv - the other most important file . File produced using bcftools gtcheck ; Raw genotype differences between strains. This file is used in manual inspection of the isotype groups isotype_count.txt - Gives a count of the number of isotypes identified. concordance/pairwise/ (directory) Contains images showing locations where regional discordance occurs among strains classified as being the isotype. You must look through all these images to ensure there are no strains being grouped that have regions with significant differences (> 2%). The image below illustrates an example of this. ED3049 and ED3046 are highly similar (> 99.9%). However, they differ in a region on the right arm of chromosome II. We believe this was enough reason to consider them separate isotypes. npr1_allele_strain.tsv - if species == c_elegans, this file will be output to show problematic strains that contain the N2 npr-1 allele and should be manually checked. Important If a new strain is flagged in this file, tell Erik, Robyn, and the wild isolate team ASAP so they can address the issue. This strain will likely be removed from further analysis.","title":"concordance-nf"},{"location":"pipeline-concordance/#concordance-nf","text":"concordance-nf Pipeline overview Software Requirements Usage Profiles Debugging the pipeline on Quest Running the pipeline on Quest Parameters --bam_coverage --vcf --info_sheet --species (optional) --concordance_cutoff (optional) --cores (optional) --out (optional) Output The concordance-nf pipeline is used to detect sample swaps and determine which wild isolate strains should be grouped together as an isotype. To determine which strains belong to the same isotype we use two criteria. First we look at the strains that group together with a concordance threshold of 99.95%. Generally this will group most isotypes without issue. However, it is possible that you will run into cases where the grouping is not clean. For example, strain A groups with B, B groups with C, but C does not group with A. In these cases you must examine the data closely to identify why strains are incompletely grouping. Our second criteria we use to group isotypes may address these types of groupings. The second criteria that we use to group isotypes regards looking for regional differences among strains. If two strains are similar but possess a region of their genome (binned at 1 Mb) that differs by more than 2% then we will separate them out into their own isotypes. The process of grouping isotypes is very hand-on. This pipeline will help process the data but you must carefully review the output and investigate closely.","title":"concordance-nf"},{"location":"pipeline-concordance/#pipeline_overview","text":"\u250c\u2500\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510\u252c\u2500\u2510\u250c\u252c\u2510\u250c\u2500\u2510\u250c\u2510\u250c\u250c\u2500\u2510\u250c\u2500\u2510 \u250c\u2510\u250c\u250c\u2500\u2510 \u2502 \u2502 \u2502\u2502\u2502\u2502\u2502 \u2502 \u2502\u251c\u252c\u2518 \u2502\u2502\u251c\u2500\u2524\u2502\u2502\u2502\u2502 \u251c\u2524\u2500\u2500\u2500\u2502\u2502\u2502\u251c\u2524 \u2514\u2500\u2518\u2514\u2500\u2518\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518\u2534\u2514\u2500\u2500\u2534\u2518\u2534 \u2534\u2518\u2514\u2518\u2514\u2500\u2518\u2514\u2500\u2518 \u2518\u2514\u2518\u2514 parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Regular job cores 4 --out Directory to output results concordance-{date} --vcf Hard filtered vcf null --bam_coverage Table with \"strain\" and \"coverage\" as header null --info_sheet Strain sheet containing exisiting isotype assignment null --species 'c_elegans' will check for npr1. All other values will skip this null --concordance_cutoff Cutoff of concordance value to count two strains as same isotype 0.9995","title":"Pipeline overview"},{"location":"pipeline-concordance/#software_requirements","text":"The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update","title":"Software Requirements"},{"location":"pipeline-concordance/#usage","text":"","title":"Usage"},{"location":"pipeline-concordance/#profiles","text":"The nextflow.config file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. Note If you forget to add a -profile , the quest profile will be chosen as default","title":"Profiles"},{"location":"pipeline-concordance/#debugging_the_pipeline_on_quest","text":"When running on Quest, you should first run the quest debug profile. The Quest debug profile will use the test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run andersenlab/concordance-nf -profile quest_debug -resume","title":"Debugging the pipeline on Quest"},{"location":"pipeline-concordance/#running_the_pipeline_on_quest","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. The pipeline can be run on Quest using the following command: nextflow run andersenlab/concordance-nf -profile quest --bam_coverage <path_to_file> --vcf <path_to_file> --species c_elegans --info_sheet <path_to_file> -resume","title":"Running the pipeline on Quest"},{"location":"pipeline-concordance/#parameters","text":"The nextflow profiles configured in nextflow.config are designed to make it so that you don't need to change the parameters. However, the pipeline offers this flexibility if it is ever called for.","title":"Parameters"},{"location":"pipeline-concordance/#--bam_coverage","text":"The sample sheet to use. This is generally the same sample sheet used for wi-gatk . The sample sheet should look like this: Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced.","title":"--bam_coverage"},{"location":"pipeline-concordance/#--vcf","text":"The hard-filtered VCF output from wi-gatk .","title":"--vcf"},{"location":"pipeline-concordance/#--info_sheet","text":"Strain sheet containing existing isotype assignment. You can download the current wild isolate species master sheet for this input.","title":"--info_sheet"},{"location":"pipeline-concordance/#--species_optional","text":"Common options include 'c_elegans', 'c_briggsae', and 'c_tropicalis'. If species == c_elegans, pipeline will check for npr-1 variant, otherwise this step will be skipped. Default = 'c_elegans'","title":"--species (optional)"},{"location":"pipeline-concordance/#--concordance_cutoff_optional","text":"Cutoff to use to determine isotype groups. Default is 0.9995.","title":"--concordance_cutoff (optional)"},{"location":"pipeline-concordance/#--cores_optional","text":"The number of cores to use during alignments and variant calling.","title":"--cores (optional)"},{"location":"pipeline-concordance/#--out_optional","text":"A directory in which to output results. By default it will be concordance-YYYYMMDD where YYYYMMDD is todays date.","title":"--out (optional)"},{"location":"pipeline-concordance/#output","text":"\u251c\u2500\u2500 concordance \u251c\u2500\u2500 gtcheck.tsv \u251c\u2500\u2500 isotype_count.txt \u251c\u2500\u2500 isotype_groups.tsv \u251c\u2500\u2500 npr1_allele_strain.tsv \u251c\u2500\u2500 problem_strains.tsv \u251c\u2500\u2500 WI_metadata.tsv \u251c\u2500\u2500 concordance.pdf/png \u251c\u2500\u2500 xconcordance.pdf/png \u2514\u2500\u2500 pairwise \u2514\u2500\u2500 within_group \u2514\u2500\u2500 {isotype_group}.{isotype}.{strain1}_{strain2}.png concordance.png/pdf - An image showing the distribution of pairwise concordances across all strains. The cutoff is at 99.9% above which pairs are considered to be in the same isotype unless issues arise. xconcordance.png/pdf - A close up view of the concordances showing more detail. isotype_groups.tsv - This is the one of the most important output files . It illustrates the isotypes identified for each strain and identifies potential issues. A file with the following structure: group - A number used to group strains (in each row) into an isotype automatically. This number should be unique with the isotype column (e.g. 1--> AB1, 112 --> CB4858, BRC20067 --> 175). The number can change between analyses. strain - the strain isotype - the currently assigned isotype for a strain taken from the WI Strain Info spreadsheet. When new strains are added this is blank. latitude longitude coverage - Depth of coverage for strain. unique_isotypes_per_group - Number of unique isotypes when grouping by the group column. This should be 1. If it is more than 1, it indicates that multiple isotypes were assigned to a grouping and that a previously assigned isotype is now being called into question. unique_groups_per_isotype Number of unique groups assigned to an isotype. This should be 1. If it is higher than 1, it indicates that a strain is concordant with strains in two different isotypes (including blanks). If it is equal to 2 and contains blanks in the isotype column it likely means that an isotype should be assigned to that strain. strain_in_multiple_isotypes - Indicates that a strain is falling into multiple isotypes (a problem!). location_issue - Indicates a location issue. This occurs when strains fall into an isotype but are located far away from one another. Some are known issues and can be ignored. strain_conflict - TRUE if any issue is present that should be investigated. Note This file might change as you manually adjust the concordance cutoff for each run gtcheck.tsv - the other most important file . File produced using bcftools gtcheck ; Raw genotype differences between strains. This file is used in manual inspection of the isotype groups isotype_count.txt - Gives a count of the number of isotypes identified. concordance/pairwise/ (directory) Contains images showing locations where regional discordance occurs among strains classified as being the isotype. You must look through all these images to ensure there are no strains being grouped that have regions with significant differences (> 2%). The image below illustrates an example of this. ED3049 and ED3046 are highly similar (> 99.9%). However, they differ in a region on the right arm of chromosome II. We believe this was enough reason to consider them separate isotypes. npr1_allele_strain.tsv - if species == c_elegans, this file will be output to show problematic strains that contain the N2 npr-1 allele and should be manually checked. Important If a new strain is flagged in this file, tell Erik, Robyn, and the wild isolate team ASAP so they can address the issue. This strain will likely be removed from further analysis.","title":"Output"},{"location":"pipeline-docker/","text":"Create docker image \u00b6 Create docker image Dockerfile Build docker image Tag image with a version Push docker image to dockerhub Running Nextflow with docker Docker can help us to maintain our computational environments. Each of our Nextflow pipeline has a dedicated docker image in our lab. And all the docker files should be avalible at dockerfile . Dockerfile \u00b6 To simplify the image building, we can use conda to install most of the tools. We can collect the tools available on conda cloud into a conda.yml file, which might looks like this. name: concordance-nf channels: - defaults - bioconda - r - biobuilds - conda-forge dependencies: - bwa=0.7.17 - sambamba=0.7.0 - samtools=1.9 - picard=2.20.6 - bcftools=1.9 # - vcfkit=0.1.6 - csvtk=0.18.2 - r=3.6.0 - r-ggplot2=3.1.1 - r-readr=1.3.1 - r-tidyverse=1.2.1 Then, build the Dockerfile as below. FROM continuumio/miniconda MAINTAINER XXX <email> COPY conda.yml . RUN \\ conda env update -n root -f conda.yml \\ && conda clean -a # install other tools not avalible on conda cloud -- tidyverse sometimes need to be installed here separately... RUN apt-get update && apt-get install -y procps RUN R -e \"install.packages('roperators',dependencies=TRUE, repos='http://cran.us.r-project.org')\" Note Put the conda.ymal and Dockerfile under the same folder. Build docker image \u00b6 To build the docker image, you need docker desktop installed on your local machine. Also you should sign up the dockerhub to enable pushing docker image to cloud. Go to the folder which have conda.ymal and Dockerfile , run docker build -t <dockerhub account>/<name of the image> . # don't ingore the dot here You can use docker image ls to check the image list you have in your local machine. Importantly, you have to check if the tools were installed sucessfully in your docker image. To test the docker image, run docker run -ti <dockerhub account>/<name of the image> sh The above command will let you into the docker image, where you can check the tools by their normal commands. Make sure all the tools you need have been installed appropriately. Tag image with a version \u00b6 There are sometimes issues with Nextflow thinking it has the latest docker image when it really doesn't. To avoid this issue, it is useful to tag each updated docker image with a version tag. Remember to update the docker call in nextflow to use the new version! docker image tag <dockerhub account>/<name of the image>:latest <dockerhub account>/<name of the image>:<version tag> Push docker image to dockerhub \u00b6 After the image check, you are ready to push the image to dockerhub which allows you to download the image when ever you need to use. docker push <dockerhub account>/<name of the image>:<version tag> Running Nextflow with docker \u00b6 If running Nextflow locally, a docker container can be used with the following line (check out the documentation ): nextflow run <your script> -with-docker [docker image] Alternatively, a docker container can be specified within the nextflow.config script to avoid adding an extra parameter: process.container = 'nextflow/examples:latest' docker.enabled = true // if on quest: // singularity.enabled = true Important When running Nextflow with a docker container on QUEST, it is necessary to replace the docker command with singularity (although you still must build a docker container). You must also load singularity using module load singularity before starting a run.","title":"Docker"},{"location":"pipeline-docker/#create_docker_image","text":"Create docker image Dockerfile Build docker image Tag image with a version Push docker image to dockerhub Running Nextflow with docker Docker can help us to maintain our computational environments. Each of our Nextflow pipeline has a dedicated docker image in our lab. And all the docker files should be avalible at dockerfile .","title":"Create docker image"},{"location":"pipeline-docker/#dockerfile","text":"To simplify the image building, we can use conda to install most of the tools. We can collect the tools available on conda cloud into a conda.yml file, which might looks like this. name: concordance-nf channels: - defaults - bioconda - r - biobuilds - conda-forge dependencies: - bwa=0.7.17 - sambamba=0.7.0 - samtools=1.9 - picard=2.20.6 - bcftools=1.9 # - vcfkit=0.1.6 - csvtk=0.18.2 - r=3.6.0 - r-ggplot2=3.1.1 - r-readr=1.3.1 - r-tidyverse=1.2.1 Then, build the Dockerfile as below. FROM continuumio/miniconda MAINTAINER XXX <email> COPY conda.yml . RUN \\ conda env update -n root -f conda.yml \\ && conda clean -a # install other tools not avalible on conda cloud -- tidyverse sometimes need to be installed here separately... RUN apt-get update && apt-get install -y procps RUN R -e \"install.packages('roperators',dependencies=TRUE, repos='http://cran.us.r-project.org')\" Note Put the conda.ymal and Dockerfile under the same folder.","title":"Dockerfile"},{"location":"pipeline-docker/#build_docker_image","text":"To build the docker image, you need docker desktop installed on your local machine. Also you should sign up the dockerhub to enable pushing docker image to cloud. Go to the folder which have conda.ymal and Dockerfile , run docker build -t <dockerhub account>/<name of the image> . # don't ingore the dot here You can use docker image ls to check the image list you have in your local machine. Importantly, you have to check if the tools were installed sucessfully in your docker image. To test the docker image, run docker run -ti <dockerhub account>/<name of the image> sh The above command will let you into the docker image, where you can check the tools by their normal commands. Make sure all the tools you need have been installed appropriately.","title":"Build docker image"},{"location":"pipeline-docker/#tag_image_with_a_version","text":"There are sometimes issues with Nextflow thinking it has the latest docker image when it really doesn't. To avoid this issue, it is useful to tag each updated docker image with a version tag. Remember to update the docker call in nextflow to use the new version! docker image tag <dockerhub account>/<name of the image>:latest <dockerhub account>/<name of the image>:<version tag>","title":"Tag image with a version"},{"location":"pipeline-docker/#push_docker_image_to_dockerhub","text":"After the image check, you are ready to push the image to dockerhub which allows you to download the image when ever you need to use. docker push <dockerhub account>/<name of the image>:<version tag>","title":"Push docker image to dockerhub"},{"location":"pipeline-docker/#running_nextflow_with_docker","text":"If running Nextflow locally, a docker container can be used with the following line (check out the documentation ): nextflow run <your script> -with-docker [docker image] Alternatively, a docker container can be specified within the nextflow.config script to avoid adding an extra parameter: process.container = 'nextflow/examples:latest' docker.enabled = true // if on quest: // singularity.enabled = true Important When running Nextflow with a docker container on QUEST, it is necessary to replace the docker command with singularity (although you still must build a docker container). You must also load singularity using module load singularity before starting a run.","title":"Running Nextflow with docker"},{"location":"pipeline-genomes-nf/","text":"genomes-nf \u00b6 genomes-nf Pipeline overview Software requirements Usage Parameters -profile (optional) Default usage: downloading genome files from Wormbase --wb_version (optional) --projects (optional) --output (optional) Alternative usage: using manually selected genomes --genome --gff Output Notes This repo contains a nextflow pipeline that downloads, indexes, and builds annotation databases for reference genomes from wormbase. The following outputs are created: A BWA Index SNPeff annotation database CSQ annotation database Samtools faidx index A GATK Sequence dictionary file Important When adding a new WormBase version reference genome, especially for c_elegans it is essential that you use this pipeline instead of downloading and adding the files to QUEST manually. These files and this file structure are essential to many other pipelines in the lab. Pipeline overview \u00b6 >AAGACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAA AGACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC GAAATA CGCG GA ATATA AAAA TCG TCGAT GC GGGC CGATCGA TAGAT GA TATATCGC TTAAC ACTAGAGGGG CTATCGAC CGAA CT GACTA CT GCG AT AGCATCACG TGGGGGCCTATC CGAC AA TCAGCTACGAAAT AGCCC TCTATCATAA TATAT T TCT TC AGCGA GA A A T TC ATAAAT TCGGGATCTAGC A CGC AT ATATATATGC GCGAT TCTAC AG GCGGGGGA AT TA AA AAGAC CG TC AT GC AGCTGGGGGC ACG GA TA AT GA CTATATATATCGC AATGC ACTAGAG GG CTATCGAC ACG A CT GACTA CT AGCGG AT AGCATCACGATGGG GCCTATC ACG C AA TCAGCTACGAAAT ACTCC TCTATCA AA AAATATAT TCTC TC AGCGA GA AAACT TC TTCATAAATCTCGG ATCTAGC ATCG AT TATATATATATGC TTAATA FCG GA ATATA AAA TCG TCGAT GC GG ACGATCGA TAGAT GA CTATATATATCGC AACACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAAT CTACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC parameters description Set/Default ========== =========== ======================== --wb_version wormbase version to build WS276 --projects comma-delimited list of `species/project_id` c_elegans/PRJNA13758,c_briggsae/PRJNA10731,c_tropicalis/PRJNA53597 --output Path of output folder /projects/b1059/data/ Software requirements \u00b6 Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env If running pipeline on Quest, you must first load singularity to access the docker container: module load singularity If running locally, Docker must be installed. For further instructions, check out our docker guide Usage \u00b6 The pipeline can be run locally or on Quest. For example: nextflow run main.nf -resume -profile local --wb_version=WS276 --projects=c_elegans/PRJNA13758 Parameters \u00b6 -profile (optional) \u00b6 Can be set to local or quest . The pipeline uses the andersenlab/genomes docker image built from env/genome.Dockerfile . The image is automatically built using github actions. See .github/workflows/build.yml for details. Note The default profile is set to -profile=quest Default usage: downloading genome files from Wormbase \u00b6 This is how the pipeline is mostly run, especially for C. elegans . --wb_version (optional) \u00b6 The wormbase version to build. For example, WS279 . Default is WS276 . --projects (optional) \u00b6 A comma-delimited list of species/project_id identifiers. A table below lists the current projects that can be downloaded. This table is regenerated as the first step of the pipeline, and stored as a file called project_species.tsv in the params.output folder ( ./genomes if working locally). By default, the pipeline will generate reference genome indices and annotations for: c_elegans/PRJNA13758 - N2 based reference genome c_briggsae/PRJNA10731 c_tropicalis/PRJNA53597 The current set of available species/projects that can be built are: species project b_xylophilus PRJEA64437 c_briggsae PRJNA10731 c_angaria PRJNA51225 a_ceylanicum PRJNA231479 a_suum PRJNA62057 a_suum PRJNA80881 b_malayi PRJNA10729 c_brenneri PRJNA20035 c_elegans PRJEB28388 c_elegans PRJNA13758 c_elegans PRJNA275000 c_latens PRJNA248912 c_remanei PRJNA248909 c_remanei PRJNA248911 c_remanei PRJNA53967 c_inopinata PRJDB5687 c_japonica PRJNA12591 c_sp11 PRJNA53597 c_sp5 PRJNA194557 c_nigoni PRJNA384657 c_sinica PRJNA194557 c_tropicalis PRJNA53597 d_immitis PRJEB1797 h_bacteriophora PRJNA13977 l_loa PRJNA60051 m_hapla PRJNA29083 m_incognita PRJEA28837 h_contortus PRJEB506 h_contortus PRJNA205202 n_americanus PRJNA72135 p_exspectatus PRJEB6009 o_tipulae PRJEB15512 p_redivivus PRJNA186477 s_ratti PRJEA62033 s_ratti PRJEB125 o_volvulus PRJEB513 p_pacificus PRJNA12644 t_muris PRJEB126 t_spiralis PRJNA12603 t_suis PRJNA208415 t_suis PRJNA208416 --output (optional) \u00b6 Path of output folder with results. Default is /projects/b1059/data/{species}/genomes/{projectID}/{WSbuild}/ Alternative usage: using manually selected genomes \u00b6 This step is mostly for making the snpEff database and making sure that the gff is in the proper format for BCSQ annotation when you have a manually curated genome/gff file. This is common for C. briggsae and C. tropicalis and might start to be used if we want to annotate C. elegans wild isolates like CB4856. --genome \u00b6 Path to manually curated genome (for genomes not downloaded from wormbase) --gff \u00b6 Path to manually curated gff generated using the above genome (for genomes not downloaded from wormbase) Output \u00b6 Outputs are nested under params.output with the following structure: c_elegans (species) \u2514\u2500\u2500 genomes \u2514\u2500\u2500 PRJNA13758 (project) \u2514\u2500\u2500 WS276 (build) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.dict (dict file) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz (fasta) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.amb (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.ann (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.bwt (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.fai (samtools faidx index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.gzi (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.pac (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.sa (bwa index) \u251c\u2500\u2500 csq \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz (CSQ annotation GFF3) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Length.tsv (protein lengths) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Scores.tsv (blosum and grantham scores) \u251c\u2500\u2500 lcr \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz (low complexity regions) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz (low complexity regions) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz.tbi (tabix index) \u2514\u2500\u2500 snpeff \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276 (tabix index) \u2502 \u251c\u2500\u2500 genes.gtf.gz (Reference GTF) \u2502 \u251c\u2500\u2500 sequences.fa (fasta genome (unzipped)) \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin (snpEff annotation db) \u2514\u2500\u2500 snpEff.config (snpEff configuration file) Notes \u00b6 The SNPeff databases are not collected together in one location as is often the case. Instead, they are stored individually with their own configuration files. The GFF3 files for some species are not as developed as C. elegans . As a consequence, the biotype is inferred from the Attributes column of the GFF. See bin/format_csq.R for more details. Warning The updated csq-formated gff script needs to be updated for other species besides C. elegans (if running the default mode)","title":"genomes-nf"},{"location":"pipeline-genomes-nf/#genomes-nf","text":"genomes-nf Pipeline overview Software requirements Usage Parameters -profile (optional) Default usage: downloading genome files from Wormbase --wb_version (optional) --projects (optional) --output (optional) Alternative usage: using manually selected genomes --genome --gff Output Notes This repo contains a nextflow pipeline that downloads, indexes, and builds annotation databases for reference genomes from wormbase. The following outputs are created: A BWA Index SNPeff annotation database CSQ annotation database Samtools faidx index A GATK Sequence dictionary file Important When adding a new WormBase version reference genome, especially for c_elegans it is essential that you use this pipeline instead of downloading and adding the files to QUEST manually. These files and this file structure are essential to many other pipelines in the lab.","title":"genomes-nf"},{"location":"pipeline-genomes-nf/#pipeline_overview","text":">AAGACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAA AGACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC GAAATA CGCG GA ATATA AAAA TCG TCGAT GC GGGC CGATCGA TAGAT GA TATATCGC TTAAC ACTAGAGGGG CTATCGAC CGAA CT GACTA CT GCG AT AGCATCACG TGGGGGCCTATC CGAC AA TCAGCTACGAAAT AGCCC TCTATCATAA TATAT T TCT TC AGCGA GA A A T TC ATAAAT TCGGGATCTAGC A CGC AT ATATATATGC GCGAT TCTAC AG GCGGGGGA AT TA AA AAGAC CG TC AT GC AGCTGGGGGC ACG GA TA AT GA CTATATATATCGC AATGC ACTAGAG GG CTATCGAC ACG A CT GACTA CT AGCGG AT AGCATCACGATGGG GCCTATC ACG C AA TCAGCTACGAAAT ACTCC TCTATCA AA AAATATAT TCTC TC AGCGA GA AAACT TC TTCATAAATCTCGG ATCTAGC ATCG AT TATATATATATGC TTAATA FCG GA ATATA AAA TCG TCGAT GC GG ACGATCGA TAGAT GA CTATATATATCGC AACACGACTAGAGGGGGCTATCGACTACGAAACTCGACTAGCTCAGCGGGATCAGCATCACGATGGGGGCCTATCTACGACAAAATCAGCTACGAAAT CTACCATCTATCATAAAAAATATATATCTCTTTCTAGCGACGATAAACTCTCTTTCATAAATCTCGGGATCTAGCTATCGCTATATATATATATATGC parameters description Set/Default ========== =========== ======================== --wb_version wormbase version to build WS276 --projects comma-delimited list of `species/project_id` c_elegans/PRJNA13758,c_briggsae/PRJNA10731,c_tropicalis/PRJNA53597 --output Path of output folder /projects/b1059/data/","title":"Pipeline overview"},{"location":"pipeline-genomes-nf/#software_requirements","text":"Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env If running pipeline on Quest, you must first load singularity to access the docker container: module load singularity If running locally, Docker must be installed. For further instructions, check out our docker guide","title":"Software requirements"},{"location":"pipeline-genomes-nf/#usage","text":"The pipeline can be run locally or on Quest. For example: nextflow run main.nf -resume -profile local --wb_version=WS276 --projects=c_elegans/PRJNA13758","title":"Usage"},{"location":"pipeline-genomes-nf/#parameters","text":"","title":"Parameters"},{"location":"pipeline-genomes-nf/#-profile_optional","text":"Can be set to local or quest . The pipeline uses the andersenlab/genomes docker image built from env/genome.Dockerfile . The image is automatically built using github actions. See .github/workflows/build.yml for details. Note The default profile is set to -profile=quest","title":"-profile (optional)"},{"location":"pipeline-genomes-nf/#default_usage_downloading_genome_files_from_wormbase","text":"This is how the pipeline is mostly run, especially for C. elegans .","title":"Default usage: downloading genome files from Wormbase"},{"location":"pipeline-genomes-nf/#--wb_version_optional","text":"The wormbase version to build. For example, WS279 . Default is WS276 .","title":"--wb_version (optional)"},{"location":"pipeline-genomes-nf/#--projects_optional","text":"A comma-delimited list of species/project_id identifiers. A table below lists the current projects that can be downloaded. This table is regenerated as the first step of the pipeline, and stored as a file called project_species.tsv in the params.output folder ( ./genomes if working locally). By default, the pipeline will generate reference genome indices and annotations for: c_elegans/PRJNA13758 - N2 based reference genome c_briggsae/PRJNA10731 c_tropicalis/PRJNA53597 The current set of available species/projects that can be built are: species project b_xylophilus PRJEA64437 c_briggsae PRJNA10731 c_angaria PRJNA51225 a_ceylanicum PRJNA231479 a_suum PRJNA62057 a_suum PRJNA80881 b_malayi PRJNA10729 c_brenneri PRJNA20035 c_elegans PRJEB28388 c_elegans PRJNA13758 c_elegans PRJNA275000 c_latens PRJNA248912 c_remanei PRJNA248909 c_remanei PRJNA248911 c_remanei PRJNA53967 c_inopinata PRJDB5687 c_japonica PRJNA12591 c_sp11 PRJNA53597 c_sp5 PRJNA194557 c_nigoni PRJNA384657 c_sinica PRJNA194557 c_tropicalis PRJNA53597 d_immitis PRJEB1797 h_bacteriophora PRJNA13977 l_loa PRJNA60051 m_hapla PRJNA29083 m_incognita PRJEA28837 h_contortus PRJEB506 h_contortus PRJNA205202 n_americanus PRJNA72135 p_exspectatus PRJEB6009 o_tipulae PRJEB15512 p_redivivus PRJNA186477 s_ratti PRJEA62033 s_ratti PRJEB125 o_volvulus PRJEB513 p_pacificus PRJNA12644 t_muris PRJEB126 t_spiralis PRJNA12603 t_suis PRJNA208415 t_suis PRJNA208416","title":"--projects (optional)"},{"location":"pipeline-genomes-nf/#--output_optional","text":"Path of output folder with results. Default is /projects/b1059/data/{species}/genomes/{projectID}/{WSbuild}/","title":"--output (optional)"},{"location":"pipeline-genomes-nf/#alternative_usage_using_manually_selected_genomes","text":"This step is mostly for making the snpEff database and making sure that the gff is in the proper format for BCSQ annotation when you have a manually curated genome/gff file. This is common for C. briggsae and C. tropicalis and might start to be used if we want to annotate C. elegans wild isolates like CB4856.","title":"Alternative usage: using manually selected genomes"},{"location":"pipeline-genomes-nf/#--genome","text":"Path to manually curated genome (for genomes not downloaded from wormbase)","title":"--genome"},{"location":"pipeline-genomes-nf/#--gff","text":"Path to manually curated gff generated using the above genome (for genomes not downloaded from wormbase)","title":"--gff"},{"location":"pipeline-genomes-nf/#output","text":"Outputs are nested under params.output with the following structure: c_elegans (species) \u2514\u2500\u2500 genomes \u2514\u2500\u2500 PRJNA13758 (project) \u2514\u2500\u2500 WS276 (build) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.dict (dict file) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz (fasta) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.amb (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.ann (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.bwt (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.fai (samtools faidx index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.gzi (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.pac (bwa index) \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.genome.fa.gz.sa (bwa index) \u251c\u2500\u2500 csq \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz (CSQ annotation GFF3) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.csq.gff3.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Length.tsv (protein lengths) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.AA_Scores.tsv (blosum and grantham scores) \u251c\u2500\u2500 lcr \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz (low complexity regions) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.repeat_masker.bed.gz.tbi (tabix index) \u2502 \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz (low complexity regions) \u2502 \u2514\u2500\u2500 c_elegans.PRJNA13758.WS276.dust.bed.gz.tbi (tabix index) \u2514\u2500\u2500 snpeff \u251c\u2500\u2500 c_elegans.PRJNA13758.WS276 (tabix index) \u2502 \u251c\u2500\u2500 genes.gtf.gz (Reference GTF) \u2502 \u251c\u2500\u2500 sequences.fa (fasta genome (unzipped)) \u2502 \u2514\u2500\u2500 snpEffectPredictor.bin (snpEff annotation db) \u2514\u2500\u2500 snpEff.config (snpEff configuration file)","title":"Output"},{"location":"pipeline-genomes-nf/#notes","text":"The SNPeff databases are not collected together in one location as is often the case. Instead, they are stored individually with their own configuration files. The GFF3 files for some species are not as developed as C. elegans . As a consequence, the biotype is inferred from the Attributes column of the GFF. See bin/format_csq.R for more details. Warning The updated csq-formated gff script needs to be updated for other species besides C. elegans (if running the default mode)","title":"Notes"},{"location":"pipeline-impute/","text":"VCF Imputation \u00b6 VCF Imputation Script overview Software requirements Usage Parameters Although there is not a full \"pipeline\" for imputation, there is a script to run and this step is crucial, as fine-mapping for NemaScan requires an imputed VCF. Script overview \u00b6 #!/bin/bash #SBATCH -J beagle #SBATCH -A b1042 #SBATCH -p genomicsguestA #SBATCH -t 6:00:00 #SBATCH -n 24 # can maybe be lower now #SBATCH --mem=50G # can change if needed ################# # edit these variables: vcf='/projects/b1059/data/c_briggsae/WI/variation/20210803/vcf/WI.20210803.hard-filter.isotype.vcf.gz' species='c_briggsae' date='20210803 ################# # load modules: module load bcftools # do the imputation with beagle5.2: for i in I II III IV V X do java -jar /projects/b1059/software/beagle/beagle5.2/beagle.28Jun21.220.jar chrom=${i} \\ window=5 \\ overlap=2 \\ impute=true \\ ne=100000 \\ nthreads=3 \\ imp-segment=0.5 \\ imp-step=0.01 \\ cluster=0.0005 \\ gt=$vcf \\ map=/projects/b1059/data/$species/genomes/genetic_map/chr${i}.map \\ out=${i}.b5 bcftools index ${i}.b5.vcf.gz done # concat all the chrom vcfs bcftools concat *.b5.vcf.gz -O z -o WI.${date}.impute.isotype.vcf.gz bcftools index -t WI.${date}.impute.isotype.vcf.gz bcftools stats --verbose WI.${date}.impute.isotype.vcf.gz > WI.${date}.impute.isotype.stats.txt Software requirements \u00b6 You can run this script on QUEST using the latest version of Beagle (5.2) in the /projects/b1059/software/ folder. Also requires bcftools (loaded in script). Usage \u00b6 Copy the script above and create a new file on QUEST (i.e. impute.sh ). Manually edit the vcf input, the species , and the date (to be the release date for all VCFs). Submit the job with sbatch impute.sh Parameters \u00b6 These parameters have been checked and decided on by previous lab members and Erik. Some chromosomes might require a window size of 3 and an overlap of 1. In recent conversation with the person who manages Beagle, they mentioned we should probably use default values unless we have done simulations to show these values are better. Note for the future maybe.","title":"imputation"},{"location":"pipeline-impute/#vcf_imputation","text":"VCF Imputation Script overview Software requirements Usage Parameters Although there is not a full \"pipeline\" for imputation, there is a script to run and this step is crucial, as fine-mapping for NemaScan requires an imputed VCF.","title":"VCF Imputation"},{"location":"pipeline-impute/#script_overview","text":"#!/bin/bash #SBATCH -J beagle #SBATCH -A b1042 #SBATCH -p genomicsguestA #SBATCH -t 6:00:00 #SBATCH -n 24 # can maybe be lower now #SBATCH --mem=50G # can change if needed ################# # edit these variables: vcf='/projects/b1059/data/c_briggsae/WI/variation/20210803/vcf/WI.20210803.hard-filter.isotype.vcf.gz' species='c_briggsae' date='20210803 ################# # load modules: module load bcftools # do the imputation with beagle5.2: for i in I II III IV V X do java -jar /projects/b1059/software/beagle/beagle5.2/beagle.28Jun21.220.jar chrom=${i} \\ window=5 \\ overlap=2 \\ impute=true \\ ne=100000 \\ nthreads=3 \\ imp-segment=0.5 \\ imp-step=0.01 \\ cluster=0.0005 \\ gt=$vcf \\ map=/projects/b1059/data/$species/genomes/genetic_map/chr${i}.map \\ out=${i}.b5 bcftools index ${i}.b5.vcf.gz done # concat all the chrom vcfs bcftools concat *.b5.vcf.gz -O z -o WI.${date}.impute.isotype.vcf.gz bcftools index -t WI.${date}.impute.isotype.vcf.gz bcftools stats --verbose WI.${date}.impute.isotype.vcf.gz > WI.${date}.impute.isotype.stats.txt","title":"Script overview"},{"location":"pipeline-impute/#software_requirements","text":"You can run this script on QUEST using the latest version of Beagle (5.2) in the /projects/b1059/software/ folder. Also requires bcftools (loaded in script).","title":"Software requirements"},{"location":"pipeline-impute/#usage","text":"Copy the script above and create a new file on QUEST (i.e. impute.sh ). Manually edit the vcf input, the species , and the date (to be the release date for all VCFs). Submit the job with sbatch impute.sh","title":"Usage"},{"location":"pipeline-impute/#parameters","text":"These parameters have been checked and decided on by previous lab members and Erik. Some chromosomes might require a window size of 3 and an overlap of 1. In recent conversation with the person who manages Beagle, they mentioned we should probably use default values unless we have done simulations to show these values are better. Note for the future maybe.","title":"Parameters"},{"location":"pipeline-nemascan/","text":"NemaScan \u00b6 GWA Mapping and Simulation with C. elegans, C. tropicalis, and C. briggsae Pipeline overview \u00b6 O~~~ O~~ O~~ ~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~~ O~~ O~~ O~~ O~~ O~~~ O~~ O~~ O~~ O~~ O~~ O~~ O~ O~~ O~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~ O~~O~~~~~ O~~ O~~ O~ O~~O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~ ~~O~ O~~ O~ O~~O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~~~ O~~~ O~ O~~ O~~ O~~~ O~~ ~~ O~~~ O~~ O~~~O~~~ O~~ parameters description Set/Default ========== =========== ======================== --traitfile Name of file containing strain and phenotype (required) --vcf Generally a CeNDR release date or path to vcf (optional - 20210121) --species c_elegans, c_briggsae, or c_tropicalis (optional - c_elegans) --sthresh Significance level for QTL (optional - BF) --maf Minimum minor allele frequency (optional - 0.05) # just a subset of main parameters for the mapping profle... Software Requirements \u00b6 This pipeline requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update On QUEST, all software requirements are provided within the pipeline using conda environments or a docker image. To run the pipeline outside of QUEST, you can load the docker image containing all necessary software, see more in profiles below. Required Software Packages (Loaded with conda or docker) \u00b6 Just for reference, pipeline keeps track of all software versions with conda or docker R-v3.6.0 nextflow-v20.0+ BCFtools-v1.9 plink-v1.9 bedtools-2.29.2 pandoc=2.12 R-coop-0.6-2 R-cowplot-1.0.0 R-data.table-1.12.8 R-DT-0.12 R-genetics-1.3.8.1.2 R-ggbeeswarm-v0.6 R-ggnewscale=0.4.5 R-ggrepel=0.8.2 R-knitr-1.28 R-plotly-4.9.2 R-Rcpp-1.0.1 R-rmarkdown-2.1 R-RSpectra-v0.13-1 R-sommer-4.0.4 R-tidyverse-v1.3.0 Usage \u00b6 Note It is not necessary to first download the git repo before running. However, you could still choose to do so with git clone https://github.com/AndersenLab/NemaScan.git Testing/debugging the mapping profile \u00b6 If you are trying to run a GWAS mapping with NemaScan, it might be a good idea to first run the debug test. This test takes only a few minutes and if it completes successfully, there is a good chance your real data run will also finish. nextflow run andersenlab/nemascan --debug To display the help message, run nextflow run andersenlab/nemascan --help Profiles and Parameters \u00b6 Mappings Profile \u00b6 This is the standard profile for running NemaScan. Use this profile to perform a genome-wide analysis with your trait of interest. To be explicit, you can use -profile mappings , however if no profile is provided, the pipeline will default to this one. nextflow run andersenlab/nemascan -profile mappings --vcf 20210121 --traitfile input_data/c_elegans/phenotypes/PC1.tsv Note You can also run specific branches or previous git commits easily. This can be especially useful to ensure that the version of NemaScan that you use doesn't change as you prepare your manuscript even if the code is updated. All you need to do is add a -r XXX to the end of your command, where XXX can be either (1) name of git branch, (2) name of git repo release, or (3) git commit ID For all runs, you can find the exact git commit used to run your analysis in the Nextflow report output after each run nextflow run andersenlab/nemascan --vcf 20210121 --traitfile input_data/c_elegans/phenotypes/PC1.tsv -r fa7046475fcfd06a49b375b4ef24a761f5133600 Check out this page for more tips and troubleshooting running Nextflow. --vcf \u00b6 CeNDR release date for the VCF file with variant data (i.e. \"20210121\") Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. If this flag is not used, the most recent VCF for the C. elegans species will be downloaded from CeNDR . Note If you want to use a custom VCF, you may provide the full path to the vcf in place of the CeNDR release date. This custom VCF will be used for BOTH GWA mapping and fine-mapping steps (instead of the imputed vcf). --traitfile \u00b6 A tab-delimited formatted (.tsv) file that contains trait information. Each phenotype file should be in the following format (replace trait_name with the phenotype of interest): strain trait_name_1 trait_name_2 JU258 32.73 19.34 ECA640 34.065378 12.32 ... ... ... ECA250 34.096 23.1 Optional Mapping Parameters \u00b6 --species - Choose between c_elegans (DEFAULT), c_tropicalis or c_briggsae --sthresh - This determines the signficance threshold required for performing post-mapping analysis of a QTL. BF corresponds to Bonferroni correction, EIGEN corresponds to correcting for the number of independent markers in your data set, and user-specified corresponds to a user-defined threshold, where you replace user-specified with a number. For example --sthresh=4 will set the threshold to a -log10(p) value of 4. We recommend using the strict BF correction as a first pass to see what the resulting data looks like. If the pipeline stops at the summarize_maps process, no significant QTL were discovered with the input threshold. You might want to consider lowering the threshold if this occurs. (Default: BF ) --mediation - 'true' or 'false' input for whether or not to run the mediation analysis (overlapping expression variation with phenotypic variation to drive a QTL). (Default: true ) --out - A user-specified output directory name. (Default: Analysis_Results-{date} ) --group_qtl - QTL within this distance of each other (bp) will be grouped as a single QTL by Find_GCTA_Intervals_*.R . (Default: 1000) --ci_size - The number of markers for which the detection interval will be extended past the last significant marker in the interval. (Default: 150) --maf - The minor allele frequency for filtering variants to use for gwas mapping (default 0.05) Genomatrix Profile \u00b6 This profile takes a list of strains and outputs the genotype matrix but does not perform any other analysis for the genome-wide association. nextflow run develop.nf -profile genomatrix --vcf 20210121 --strains input_data/elegans/phenotypes/strain_file.tsv --vcf \u00b6 CeNDR release date for the VCF file with variant data (i.e. \"20210121\") Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. If this flag is not used, the most recent VCF for the C. elegans species will be downloaded from CeNDR . Alternatively, you could choose to provide the full path to a custom VCF --strains \u00b6 A file (.tsv) that contains a list of strains used for generating the genotype matrix. There is no header: JU258 ECA640 ... ECA250 Simulations Profile \u00b6 This profile uses simulations to establish GWA performance benchmarks. Users can specify the heritability of simulated traits, the number of QTL underlying simulated traits of interest, the strains the user intends to use in a prospective GWA mapping experiment, or the location of previously detected QTL. Understanding the null expectations of GWA mappings within given parameter spaces may provide experimenters with additional guidance before initiating an experiment, or serve as a validation tool for previous mappings. nextflow develop.nf -profile simulations --vcf 20210121 --simulate_nqtl input_data/all_species/simulate_nqtl.csv --simulate_reps 2 --simulate_h2 input_data/all_species/simulate_h2.csv --simulate_eff input_data/all_species/simulate_effect_sizes.csv --simulate_strains input_data/all_species/simulate_strains.tsv --out example_simulation_output module load R/3.6.3 Rscript bin/Assess_Simulated_Mappings.R example_simulation_output --vcf \u00b6 CeNDR release date for the VCF file with variant data (i.e. \"20210121\") Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. If this flag is not used, the most recent VCF for the C. elegans species will be downloaded from CeNDR . --simulate_nqtl \u00b6 A single column CSV file that defines the number of QTL to simulate (format: one number per line, no column header) (Default is provided: input_data/all_species/simulate_nqtl.csv ). --simulate_reps \u00b6 The number of replicates to simulate per number of QTL and heritability (Default: 2). --simulate_h2 \u00b6 A CSV file with phenotype heritability. (format: one value per line, no column header) (Default is located: input_data/all_species/simulate_h2.csv ). --simulate_eff \u00b6 A CSV file specifying a range of causal QTL effects. QTL effects will be drawn from a uniform distribution bound by these two values. If the user wants to specify Gamma distributed effects, the value in this file can be simply specified as \"gamma\". (format: one value per line, no column header) (Default is located: input_data/all_species/simulate_effect_sizes.csv). --simulate_strains \u00b6 A TSV file specifying the population in which to simulate GWA mappings. Multiple populations can be simulated at once, but causal QTL will be drawn independently for each population as a result of minor allele frequency and LD pruning prior to mapping. (format: one line per population; supplied population name and a comma-separated list of each strain in the population) (Default is located: input_data/all_species/simulate_strains.tsv). Optional Simulation Parameters \u00b6 --simulate_maf - A single column CSV file that defines the minor allele frequency threshold used to filter the VCF prior to simulations (Default: 0.05). --simulate_qtlloc - A .bed file specifying genomic regions from which causal QTL are to be drawn after MAF filtering and LD pruning. (format: CHROM START END for each genomic region, with no header. NOTE: CHROM is specified as NUMERIC, not roman numerals as is convention in C. elegans )(Default is located: input_data/all_species/simulate_locations.bed). --group_qtl - QTL within this distance of each other (bp) will be grouped as a single QTL by Find_GCTA_Intervals_*.R . (Default: 1000) --ci_size - The number of markers for which the detection interval will be extended past the last significant marker in the interval. (Default: 150) Annotations Profile (in development) \u00b6 nextflow develop.nf --vcf 20210121 -profile annotations --species briggsae --wb_build WS270 --species - specifies what species information to download from WormBase (options: elegans, briggsae, tropicalis). --wb_build - specifies what WormBase build to download annotation information from (format: WSXXX, where XXX is a number greater than 270 and less than 277). GWA Mapping with Docker Profile \u00b6 This profile uses a docker image instead of local conda environments to perform the GWA mapping. Use this profile if you have issue with conda on QUEST or if you are running the pipeline outside of quest. NOTE: Docker or singularity is required On QUEST: module load singularity nextflow run andersenlab/nemascan --traitfile <file> --vcf 20210121 -profile mappings_docker Local make sure you have installed docker and that it is actively running. See here for help. nextflow run andersenlab/nemascan --traitfile <file> --vcf 20210121 -profile local Input Data Folder Structure ( NemaScan/input_data ) \u00b6 all_species \u251c\u2500\u2500 rename_chromosomes \u251c\u2500\u2500 simulate_effect_sizes.csv \u251c\u2500\u2500 simulate_h2.csv \u251c\u2500\u2500 simulate_maf.csv \u251c\u2500\u2500 simulate_nqtl.csv \u251c\u2500\u2500 simulate_strains.tsv \u2514\u2500\u2500 simulate_locations.bed c_elegans (repeated for c_tropicalis and c_briggsae) \u2514\u2500\u2500 genotypes \u251c\u2500\u2500 test_vcf \u251c\u2500\u2500 test_vcf_index \u2514\u2500\u2500 test_bcsq_annotation \u2514\u2500\u2500 phenotypes \u251c\u2500\u2500 PC1.tsv \u251c\u2500\u2500 strain_file.tsv \u2514\u2500\u2500 test_pheno.tsv \u2514\u2500\u2500 annotations \u251c\u2500\u2500 GTF file \u2514\u2500\u2500 refFlat file \u2514\u2500\u2500 isotypes \u251c\u2500\u2500 div_isotype_list.txt \u251c\u2500\u2500 divergent_bins.bed \u251c\u2500\u2500 divergent_df_isotype.bed \u251c\u2500\u2500 haplotype_df_isotype.bed \u2514\u2500\u2500 strain_isotype_lookup.tsv Mapping Output Folder Structure \u00b6 Phenotypes \u251c\u2500\u2500 strain_issues.txt \u2514\u2500\u2500 pr_traitname.tsv Genotype_Matrix \u251c\u2500\u2500 Genotype_Matrix.tsv \u2514\u2500\u2500 total_independent_tests.txt Mapping \u2514\u2500\u2500 Raw \u251c\u2500\u2500 traitname_lmm-exact_inbred.fastGWA \u2514\u2500\u2500 traitname_lmm-exact.loco.mlma \u2514\u2500\u2500 Processed \u251c\u2500\u2500 traitname_AGGREGATE_qtl_region.tsv \u251c\u2500\u2500 processed_traitname_AGGREGATE_mapping.tsv \u2514\u2500\u2500 QTL_peaks.tsv Plots \u2514\u2500\u2500 ManhattanPlots \u2514\u2500\u2500 traitname_manhattan.plot.png \u2514\u2500\u2500 LDPlots \u2514\u2500\u2500 traitname_LD.plot.png (if > 1 QTL detected) \u2514\u2500\u2500 EffectPlots \u251c\u2500\u2500 traitname_[QTL.INFO]_LOCO_effect.plot.png (if detected) \u2514\u2500\u2500 traitname_[QTL.INFO]_INBRED_effect.plot.png (if detected) Fine_Mappings \u2514\u2500\u2500 Data \u251c\u2500\u2500 traitname_[QTL.INFO]_bcsq_genes.tsv \u251c\u2500\u2500 traitname_[QTL.INFO]_ROI_Genotype_Matrix.tsv \u251c\u2500\u2500 traitname_[QTL.INFO]_finemap_inbred.fastGWA \u2514\u2500\u2500 traitname_[QTL.INFO]_LD.tsv \u2514\u2500\u2500 Plots \u251c\u2500\u2500 traitname_[QTL.INFO]_finemap_plot.pdf \u2514\u2500\u2500 traitname_[QTL.INFO]_gene_plot_bcsq.pdf Divergent_and_haplotype \u251c\u2500\u2500 all_QTL_bins.bed \u251c\u2500\u2500 all_QTL_div.bed \u251c\u2500\u2500 div_isotype_list.txt \u2514\u2500\u2500 haplotype_in_QTL_region.txt Reports \u251c\u2500\u2500 NemaScan_Report_traitname_main.html \u2514\u2500\u2500 NemaScan_Report_traitname_main.Rmd Phenotypes folder \u00b6 strain_issues.txt - Output of any strain names that were changed to match vcf (i.e. isotypes that are not reference strains) pr_traitname.tsv - Processed phenotype file for each trait. This is the file that goes into the mapping Genotype_Matrix folder \u00b6 Genotype_Matrix.tsv - LD-pruned genotype matrix used for GWAS and construction of kinship matrix total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix Mapping folder \u00b6 Raw \u00b6 traitname_lmm-exact_inbred.fastGWA - Raw mapping results from GCTA's fastGWA program using an inbred kinship matrix traitname_lmm-exact.loco.mlma - Raw mapping results from GCTA's mlma program using a kinship matrix constructed from all chromosomes except for the chromosome containing each tested variant Processed \u00b6 traitname_AGGREGATE_mapping.tsv - Combined processed mapping results from lmm-exact_inbred and lmm-exact.loco.mlma raw mappings. Contains additional information nested such as 1) rough intervals (see parameters for calculation) and estimates of the variance explained by the detected QTL 2) phenotype information and genotype status for each strain at the detected QTL. traitname_AGGREGATE_qtl_region.tsv - Contains only QTL information for each mapping. If no QTL are detected, an empty data frame is written. QTL_peaks.tsv - contains QTL information for each mapping for all traits combined. Plots \u00b6 traitname_manhattan.plot.png - Standard output for GWA; association of marker differences with phenotypic variation in the population. traitname_LD.plot.png - If more than 1 QTL are detected for a trait, a plot showing the linkage disequilibrium between each QTL is generated. traitname_[QTL.INFO]_INBRED_effect.plot.png - Phenotypes for each strain are plotted against their marker genotype at the peak marker for each QTL detected for a trait. The dot representing each strain is shaded according to the percentage of the chromosome containing the QTL that is characterized as a selective sweep region. Fine_Mappings folder \u00b6 Data \u00b6 traitname_bcsq_genes.tsv - Fine-mapping data frame for all significant QTL Plots \u00b6 traitname_qtlinterval_finemap_plot.pdf - Fine map plot of QTL interval, colored by marker LD with the peak QTL identified from the genome-wide scan traitname_qtlinterval_gene_plot.pdf - variant annotation plot overlaid with gene CDS for QTL interval Simulation Output Folder Structure \u00b6 Genotype_Matrix \u251c\u2500\u2500 [strain_set]_[MAF]_Genotype_Matrix.tsv \u2514\u2500\u2500 [strain_set]_[MAF]_total_independent_tests.txt Simulations \u251c\u2500\u2500 NemaScan_Performance.example_simulation_output.RData \u2514\u2500\u2500 [specified effect range (simulate_effect_sizes.csv)] \u2514\u2500\u2500 [specified number of simulated QTL (simulate_nqtl.csv)] \u2514\u2500\u2500 Mappings \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_processed_LMM_EXACT_INBRED_mapping.tsv \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_processed_LMM_EXACT_LOCO_mapping.tsv \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_lmm-exact_inbred.fastGWA \u2514\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_lmm-exact.loco.mlma \u2514\u2500\u2500 Phenotypes \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.phen \u2514\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.par \u2514\u2500\u2500 (if applicable) [NEXT specified effect range] \u2514\u2500\u2500 ... \u2514\u2500\u2500 (if applicable) [NEXT specified effect range] \u2514\u2500\u2500 ... Genotype_Matrix folder \u00b6 *Genotype_Matrix.tsv - pruned LD-pruned genotype matrix used for GWAS and construction of kinship matrix. This will be appended with the chosen minor allele frequency cutoff and strain set, as they are generated separately for each strain set. *total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix. This will be also be appended with the chosen minor allele frequency cutoff and strain set, as they are generated separately for each strain set. Simulations \u00b6 NemaScan_Performance.*.RData - RData file containing all simulated and detected QTL from each successful simulated mapping. Contains: 1. Simulated and Detected status for each QTL. 2. Minor allele frequency and simulated or estimated effect for each QTL. 3. Detection interval according to specified grouping size and CI extension. 4. Estimated variance explained for each detected QTL. 5. Simulation parameters and the algorithm used for that particular regime. Mappings \u00b6 As with the mapping profile, raw and processed mappings for each simulation regime are nested within folders corresponding each specified effect range and number of simulated QTL. QTL region files are not provided in the simulation profile; this information along with other information related to mapping performance are iteratively gathered in the generation of the performance .RData file. Phenotypes \u00b6 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.phen - Simulated strain phenotypes for each simulation regime. [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.par - Simulated QTL effects for each simulation regime. NOTE: Simulation regimes with identical numbers of simulated QTL, replicate indices, and simulated heritabilities should have identical simulated QTL and effects.","title":"NemaScan"},{"location":"pipeline-nemascan/#nemascan","text":"GWA Mapping and Simulation with C. elegans, C. tropicalis, and C. briggsae","title":"NemaScan"},{"location":"pipeline-nemascan/#pipeline_overview","text":"O~~~ O~~ O~~ ~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~~ O~~ O~~ O~~ O~~ O~~~ O~~ O~~ O~~ O~~ O~~ O~~ O~ O~~ O~~ O~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~ O~~O~~~~~ O~~ O~~ O~ O~~O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~ ~~O~ O~~ O~ O~~O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~ O~~~~ O~~~ O~ O~~ O~~ O~~~ O~~ ~~ O~~~ O~~ O~~~O~~~ O~~ parameters description Set/Default ========== =========== ======================== --traitfile Name of file containing strain and phenotype (required) --vcf Generally a CeNDR release date or path to vcf (optional - 20210121) --species c_elegans, c_briggsae, or c_tropicalis (optional - c_elegans) --sthresh Significance level for QTL (optional - BF) --maf Minimum minor allele frequency (optional - 0.05) # just a subset of main parameters for the mapping profle...","title":"Pipeline overview"},{"location":"pipeline-nemascan/#software_requirements","text":"This pipeline requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update On QUEST, all software requirements are provided within the pipeline using conda environments or a docker image. To run the pipeline outside of QUEST, you can load the docker image containing all necessary software, see more in profiles below.","title":"Software Requirements"},{"location":"pipeline-nemascan/#required_software_packages_loaded_with_conda_or_docker","text":"Just for reference, pipeline keeps track of all software versions with conda or docker R-v3.6.0 nextflow-v20.0+ BCFtools-v1.9 plink-v1.9 bedtools-2.29.2 pandoc=2.12 R-coop-0.6-2 R-cowplot-1.0.0 R-data.table-1.12.8 R-DT-0.12 R-genetics-1.3.8.1.2 R-ggbeeswarm-v0.6 R-ggnewscale=0.4.5 R-ggrepel=0.8.2 R-knitr-1.28 R-plotly-4.9.2 R-Rcpp-1.0.1 R-rmarkdown-2.1 R-RSpectra-v0.13-1 R-sommer-4.0.4 R-tidyverse-v1.3.0","title":"Required Software Packages (Loaded with conda or docker)"},{"location":"pipeline-nemascan/#usage","text":"Note It is not necessary to first download the git repo before running. However, you could still choose to do so with git clone https://github.com/AndersenLab/NemaScan.git","title":"Usage"},{"location":"pipeline-nemascan/#testingdebugging_the_mapping_profile","text":"If you are trying to run a GWAS mapping with NemaScan, it might be a good idea to first run the debug test. This test takes only a few minutes and if it completes successfully, there is a good chance your real data run will also finish. nextflow run andersenlab/nemascan --debug To display the help message, run nextflow run andersenlab/nemascan --help","title":"Testing/debugging the mapping profile"},{"location":"pipeline-nemascan/#profiles_and_parameters","text":"","title":"Profiles and Parameters"},{"location":"pipeline-nemascan/#mappings_profile","text":"This is the standard profile for running NemaScan. Use this profile to perform a genome-wide analysis with your trait of interest. To be explicit, you can use -profile mappings , however if no profile is provided, the pipeline will default to this one. nextflow run andersenlab/nemascan -profile mappings --vcf 20210121 --traitfile input_data/c_elegans/phenotypes/PC1.tsv Note You can also run specific branches or previous git commits easily. This can be especially useful to ensure that the version of NemaScan that you use doesn't change as you prepare your manuscript even if the code is updated. All you need to do is add a -r XXX to the end of your command, where XXX can be either (1) name of git branch, (2) name of git repo release, or (3) git commit ID For all runs, you can find the exact git commit used to run your analysis in the Nextflow report output after each run nextflow run andersenlab/nemascan --vcf 20210121 --traitfile input_data/c_elegans/phenotypes/PC1.tsv -r fa7046475fcfd06a49b375b4ef24a761f5133600 Check out this page for more tips and troubleshooting running Nextflow.","title":"Mappings Profile"},{"location":"pipeline-nemascan/#--vcf","text":"CeNDR release date for the VCF file with variant data (i.e. \"20210121\") Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. If this flag is not used, the most recent VCF for the C. elegans species will be downloaded from CeNDR . Note If you want to use a custom VCF, you may provide the full path to the vcf in place of the CeNDR release date. This custom VCF will be used for BOTH GWA mapping and fine-mapping steps (instead of the imputed vcf).","title":"--vcf"},{"location":"pipeline-nemascan/#--traitfile","text":"A tab-delimited formatted (.tsv) file that contains trait information. Each phenotype file should be in the following format (replace trait_name with the phenotype of interest): strain trait_name_1 trait_name_2 JU258 32.73 19.34 ECA640 34.065378 12.32 ... ... ... ECA250 34.096 23.1","title":"--traitfile"},{"location":"pipeline-nemascan/#optional_mapping_parameters","text":"--species - Choose between c_elegans (DEFAULT), c_tropicalis or c_briggsae --sthresh - This determines the signficance threshold required for performing post-mapping analysis of a QTL. BF corresponds to Bonferroni correction, EIGEN corresponds to correcting for the number of independent markers in your data set, and user-specified corresponds to a user-defined threshold, where you replace user-specified with a number. For example --sthresh=4 will set the threshold to a -log10(p) value of 4. We recommend using the strict BF correction as a first pass to see what the resulting data looks like. If the pipeline stops at the summarize_maps process, no significant QTL were discovered with the input threshold. You might want to consider lowering the threshold if this occurs. (Default: BF ) --mediation - 'true' or 'false' input for whether or not to run the mediation analysis (overlapping expression variation with phenotypic variation to drive a QTL). (Default: true ) --out - A user-specified output directory name. (Default: Analysis_Results-{date} ) --group_qtl - QTL within this distance of each other (bp) will be grouped as a single QTL by Find_GCTA_Intervals_*.R . (Default: 1000) --ci_size - The number of markers for which the detection interval will be extended past the last significant marker in the interval. (Default: 150) --maf - The minor allele frequency for filtering variants to use for gwas mapping (default 0.05)","title":"Optional Mapping Parameters"},{"location":"pipeline-nemascan/#genomatrix_profile","text":"This profile takes a list of strains and outputs the genotype matrix but does not perform any other analysis for the genome-wide association. nextflow run develop.nf -profile genomatrix --vcf 20210121 --strains input_data/elegans/phenotypes/strain_file.tsv","title":"Genomatrix Profile"},{"location":"pipeline-nemascan/#--vcf_1","text":"CeNDR release date for the VCF file with variant data (i.e. \"20210121\") Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. If this flag is not used, the most recent VCF for the C. elegans species will be downloaded from CeNDR . Alternatively, you could choose to provide the full path to a custom VCF","title":"--vcf"},{"location":"pipeline-nemascan/#--strains","text":"A file (.tsv) that contains a list of strains used for generating the genotype matrix. There is no header: JU258 ECA640 ... ECA250","title":"--strains"},{"location":"pipeline-nemascan/#simulations_profile","text":"This profile uses simulations to establish GWA performance benchmarks. Users can specify the heritability of simulated traits, the number of QTL underlying simulated traits of interest, the strains the user intends to use in a prospective GWA mapping experiment, or the location of previously detected QTL. Understanding the null expectations of GWA mappings within given parameter spaces may provide experimenters with additional guidance before initiating an experiment, or serve as a validation tool for previous mappings. nextflow develop.nf -profile simulations --vcf 20210121 --simulate_nqtl input_data/all_species/simulate_nqtl.csv --simulate_reps 2 --simulate_h2 input_data/all_species/simulate_h2.csv --simulate_eff input_data/all_species/simulate_effect_sizes.csv --simulate_strains input_data/all_species/simulate_strains.tsv --out example_simulation_output module load R/3.6.3 Rscript bin/Assess_Simulated_Mappings.R example_simulation_output","title":"Simulations Profile"},{"location":"pipeline-nemascan/#--vcf_2","text":"CeNDR release date for the VCF file with variant data (i.e. \"20210121\") Hard-filter VCF will be used for the GWA mapping and imputed VCF will be used for fine mapping. If this flag is not used, the most recent VCF for the C. elegans species will be downloaded from CeNDR .","title":"--vcf"},{"location":"pipeline-nemascan/#--simulate_nqtl","text":"A single column CSV file that defines the number of QTL to simulate (format: one number per line, no column header) (Default is provided: input_data/all_species/simulate_nqtl.csv ).","title":"--simulate_nqtl"},{"location":"pipeline-nemascan/#--simulate_reps","text":"The number of replicates to simulate per number of QTL and heritability (Default: 2).","title":"--simulate_reps"},{"location":"pipeline-nemascan/#--simulate_h2","text":"A CSV file with phenotype heritability. (format: one value per line, no column header) (Default is located: input_data/all_species/simulate_h2.csv ).","title":"--simulate_h2"},{"location":"pipeline-nemascan/#--simulate_eff","text":"A CSV file specifying a range of causal QTL effects. QTL effects will be drawn from a uniform distribution bound by these two values. If the user wants to specify Gamma distributed effects, the value in this file can be simply specified as \"gamma\". (format: one value per line, no column header) (Default is located: input_data/all_species/simulate_effect_sizes.csv).","title":"--simulate_eff"},{"location":"pipeline-nemascan/#--simulate_strains","text":"A TSV file specifying the population in which to simulate GWA mappings. Multiple populations can be simulated at once, but causal QTL will be drawn independently for each population as a result of minor allele frequency and LD pruning prior to mapping. (format: one line per population; supplied population name and a comma-separated list of each strain in the population) (Default is located: input_data/all_species/simulate_strains.tsv).","title":"--simulate_strains"},{"location":"pipeline-nemascan/#optional_simulation_parameters","text":"--simulate_maf - A single column CSV file that defines the minor allele frequency threshold used to filter the VCF prior to simulations (Default: 0.05). --simulate_qtlloc - A .bed file specifying genomic regions from which causal QTL are to be drawn after MAF filtering and LD pruning. (format: CHROM START END for each genomic region, with no header. NOTE: CHROM is specified as NUMERIC, not roman numerals as is convention in C. elegans )(Default is located: input_data/all_species/simulate_locations.bed). --group_qtl - QTL within this distance of each other (bp) will be grouped as a single QTL by Find_GCTA_Intervals_*.R . (Default: 1000) --ci_size - The number of markers for which the detection interval will be extended past the last significant marker in the interval. (Default: 150)","title":"Optional Simulation Parameters"},{"location":"pipeline-nemascan/#annotations_profile_in_development","text":"nextflow develop.nf --vcf 20210121 -profile annotations --species briggsae --wb_build WS270 --species - specifies what species information to download from WormBase (options: elegans, briggsae, tropicalis). --wb_build - specifies what WormBase build to download annotation information from (format: WSXXX, where XXX is a number greater than 270 and less than 277).","title":"Annotations Profile (in development)"},{"location":"pipeline-nemascan/#gwa_mapping_with_docker_profile","text":"This profile uses a docker image instead of local conda environments to perform the GWA mapping. Use this profile if you have issue with conda on QUEST or if you are running the pipeline outside of quest. NOTE: Docker or singularity is required On QUEST: module load singularity nextflow run andersenlab/nemascan --traitfile <file> --vcf 20210121 -profile mappings_docker Local make sure you have installed docker and that it is actively running. See here for help. nextflow run andersenlab/nemascan --traitfile <file> --vcf 20210121 -profile local","title":"GWA Mapping with Docker Profile"},{"location":"pipeline-nemascan/#input_data_folder_structure_nemascaninput_data","text":"all_species \u251c\u2500\u2500 rename_chromosomes \u251c\u2500\u2500 simulate_effect_sizes.csv \u251c\u2500\u2500 simulate_h2.csv \u251c\u2500\u2500 simulate_maf.csv \u251c\u2500\u2500 simulate_nqtl.csv \u251c\u2500\u2500 simulate_strains.tsv \u2514\u2500\u2500 simulate_locations.bed c_elegans (repeated for c_tropicalis and c_briggsae) \u2514\u2500\u2500 genotypes \u251c\u2500\u2500 test_vcf \u251c\u2500\u2500 test_vcf_index \u2514\u2500\u2500 test_bcsq_annotation \u2514\u2500\u2500 phenotypes \u251c\u2500\u2500 PC1.tsv \u251c\u2500\u2500 strain_file.tsv \u2514\u2500\u2500 test_pheno.tsv \u2514\u2500\u2500 annotations \u251c\u2500\u2500 GTF file \u2514\u2500\u2500 refFlat file \u2514\u2500\u2500 isotypes \u251c\u2500\u2500 div_isotype_list.txt \u251c\u2500\u2500 divergent_bins.bed \u251c\u2500\u2500 divergent_df_isotype.bed \u251c\u2500\u2500 haplotype_df_isotype.bed \u2514\u2500\u2500 strain_isotype_lookup.tsv","title":"Input Data Folder Structure (NemaScan/input_data)"},{"location":"pipeline-nemascan/#mapping_output_folder_structure","text":"Phenotypes \u251c\u2500\u2500 strain_issues.txt \u2514\u2500\u2500 pr_traitname.tsv Genotype_Matrix \u251c\u2500\u2500 Genotype_Matrix.tsv \u2514\u2500\u2500 total_independent_tests.txt Mapping \u2514\u2500\u2500 Raw \u251c\u2500\u2500 traitname_lmm-exact_inbred.fastGWA \u2514\u2500\u2500 traitname_lmm-exact.loco.mlma \u2514\u2500\u2500 Processed \u251c\u2500\u2500 traitname_AGGREGATE_qtl_region.tsv \u251c\u2500\u2500 processed_traitname_AGGREGATE_mapping.tsv \u2514\u2500\u2500 QTL_peaks.tsv Plots \u2514\u2500\u2500 ManhattanPlots \u2514\u2500\u2500 traitname_manhattan.plot.png \u2514\u2500\u2500 LDPlots \u2514\u2500\u2500 traitname_LD.plot.png (if > 1 QTL detected) \u2514\u2500\u2500 EffectPlots \u251c\u2500\u2500 traitname_[QTL.INFO]_LOCO_effect.plot.png (if detected) \u2514\u2500\u2500 traitname_[QTL.INFO]_INBRED_effect.plot.png (if detected) Fine_Mappings \u2514\u2500\u2500 Data \u251c\u2500\u2500 traitname_[QTL.INFO]_bcsq_genes.tsv \u251c\u2500\u2500 traitname_[QTL.INFO]_ROI_Genotype_Matrix.tsv \u251c\u2500\u2500 traitname_[QTL.INFO]_finemap_inbred.fastGWA \u2514\u2500\u2500 traitname_[QTL.INFO]_LD.tsv \u2514\u2500\u2500 Plots \u251c\u2500\u2500 traitname_[QTL.INFO]_finemap_plot.pdf \u2514\u2500\u2500 traitname_[QTL.INFO]_gene_plot_bcsq.pdf Divergent_and_haplotype \u251c\u2500\u2500 all_QTL_bins.bed \u251c\u2500\u2500 all_QTL_div.bed \u251c\u2500\u2500 div_isotype_list.txt \u2514\u2500\u2500 haplotype_in_QTL_region.txt Reports \u251c\u2500\u2500 NemaScan_Report_traitname_main.html \u2514\u2500\u2500 NemaScan_Report_traitname_main.Rmd","title":"Mapping Output Folder Structure"},{"location":"pipeline-nemascan/#phenotypes_folder","text":"strain_issues.txt - Output of any strain names that were changed to match vcf (i.e. isotypes that are not reference strains) pr_traitname.tsv - Processed phenotype file for each trait. This is the file that goes into the mapping","title":"Phenotypes folder"},{"location":"pipeline-nemascan/#genotype_matrix_folder","text":"Genotype_Matrix.tsv - LD-pruned genotype matrix used for GWAS and construction of kinship matrix total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix","title":"Genotype_Matrix folder"},{"location":"pipeline-nemascan/#mapping_folder","text":"","title":"Mapping folder"},{"location":"pipeline-nemascan/#raw","text":"traitname_lmm-exact_inbred.fastGWA - Raw mapping results from GCTA's fastGWA program using an inbred kinship matrix traitname_lmm-exact.loco.mlma - Raw mapping results from GCTA's mlma program using a kinship matrix constructed from all chromosomes except for the chromosome containing each tested variant","title":"Raw"},{"location":"pipeline-nemascan/#processed","text":"traitname_AGGREGATE_mapping.tsv - Combined processed mapping results from lmm-exact_inbred and lmm-exact.loco.mlma raw mappings. Contains additional information nested such as 1) rough intervals (see parameters for calculation) and estimates of the variance explained by the detected QTL 2) phenotype information and genotype status for each strain at the detected QTL. traitname_AGGREGATE_qtl_region.tsv - Contains only QTL information for each mapping. If no QTL are detected, an empty data frame is written. QTL_peaks.tsv - contains QTL information for each mapping for all traits combined.","title":"Processed"},{"location":"pipeline-nemascan/#plots","text":"traitname_manhattan.plot.png - Standard output for GWA; association of marker differences with phenotypic variation in the population. traitname_LD.plot.png - If more than 1 QTL are detected for a trait, a plot showing the linkage disequilibrium between each QTL is generated. traitname_[QTL.INFO]_INBRED_effect.plot.png - Phenotypes for each strain are plotted against their marker genotype at the peak marker for each QTL detected for a trait. The dot representing each strain is shaded according to the percentage of the chromosome containing the QTL that is characterized as a selective sweep region.","title":"Plots"},{"location":"pipeline-nemascan/#fine_mappings_folder","text":"","title":"Fine_Mappings folder"},{"location":"pipeline-nemascan/#data","text":"traitname_bcsq_genes.tsv - Fine-mapping data frame for all significant QTL","title":"Data"},{"location":"pipeline-nemascan/#plots_1","text":"traitname_qtlinterval_finemap_plot.pdf - Fine map plot of QTL interval, colored by marker LD with the peak QTL identified from the genome-wide scan traitname_qtlinterval_gene_plot.pdf - variant annotation plot overlaid with gene CDS for QTL interval","title":"Plots"},{"location":"pipeline-nemascan/#simulation_output_folder_structure","text":"Genotype_Matrix \u251c\u2500\u2500 [strain_set]_[MAF]_Genotype_Matrix.tsv \u2514\u2500\u2500 [strain_set]_[MAF]_total_independent_tests.txt Simulations \u251c\u2500\u2500 NemaScan_Performance.example_simulation_output.RData \u2514\u2500\u2500 [specified effect range (simulate_effect_sizes.csv)] \u2514\u2500\u2500 [specified number of simulated QTL (simulate_nqtl.csv)] \u2514\u2500\u2500 Mappings \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_processed_LMM_EXACT_INBRED_mapping.tsv \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_processed_LMM_EXACT_LOCO_mapping.tsv \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_lmm-exact_inbred.fastGWA \u2514\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_lmm-exact.loco.mlma \u2514\u2500\u2500 Phenotypes \u251c\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.phen \u2514\u2500\u2500 [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.par \u2514\u2500\u2500 (if applicable) [NEXT specified effect range] \u2514\u2500\u2500 ... \u2514\u2500\u2500 (if applicable) [NEXT specified effect range] \u2514\u2500\u2500 ...","title":"Simulation Output Folder Structure"},{"location":"pipeline-nemascan/#genotype_matrix_folder_1","text":"*Genotype_Matrix.tsv - pruned LD-pruned genotype matrix used for GWAS and construction of kinship matrix. This will be appended with the chosen minor allele frequency cutoff and strain set, as they are generated separately for each strain set. *total_independent_tests.txt - number of independent tests determined through spectral decomposition of the genotype matrix. This will be also be appended with the chosen minor allele frequency cutoff and strain set, as they are generated separately for each strain set.","title":"Genotype_Matrix folder"},{"location":"pipeline-nemascan/#simulations","text":"NemaScan_Performance.*.RData - RData file containing all simulated and detected QTL from each successful simulated mapping. Contains: 1. Simulated and Detected status for each QTL. 2. Minor allele frequency and simulated or estimated effect for each QTL. 3. Detection interval according to specified grouping size and CI extension. 4. Estimated variance explained for each detected QTL. 5. Simulation parameters and the algorithm used for that particular regime.","title":"Simulations"},{"location":"pipeline-nemascan/#mappings","text":"As with the mapping profile, raw and processed mappings for each simulation regime are nested within folders corresponding each specified effect range and number of simulated QTL. QTL region files are not provided in the simulation profile; this information along with other information related to mapping performance are iteratively gathered in the generation of the performance .RData file.","title":"Mappings"},{"location":"pipeline-nemascan/#phenotypes","text":"[nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.phen - Simulated strain phenotypes for each simulation regime. [nQTL]_[rep]_[h2]_[MAF]_[effect range]_[strain_set]_sims.par - Simulated QTL effects for each simulation regime. NOTE: Simulation regimes with identical numbers of simulated QTL, replicate indices, and simulated heritabilities should have identical simulated QTL and effects.","title":"Phenotypes"},{"location":"pipeline-nil-ril/","text":"nil-ril-nf \u00b6 nil-ril-nf Overview Docker image Usage Requirements Profiles and Running the Pipeline Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Testing Parameters --fqs --vcf --reference --A, --B (optional) --debug (optional) --cores (optional) --cA, --cB (optional) --out (optional) --relative (optional) --cross_obj (optional) --tmpdir (optional) Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ Organizing final data Adding NIL sequence data to lab website The nil-ril-nf pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data. Overview \u00b6 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --relative use relative fastq prefix true --reference Reference Genome (required) --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default). Alignment - Performed using bwa-mem Merge Bams - Combines bam files aligned individually for each fastq-pair. Sambamba is actually used in place of samtools, but it's a drop-in, faster replacement. Bam Stats - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis. Mark Duplicates - Duplicate reads are marked using Picard. Call Variants individual - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population. Pull parental genotypes - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF. Call variants union - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls. Merge VCF - Merges in the parental VCF (which has been filtered only for variants with discordant calls). Call HMM - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data. Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible. Docker image \u00b6 The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image: andersenlab/nil-ril-nf The Dockerfile is stored in the root of the nil-ril-nf github repo and is automatically built on Dockerhub whenever the repo is pushed. Usage \u00b6 Requirements \u00b6 The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Profiles and Running the Pipeline \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. The nextflow.config file included with this pipeline contains four profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. travis - Used by travis-ci for testing purposes. Running the pipeline locally \u00b6 When running locally, the pipeline will run using the andersenlab/nil-ril-nf docker image. You must have docker installed. You will need to obtain a reference genome to run the alignment with as well. You can use the following command to obtain the reference: curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz Run the pipeline locally with: nextflow run andersenlab/nil-ril-nf -profile local -resume Debugging the pipeline on Quest \u00b6 If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run andersenlab/nil-ril-nf -profile quest_debug -resume Note There is no need to clone the git repo before running the pipeline. However, you may still choose to do so if you plan to manually track the git commit used to generate data. Running the pipeline on Quest \u00b6 If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity The pipeline can be run on Quest using the following command: nextflow run andersenlab/nil-ril-nf -profile quest -resume Testing \u00b6 If you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the -resume option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly. Additionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci here , and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail. The command below can be used to test the pipeline locally. # Downloads a pre-indexed reference curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz # Run nextflow nextflow run andersenlab/nil-ril-nf \\ -with-docker andersenlab/nil-ril-nf \\ --debug \\ --reference=WS276.fa.gz \\ -resume Note that the path to the vcf will change slightly in releases later than WI-20170531; See the wi-gatk pipeline for details. The command above will automatically place results in a folder: NIL-N2-CB4856-YYYY-MM-DD Parameters \u00b6 --fqs \u00b6 In order to process NIL/RIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq can be specified as relative or absolute . By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not include the parental strains in the fq_sheet. If you re-sequenced the parent strains and want to include them in the analysis as a control, you need to rename the parent strains to avoid an error in merging the VCFs (i.e. N2 becomes N2-1). --vcf \u00b6 Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, the most recent C. elegans VCF could be found here: /projects/b1059/data/c_elegans/WI/variation/20210121/vcf/WI.20210121.hard-filter.isotype.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20210121.hard-filter.isotype.vcf.gz --reference \u00b6 A fasta reference indexed with BWA. For example, the C. elegans reference could be found here: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz --A, --B (optional) \u00b6 Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details. --debug (optional) \u00b6 The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the Testing section for more information. --cores (optional) \u00b6 The number of cores to use during alignments and variant calling. Default is 4. --cA, --cB (optional) \u00b6 The color to use for parental strain A and B on plots. Default is orange and blue. --out (optional) \u00b6 A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains. --relative (optional) \u00b6 If you want to specify fastqs using an absolute path use --relative=false . Set to true by default. --cross_obj (optional) \u00b6 If you are running a set of RILs, you might want to add the --cross_obj true parameter. When true , the pipeline will run an additional step to pair down the total genetic variants to only the informative variants and output a smaller genotype matrix to input directly into a new cross object. An example of how to generate a cross object can be found in the bin . There is no need to run this option for NIL data. --tmpdir (optional) \u00b6 A directory for storing temporary data. Output \u00b6 The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u251c\u2500\u2500 SM_union_vcfs.txt \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u251c\u2500\u2500 gt_hmm.tsv \u2502 \u251c\u2500\u2500 gt_hmm_fill.tsv \u2502 \u251c\u2500\u2500 NIL.filtered.stats.txt \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u2502 \u2514\u2500\u2500 gt_hmm_genotypes.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u2514\u2500 sitelist \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi] log.txt \u00b6 A summary of the nextflow run. duplicates/ \u00b6 bam_duplicates.tsv - A summary of duplicate reads from aligned bams. fq/ \u00b6 fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq SM/ \u00b6 If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. SM_union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz hmm/ \u00b6 Important gt_hmm_fill.tsv is for visualization purposes only. To determine breakpoints you should use gt_hmm.tsv . The --infill and --endfill options are applied to the gt_hmm_fill.tsv file. You need to be cautious when examining this data as it is generated primarily for visualization purposes . gt_hmm.(png/svg) - Haplotype plot using --infill and --endfill . gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation . This file is used to generate the plots. gt_hmm.tsv - Haplotypes defined by region with associated information. Does not use --infill and --endfill gt_hmm_genotypes.tsv - Long form genotypes file. NIL/RIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL/RIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL/RIL.hmm.vcf.gz - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes. plots/ \u00b6 coverage_comparison.png - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/pdf) - Coverage vs. percent duplicated. unmapped_reads.png - Coverage vs. unmapped read percent. sitelist/ \u00b6 <A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. <A>.<B>.sitelist.vcf.gz[+.tbi] - A vcf of sites found to be different between both parental strains. Organizing final data \u00b6 After the run is complete and you are satisfied with the results, follow these steps to ensure correct data storage on QUEST: Move the raw fastq files to /projects/b1059/data/{species}/{NIL or RIL}/fastq/ . You might want to use mv -i to ensure no files are overwritten. Move the BAM files from the output folder to /projects/b1059/data/{species}/{NIL or RIL}/alignments/ . You might want to use mv -i to ensure no files are overwritten. Delete the now empty bam folder in the output directory. Move the sample sheet generated for analysis into the output directory. Make sure the output directory follows the default naming structure that is informative about the analysis (i.e. NIL-20200322-N2-CB4856 (if NIL/RIL analysis is performed for another lab, consider adding a -{LabName} like -Baugh to the end of the folder name)). Move the entire output folder to /projects/b1059/data/{species}/{NIL or RIL}/variation . Adding NIL sequence data to lab website \u00b6 If your sequencing was N2-CB4856 NILs (and maybe other C. elegans NILs as well...?) you probably want to add this sequencing data to the lab website to be accessed by everyone when looking for NIL genotypes. Check out this page for instructions on how to do that. Once done, you should be able to view your NILs on the NIL browser shiny app .","title":"nil-ril-nf"},{"location":"pipeline-nil-ril/#nil-ril-nf","text":"nil-ril-nf Overview Docker image Usage Requirements Profiles and Running the Pipeline Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Testing Parameters --fqs --vcf --reference --A, --B (optional) --debug (optional) --cores (optional) --cA, --cB (optional) --out (optional) --relative (optional) --cross_obj (optional) --tmpdir (optional) Output log.txt duplicates/ fq/ SM/ hmm/ plots/ sitelist/ Organizing final data Adding NIL sequence data to lab website The nil-ril-nf pipeline will align, call variants, and generate datasets for NIL and RIL sequence data. It runs a hidden-markov-model to fill in missing genotypes from low-coverage sequence data.","title":"nil-ril-nf"},{"location":"pipeline-nil-ril/#overview","text":"\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2557 \u2588\u2588\u2588\u2557 \u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551 \u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d \u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u255d \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u2588\u2588\u2551\u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551 \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u255d\u255a\u2550\u255d\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u255d\u255a\u2550\u255d parameters description Set/Default ========== =========== ======= --debug Set to 'true' to test false --cores Number of cores 4 --A Parent A N2 --B Parent B CB4856 --cA Parent A color (for plots) #0080FF --cB Parent B color (for plots) #FF8000 --out Directory to output results NIL-N2-CB4856-2017-09-27 --fqs fastq file (see help) (required) --relative use relative fastq prefix true --reference Reference Genome (required) --vcf VCF to fetch parents from (required) --tmpdir A temporary directory tmp/ The Set/Default column shows what the value is currently set to or would be set to if it is not specified (it's default). Alignment - Performed using bwa-mem Merge Bams - Combines bam files aligned individually for each fastq-pair. Sambamba is actually used in place of samtools, but it's a drop-in, faster replacement. Bam Stats - A variety of metrics are calculated for bams and combined into individual files for downstream analsyis. Mark Duplicates - Duplicate reads are marked using Picard. Call Variants individual - Variants are called for each strain inidividually first. This generates a sitelist which is used to identify all variant sites in the population. Pull parental genotypes - Pulls out parental genotypes from the given VCF. The list of genotypes is filtered for discordant calls (i.e. different genotypes). This is VCF is used to generate a sitelist for calling low-coverage bams and later is merged into the resulting VCF. Call variants union - Uses the sitelist from the previous step to call variants on low-coverage sequence data. The resulting VCF will have a lot of missing calls. Merge VCF - Merges in the parental VCF (which has been filtered only for variants with discordant calls). Call HMM - VCF-kit is run in various ways to infer the appropriate genotypes from the low-coverage sequence data. Important Do not perform any pre-processing on NIL data. NIL-data is low-coverage by design and you want to retain as much sequence data (however poor) as possible.","title":"Overview"},{"location":"pipeline-nil-ril/#docker_image","text":"The docker image used by the nil-ril-nf pipeline is the nil-ril-nf docker image: andersenlab/nil-ril-nf The Dockerfile is stored in the root of the nil-ril-nf github repo and is automatically built on Dockerhub whenever the repo is pushed.","title":"Docker image"},{"location":"pipeline-nil-ril/#usage","text":"","title":"Usage"},{"location":"pipeline-nil-ril/#requirements","text":"The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env","title":"Requirements"},{"location":"pipeline-nil-ril/#profiles_and_running_the_pipeline","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. The nextflow.config file included with this pipeline contains four profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. travis - Used by travis-ci for testing purposes.","title":"Profiles and Running the Pipeline"},{"location":"pipeline-nil-ril/#running_the_pipeline_locally","text":"When running locally, the pipeline will run using the andersenlab/nil-ril-nf docker image. You must have docker installed. You will need to obtain a reference genome to run the alignment with as well. You can use the following command to obtain the reference: curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz Run the pipeline locally with: nextflow run andersenlab/nil-ril-nf -profile local -resume","title":"Running the pipeline locally"},{"location":"pipeline-nil-ril/#debugging_the_pipeline_on_quest","text":"If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run andersenlab/nil-ril-nf -profile quest_debug -resume Note There is no need to clone the git repo before running the pipeline. However, you may still choose to do so if you plan to manually track the git commit used to generate data.","title":"Debugging the pipeline on Quest"},{"location":"pipeline-nil-ril/#running_the_pipeline_on_quest","text":"If running the pipeline on QUEST, you need to load singularity to access the docker container: module load singularity The pipeline can be run on Quest using the following command: nextflow run andersenlab/nil-ril-nf -profile quest -resume","title":"Running the pipeline on Quest"},{"location":"pipeline-nil-ril/#testing","text":"If you are going to modify the pipeline, I highly recommend doing so in a testing environment. The pipeline includes a debug dataset that runs rather quickly (~10 minutes). If you cache results initially and re-run with the -resume option it is fairly easy to add new processes or modify existing ones and still ensure that things are output correctly. Additionally - note that the pipeline is tested everytime a change is made and pushed to github. Testing takes place on travis-ci here , and a badge is visible on the readme indicating the current 'build status'. If the pipeline encounters any errors when being run on travis-ci the 'build' will fail. The command below can be used to test the pipeline locally. # Downloads a pre-indexed reference curl ftp://wormbase.org/pub/wormbase/releases/WS276/species/c_elegans/PRJNA13758/c_elegans.PRJNA13758.WS276.genomc.fa.gz > WS276.fa.gz # Run nextflow nextflow run andersenlab/nil-ril-nf \\ -with-docker andersenlab/nil-ril-nf \\ --debug \\ --reference=WS276.fa.gz \\ -resume Note that the path to the vcf will change slightly in releases later than WI-20170531; See the wi-gatk pipeline for details. The command above will automatically place results in a folder: NIL-N2-CB4856-YYYY-MM-DD","title":"Testing"},{"location":"pipeline-nil-ril/#parameters","text":"","title":"Parameters"},{"location":"pipeline-nil-ril/#--fqs","text":"In order to process NIL/RIL data, you need to move the sequence data to a folder and create a fq_sheet.tsv . This file defines the fastqs that should be processed. The fastq can be specified as relative or absolute . By default, they are expected to be relative to the fastq file. The FASTQ sheet details strain names, ids, library, and files. It should be tab-delimited and look like this: NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz Notice that the file does not include a header. The table with corresponding columns looks like this. strain fastq_pair_id library fastq-1-path fastq-2-path NIL_01 NIL_01_ID S16 NIL_01_1.fq.gz NIL_01_2.fq.gz NIL_02 NIL_02_ID S1 NIL_02_1.fq.gz NIL_02_2.fq.gz The columns are detailed below: strain - The name of the strain. If a strain was sequenced multiple times this file is used to identify that fact and merge those fastq-pairs together following alignment. fastq_pair_id - This must be unique identifier for all individual FASTQ pairs. library - A string identifying the DNA library. If you sequenced a strain from different library preps it can be beneficial when calling variants. The string can be arbitrary (e.g. LIB1) as well if only one library prep was used. fastq-1-path - The relative path of the first fastq. fastq-2-path - The relative path of the second fastq. This file needs to be placed along with the sequence data into a folder. The tree will look like this: NIL_SEQ_DATA/ \u251c\u2500\u2500 NIL_01_1.fq.gz \u251c\u2500\u2500 NIL_01_2.fq.gz \u251c\u2500\u2500 NIL_02_1.fq.gz \u251c\u2500\u2500 NIL_02_2.fq.gz \u2514\u2500\u2500 fq_sheet.tsv Set --fqs as --fqs=/the/path/to/fq_sheet.tsv . Important Do not include the parental strains in the fq_sheet. If you re-sequenced the parent strains and want to include them in the analysis as a control, you need to rename the parent strains to avoid an error in merging the VCFs (i.e. N2 becomes N2-1).","title":"--fqs"},{"location":"pipeline-nil-ril/#--vcf","text":"Before you begin, you will need access to a VCF with high-coverage data from the parental strains. In general, this can be obtained using the latest release of the wild-isolate data which is usually located in the b1059 analysis folder. For example, the most recent C. elegans VCF could be found here: /projects/b1059/data/c_elegans/WI/variation/20210121/vcf/WI.20210121.hard-filter.isotype.vcf.gz This is the hard-filtered VCF, meaning that poor quality variants have been stripped. Use hard-filtered VCFs for this pipeline. Set the parental VCF as --vcf=/the/path/to/WI.20210121.hard-filter.isotype.vcf.gz","title":"--vcf"},{"location":"pipeline-nil-ril/#--reference","text":"A fasta reference indexed with BWA. For example, the C. elegans reference could be found here: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz","title":"--reference"},{"location":"pipeline-nil-ril/#--a_--b_optional","text":"Two parental strains must be provided. By default these are N2 and CB4856. The parental strains provided must be present in the VCF provided. Their genotypes are pulled from that VCF and used to generate the HMM. See below for more details.","title":"--A, --B (optional)"},{"location":"pipeline-nil-ril/#--debug_optional","text":"The pipeline comes pre-packed with fastq's and a VCF that can be used to debug. See the Testing section for more information.","title":"--debug (optional)"},{"location":"pipeline-nil-ril/#--cores_optional","text":"The number of cores to use during alignments and variant calling. Default is 4.","title":"--cores (optional)"},{"location":"pipeline-nil-ril/#--ca_--cb_optional","text":"The color to use for parental strain A and B on plots. Default is orange and blue.","title":"--cA, --cB (optional)"},{"location":"pipeline-nil-ril/#--out_optional","text":"A directory in which to output results. By default it will be NIL-A-B-YYYY-MM-DD where A and be are the parental strains.","title":"--out (optional)"},{"location":"pipeline-nil-ril/#--relative_optional","text":"If you want to specify fastqs using an absolute path use --relative=false . Set to true by default.","title":"--relative (optional)"},{"location":"pipeline-nil-ril/#--cross_obj_optional","text":"If you are running a set of RILs, you might want to add the --cross_obj true parameter. When true , the pipeline will run an additional step to pair down the total genetic variants to only the informative variants and output a smaller genotype matrix to input directly into a new cross object. An example of how to generate a cross object can be found in the bin . There is no need to run this option for NIL data.","title":"--cross_obj (optional)"},{"location":"pipeline-nil-ril/#--tmpdir_optional","text":"A directory for storing temporary data.","title":"--tmpdir (optional)"},{"location":"pipeline-nil-ril/#output","text":"The final output directory looks like this: . \u251c\u2500\u2500 log.txt \u251c\u2500\u2500 fq \u2502 \u251c\u2500\u2500 fq_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 fq_bam_stats.tsv \u2502 \u251c\u2500\u2500 fq_coverage.full.tsv \u2502 \u2514\u2500\u2500 fq_coverage.tsv \u251c\u2500\u2500 SM \u2502 \u251c\u2500\u2500 SM_bam_idxstats.tsv \u2502 \u251c\u2500\u2500 SM_bam_stats.tsv \u2502 \u251c\u2500\u2500 SM_coverage.full.tsv \u2502 \u251c\u2500\u2500 SM_union_vcfs.txt \u2502 \u2514\u2500\u2500 SM_coverage.tsv \u251c\u2500\u2500 hmm \u2502 \u251c\u2500\u2500 gt_hmm.(png/svg) \u2502 \u251c\u2500\u2500 gt_hmm.tsv \u2502 \u251c\u2500\u2500 gt_hmm_fill.tsv \u2502 \u251c\u2500\u2500 NIL.filtered.stats.txt \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz \u2502 \u251c\u2500\u2500 NIL.filtered.vcf.gz.csi \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz \u2502 \u251c\u2500\u2500 NIL.hmm.vcf.gz.csi \u2502 \u2514\u2500\u2500 gt_hmm_genotypes.tsv \u251c\u2500\u2500 bam \u2502 \u2514\u2500\u2500 <BAMS + indices> \u251c\u2500\u2500 duplicates \u2502 \u2514\u2500\u2500 bam_duplicates.tsv \u2514\u2500 sitelist \u251c\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz \u2514\u2500\u2500 N2.CB4856.sitelist.[tsv/vcf].gz.[tbi/csi]","title":"Output"},{"location":"pipeline-nil-ril/#logtxt","text":"A summary of the nextflow run.","title":"log.txt"},{"location":"pipeline-nil-ril/#duplicates","text":"bam_duplicates.tsv - A summary of duplicate reads from aligned bams.","title":"duplicates/"},{"location":"pipeline-nil-ril/#fq","text":"fq_bam_idxstats.tsv - A summary of mapped and unmapped reads by fastq pair. fq_bam_stats.tsv - BAM summary by fastq pair. fq_coverage.full.tsv - Coverage summary by chromosome fq_coverage.tsv - Simple coverage file by fastq","title":"fq/"},{"location":"pipeline-nil-ril/#sm","text":"If you have multiple fastq pairs per sample, their alignments will be combined into a strain or sample-level BAM and the results will be output to this directory. SM_bam_idxstats.tsv - A summary of mapped and unmapped reads by sample. SM_bam_stats.tsv - BAM summary at the sample level SM_coverage.full.tsv - Coverage at the sample level SM_coverage.tsv - Simple coverage at the sample level. SM_union_vcfs.txt - A list of VCFs that were merged to generate RIL.filter.vcf.gz","title":"SM/"},{"location":"pipeline-nil-ril/#hmm","text":"Important gt_hmm_fill.tsv is for visualization purposes only. To determine breakpoints you should use gt_hmm.tsv . The --infill and --endfill options are applied to the gt_hmm_fill.tsv file. You need to be cautious when examining this data as it is generated primarily for visualization purposes . gt_hmm.(png/svg) - Haplotype plot using --infill and --endfill . gt_hmm_fill.tsv - Same as above, but using --infill and --endfill with VCF-Kit. For more information, see VCF-Kit Documentation . This file is used to generate the plots. gt_hmm.tsv - Haplotypes defined by region with associated information. Does not use --infill and --endfill gt_hmm_genotypes.tsv - Long form genotypes file. NIL/RIL.filtered.vcf.gz - A VCF genotypes including the NILs and parental genotypes. NIL/RIL.filtered.stats.txt - Summary of filtered genotypes. Generated by bcftools stats NIL.filtered.vcf.gz NIL/RIL.hmm.vcf.gz - The NIL/RIL VCF as output by VCF-Kit; HMM applied to determine genotypes.","title":"hmm/"},{"location":"pipeline-nil-ril/#plots","text":"coverage_comparison.png - Compares FASTQ and Sample-level coverage. Note that coverage is not simply cumulative. Only uniquely mapped reads count towards coverage, so it is possible that the sample-level coverage will not equal to the cumulative sum of the coverages of individual FASTQ pairs. duplicates.(png/pdf) - Coverage vs. percent duplicated. unmapped_reads.png - Coverage vs. unmapped read percent.","title":"plots/"},{"location":"pipeline-nil-ril/#sitelist","text":"<A>.<B>.sitelist.tsv.gz[+.tbi] - A tabix-indexed list of sites found to be different between both parental strains. <A>.<B>.sitelist.vcf.gz[+.tbi] - A vcf of sites found to be different between both parental strains.","title":"sitelist/"},{"location":"pipeline-nil-ril/#organizing_final_data","text":"After the run is complete and you are satisfied with the results, follow these steps to ensure correct data storage on QUEST: Move the raw fastq files to /projects/b1059/data/{species}/{NIL or RIL}/fastq/ . You might want to use mv -i to ensure no files are overwritten. Move the BAM files from the output folder to /projects/b1059/data/{species}/{NIL or RIL}/alignments/ . You might want to use mv -i to ensure no files are overwritten. Delete the now empty bam folder in the output directory. Move the sample sheet generated for analysis into the output directory. Make sure the output directory follows the default naming structure that is informative about the analysis (i.e. NIL-20200322-N2-CB4856 (if NIL/RIL analysis is performed for another lab, consider adding a -{LabName} like -Baugh to the end of the folder name)). Move the entire output folder to /projects/b1059/data/{species}/{NIL or RIL}/variation .","title":"Organizing final data"},{"location":"pipeline-nil-ril/#adding_nil_sequence_data_to_lab_website","text":"If your sequencing was N2-CB4856 NILs (and maybe other C. elegans NILs as well...?) you probably want to add this sequencing data to the lab website to be accessed by everyone when looking for NIL genotypes. Check out this page for instructions on how to do that. Once done, you should be able to view your NILs on the NIL browser shiny app .","title":"Adding NIL sequence data to lab website"},{"location":"pipeline-overview/","text":"Pipeline Overview \u00b6 Pipeline Overview Wild isolate sequencing An overview of the sequencing pipelines is shown below. Wild isolate data are processed by multiple pipelines. NIL/RIL sequence data are only processed by one pipeline. Wild isolate sequencing \u00b6 Note The full protocol (in development) can be found here","title":"Overview"},{"location":"pipeline-overview/#pipeline_overview","text":"Pipeline Overview Wild isolate sequencing An overview of the sequencing pipelines is shown below. Wild isolate data are processed by multiple pipelines. NIL/RIL sequence data are only processed by one pipeline.","title":"Pipeline Overview"},{"location":"pipeline-overview/#wild_isolate_sequencing","text":"Note The full protocol (in development) can be found here","title":"Wild isolate sequencing"},{"location":"pipeline-postGATK/","text":"post-gatk-nf \u00b6 post-gatk-nf Pipeline overview Software Requirements Usage Testing on Quest Running on Quest Profiles Parameters --debug --sample_sheet --vcf_folder --species (optional) --snv_vcf (pca profile) --pops (pca profile) --eigen_ld (pca) --anc (pca) --output (optional) Output Data storage Cleanup Updating CeNDR Updating NemaScan The post-gatk-nf pipeline performs population genetics analyses (such as identifying shared haplotypes and divergent regions) at the isotype level. The VCFs output from this pipeline are used within the lab and also released to the world via CeNDR. This page details how to run the pipeline. Pipeline overview \u00b6 * * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * ** *** * * * * * * * * *** * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode (optional) --vcf_folder Folder to hard and soft filtered vcf (required) --sample_sheet TSV with column iso-ref strain, bam, bai (no header) (required) --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' c_elegans --output Output folder name. popgen-date (in current folder) Software Requirements \u00b6 The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update Important This pipeline currently only supports analysis on Quest, cannot be run locally Usage \u00b6 Note For more info about running Nextflow pipelines in the Andersen Lab, check out this page Testing on Quest \u00b6 This command uses a test dataset nextflow run andersenlab/post-gatk-nf --debug Running on Quest \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. You should run this in a screen session. Profiles \u00b6 There are now three ways to run this pipeline: -profile standard (default): runs original processes including subseting VCF and divergent and haplotype calls. sample_sheet, vcf_folder, (species) -profile pca : does not run the original post-gatk processes, only the PCA analysis. Note: requires different parameters snv_vcf, species, anc, eigen_ld, pops -profile standard --pca : runs all processes including subseting VCF, divergent and haplotype calls, PCA analysis of isotypes. Requires additional parameters relating to PCA sample_sheet, vcf_folder, species, anc, eigen_ld Note: the -profile standard is optional, just adding the --pca param is enough. nextflow run andersenlab/post-gatk-nf --vcf <path_to_vcf> --sample_sheet <path_to_sample_sheet> Parameters \u00b6 --debug \u00b6 You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run andersenlab/post-gatk-nf --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv --sample_sheet \u00b6 A custom sample sheet can be specified using --sample_sheet . The sample sheet is generated from the sample sheet used as input for wi-gatk-nf with only columns for strain, bam, and bai subsetted. Make sure to remove any strains that you do not want to include in this analysis. ( i.e. subset to keep only ISOTYPE strains ) Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . Important There is no header for the sample sheet! The sample sheet has the following columns: strain - the name of the strain bam - name of the bam alignment file bai - name of the bam alignment index file Note As of 20210501, bam and bam.bai files for all strains of a particular species can be found in one singular location: /projects/b1059/data/{species}/WI/alignments/ so there is no longer need to provide the location of the bam files. --vcf_folder \u00b6 Path to the folder containing both the hard-filtered and soft-filtered vcf outputs from wi-gatk . VCF should contain ALL strains, the first step will be to subset isotype reference strains for further analysis. Note This should be the path to the folder , we want to isotype-subset both hard and soft filtered VCFs. For example: --vcf_folder /projects/b1059/projects/Katie/wi-gatk/WI-20210121/variation/ --species (optional) \u00b6 default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis --snv_vcf (pca profile) \u00b6 File path to SNV-filtered VCF --pops (pca profile) \u00b6 Strain list to filter VCF for PCA analysis. No header: AB1 CB4856 ECA788 Note If you run the standard profile with pca this file will be automatically generated to include all isotypes. --eigen_ld (pca) \u00b6 LD thresholds to test for PCA. Can provide multiple with --eigen_ld 0.8,0.6,0.4 --anc (pca) \u00b6 Ancestor strain to use for PCA. Note Make sure this strain is in your VCF* --output (optional) \u00b6 default - popgen-YYYYMMDD A directory in which to output results. If you have set --debug true , the default output directory will be popgen-YYYYMMDD-debug . Output \u00b6 \u251c\u2500\u2500 ANNOTATE_VCF \u2502 \u251c\u2500\u2500 ANC.bed.gz \u2502 \u251c\u2500\u2500 ANC.bed.gz.tbi \u2502 \u251c\u2500\u2500 Ce330_annotated.vcf.gz | \u2514\u2500\u2500 Ce330_annotated.vcf.tbi \u251c\u2500\u2500 EIGESTRAT \u2502 \u2514\u2500\u2500 LD_{eigen_ld} \u2502 \u251c\u2500\u2500 INPUT_FILES \u2502 \u2502 \u2514\u2500\u2500 * \u2502 \u251c\u2500\u2500 OUTLIER_REMOVAL \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed_relatedness \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed_relatedness.id \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed.evac \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed.eval \u2502 \u2502 \u251c\u2500\u2500 logfile_outlier.txt \u2502 \u2502 \u2514\u2500\u2500 TracyWidom_statistics_outlier_removal.tsv \u2502 \u2514\u2500\u2500 NO_REMOVAL \u2502 \u2514\u2500\u2500 same as outlier_removal \u251c\u2500\u2500 pca_report.html \u251c\u2500\u2500 divergent_regions \u2502 \u251c\u2500\u2500 Mask_DF \u2502 \u2502 \u2514\u2500\u2500 [strain]_Mask_DF.tsv | \u2514\u2500\u2500 divergent_regions_strain.bed \u251c\u2500\u2500 haplotype \u2502 \u251c\u2500\u2500 haplotype_length.pdf \u2502 \u251c\u2500\u2500 sweep_summary.tsv \u2502 \u251c\u2500\u2500 max_haplotype_genome_wide.pdf \u2502 \u251c\u2500\u2500 haplotype.pdf \u2502 \u251c\u2500\u2500 haplotype.tsv \u2502 \u251c\u2500\u2500 [chr].ibd \u2502 \u2514\u2500\u2500 haplotype_plot_df.Rda \u251c\u2500\u2500 tree \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.min4.tree \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.min4.tree.pdf \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.min4.tree \u2502 \u2514\u2500\u2500 WI.{date}.hard-filter.min4.tree.pdf \u251c\u2500\u2500 NemaScan \u2502 \u251c\u2500\u2500 strain_isotype_lookup.tsv \u2502 \u251c\u2500\u2500 div_isotype_list.txt \u2502 \u251c\u2500\u2500 haplotype_df_isotype.bed \u2502 \u251c\u2500\u2500 divergent_bins.bed \u2502 \u2514\u2500\u2500 divergent_df_isotype.bed \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.{date}.small.hard-filter.isotype.vcf.gz \u251c\u2500\u2500 WI.{date}.small.hard-filter.isotype.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.SNV.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.SNV.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.soft-filter.isotype.vcf.gz \u251c\u2500\u2500 WI.{date}.soft-filter.isotype.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz \u2514\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz.tbi Data storage \u00b6 Cleanup \u00b6 Once the pipeline has complete successfully and you are satisfied with the results, the final data can be moved to their final storage place on Quest accordingly: Everything in the haplotype foder can be moved to /projects/b1059/data/{species}/WI/haplotype/{date} Everything in the divergent_regions folder can be moved to /projects/b1059/data/{species}/WI/divergent_regions/{date} Everything in the tree folder can be moved to /projects/b1059/data/{species}/WI/tree/{date} Everything in the variation folder can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf Everything in the NemaScan folder can replace the old verions in NemaScan when ready NemaScan/input_data/{species}/isotypes Everything in the EIGENSTRAT folder can be moved to /projects/b1059/data/{species}/WI/pca/{date} Updating CeNDR \u00b6 Check out the CeNDR page for more information about updating a new data release for CeNDR. Updating NemaScan \u00b6 Once a new CeNDR release is ready, it is important to also update the genome-wide association mapping packages to ensure users can appropriately analyze data from new strains as well as old strains. Here is a list of things that need to be updated: The default vcf should be changed to the newest release date (i.e. from 20200815 to 20210121). Users will still have the option to use an earlier vcf. Everything in the NemaScan folder can replace the old verions in NemaScan when ready NemaScan/input_data/{species}/isotypes Be sure that the small-vcf is stored in the proper file location to be accessed, both on QUEST and on GCP for CeNDR/local users. Note Although users will have the option to use an older vcf, the divergent region data will always be pulled from the most recent release. There could be minor changes from release to release. If this is a concern, we could switch to pulling the divergent data directly from b1059 instead of including it in the bin/ of the mapping pipeline.","title":"post-gatk-nf"},{"location":"pipeline-postGATK/#post-gatk-nf","text":"post-gatk-nf Pipeline overview Software Requirements Usage Testing on Quest Running on Quest Profiles Parameters --debug --sample_sheet --vcf_folder --species (optional) --snv_vcf (pca profile) --pops (pca profile) --eigen_ld (pca) --anc (pca) --output (optional) Output Data storage Cleanup Updating CeNDR Updating NemaScan The post-gatk-nf pipeline performs population genetics analyses (such as identifying shared haplotypes and divergent regions) at the isotype level. The VCFs output from this pipeline are used within the lab and also released to the world via CeNDR. This page details how to run the pipeline.","title":"post-gatk-nf"},{"location":"pipeline-postGATK/#pipeline_overview","text":"* * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * ** *** * * * * * * * * *** * * * * * * * * * * * * ** * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ** * * * * * * * parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode (optional) --vcf_folder Folder to hard and soft filtered vcf (required) --sample_sheet TSV with column iso-ref strain, bam, bai (no header) (required) --species Species: 'c_elegans', 'c_tropicalis' or 'c_briggsae' c_elegans --output Output folder name. popgen-date (in current folder)","title":"Pipeline overview"},{"location":"pipeline-postGATK/#software_requirements","text":"The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update Important This pipeline currently only supports analysis on Quest, cannot be run locally","title":"Software Requirements"},{"location":"pipeline-postGATK/#usage","text":"Note For more info about running Nextflow pipelines in the Andersen Lab, check out this page","title":"Usage"},{"location":"pipeline-postGATK/#testing_on_quest","text":"This command uses a test dataset nextflow run andersenlab/post-gatk-nf --debug","title":"Testing on Quest"},{"location":"pipeline-postGATK/#running_on_quest","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. You should run this in a screen session.","title":"Running on Quest"},{"location":"pipeline-postGATK/#profiles","text":"There are now three ways to run this pipeline: -profile standard (default): runs original processes including subseting VCF and divergent and haplotype calls. sample_sheet, vcf_folder, (species) -profile pca : does not run the original post-gatk processes, only the PCA analysis. Note: requires different parameters snv_vcf, species, anc, eigen_ld, pops -profile standard --pca : runs all processes including subseting VCF, divergent and haplotype calls, PCA analysis of isotypes. Requires additional parameters relating to PCA sample_sheet, vcf_folder, species, anc, eigen_ld Note: the -profile standard is optional, just adding the --pca param is enough. nextflow run andersenlab/post-gatk-nf --vcf <path_to_vcf> --sample_sheet <path_to_sample_sheet>","title":"Profiles"},{"location":"pipeline-postGATK/#parameters","text":"","title":"Parameters"},{"location":"pipeline-postGATK/#--debug","text":"You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data folder). For example: nextflow run andersenlab/post-gatk-nf --debug -resume Using --debug will automatically set the sample sheet to test_data/sample_sheet.tsv","title":"--debug"},{"location":"pipeline-postGATK/#--sample_sheet","text":"A custom sample sheet can be specified using --sample_sheet . The sample sheet is generated from the sample sheet used as input for wi-gatk-nf with only columns for strain, bam, and bai subsetted. Make sure to remove any strains that you do not want to include in this analysis. ( i.e. subset to keep only ISOTYPE strains ) Remember that in --debug mode the pipeline will use the sample sheet located in test_data/sample_sheet.tsv . Important There is no header for the sample sheet! The sample sheet has the following columns: strain - the name of the strain bam - name of the bam alignment file bai - name of the bam alignment index file Note As of 20210501, bam and bam.bai files for all strains of a particular species can be found in one singular location: /projects/b1059/data/{species}/WI/alignments/ so there is no longer need to provide the location of the bam files.","title":"--sample_sheet"},{"location":"pipeline-postGATK/#--vcf_folder","text":"Path to the folder containing both the hard-filtered and soft-filtered vcf outputs from wi-gatk . VCF should contain ALL strains, the first step will be to subset isotype reference strains for further analysis. Note This should be the path to the folder , we want to isotype-subset both hard and soft filtered VCFs. For example: --vcf_folder /projects/b1059/projects/Katie/wi-gatk/WI-20210121/variation/","title":"--vcf_folder"},{"location":"pipeline-postGATK/#--species_optional","text":"default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis","title":"--species (optional)"},{"location":"pipeline-postGATK/#--snv_vcf_pca_profile","text":"File path to SNV-filtered VCF","title":"--snv_vcf (pca profile)"},{"location":"pipeline-postGATK/#--pops_pca_profile","text":"Strain list to filter VCF for PCA analysis. No header: AB1 CB4856 ECA788 Note If you run the standard profile with pca this file will be automatically generated to include all isotypes.","title":"--pops (pca profile)"},{"location":"pipeline-postGATK/#--eigen_ld_pca","text":"LD thresholds to test for PCA. Can provide multiple with --eigen_ld 0.8,0.6,0.4","title":"--eigen_ld (pca)"},{"location":"pipeline-postGATK/#--anc_pca","text":"Ancestor strain to use for PCA. Note Make sure this strain is in your VCF*","title":"--anc (pca)"},{"location":"pipeline-postGATK/#--output_optional","text":"default - popgen-YYYYMMDD A directory in which to output results. If you have set --debug true , the default output directory will be popgen-YYYYMMDD-debug .","title":"--output (optional)"},{"location":"pipeline-postGATK/#output","text":"\u251c\u2500\u2500 ANNOTATE_VCF \u2502 \u251c\u2500\u2500 ANC.bed.gz \u2502 \u251c\u2500\u2500 ANC.bed.gz.tbi \u2502 \u251c\u2500\u2500 Ce330_annotated.vcf.gz | \u2514\u2500\u2500 Ce330_annotated.vcf.tbi \u251c\u2500\u2500 EIGESTRAT \u2502 \u2514\u2500\u2500 LD_{eigen_ld} \u2502 \u251c\u2500\u2500 INPUT_FILES \u2502 \u2502 \u2514\u2500\u2500 * \u2502 \u251c\u2500\u2500 OUTLIER_REMOVAL \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed_relatedness \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed_relatedness.id \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed.evac \u2502 \u2502 \u251c\u2500\u2500 eigenstrat_outliers_removed.eval \u2502 \u2502 \u251c\u2500\u2500 logfile_outlier.txt \u2502 \u2502 \u2514\u2500\u2500 TracyWidom_statistics_outlier_removal.tsv \u2502 \u2514\u2500\u2500 NO_REMOVAL \u2502 \u2514\u2500\u2500 same as outlier_removal \u251c\u2500\u2500 pca_report.html \u251c\u2500\u2500 divergent_regions \u2502 \u251c\u2500\u2500 Mask_DF \u2502 \u2502 \u2514\u2500\u2500 [strain]_Mask_DF.tsv | \u2514\u2500\u2500 divergent_regions_strain.bed \u251c\u2500\u2500 haplotype \u2502 \u251c\u2500\u2500 haplotype_length.pdf \u2502 \u251c\u2500\u2500 sweep_summary.tsv \u2502 \u251c\u2500\u2500 max_haplotype_genome_wide.pdf \u2502 \u251c\u2500\u2500 haplotype.pdf \u2502 \u251c\u2500\u2500 haplotype.tsv \u2502 \u251c\u2500\u2500 [chr].ibd \u2502 \u2514\u2500\u2500 haplotype_plot_df.Rda \u251c\u2500\u2500 tree \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.min4.tree \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.min4.tree.pdf \u2502 \u251c\u2500\u2500 WI.{date}.hard-filter.min4.tree \u2502 \u2514\u2500\u2500 WI.{date}.hard-filter.min4.tree.pdf \u251c\u2500\u2500 NemaScan \u2502 \u251c\u2500\u2500 strain_isotype_lookup.tsv \u2502 \u251c\u2500\u2500 div_isotype_list.txt \u2502 \u251c\u2500\u2500 haplotype_df_isotype.bed \u2502 \u251c\u2500\u2500 divergent_bins.bed \u2502 \u2514\u2500\u2500 divergent_df_isotype.bed \u2514\u2500\u2500 variation \u251c\u2500\u2500 WI.{date}.small.hard-filter.isotype.vcf.gz \u251c\u2500\u2500 WI.{date}.small.hard-filter.isotype.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.SNV.vcf.gz \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.SNV.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.soft-filter.isotype.vcf.gz \u251c\u2500\u2500 WI.{date}.soft-filter.isotype.vcf.gz.tbi \u251c\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz \u2514\u2500\u2500 WI.{date}.hard-filter.isotype.vcf.gz.tbi","title":"Output"},{"location":"pipeline-postGATK/#data_storage","text":"","title":"Data storage"},{"location":"pipeline-postGATK/#cleanup","text":"Once the pipeline has complete successfully and you are satisfied with the results, the final data can be moved to their final storage place on Quest accordingly: Everything in the haplotype foder can be moved to /projects/b1059/data/{species}/WI/haplotype/{date} Everything in the divergent_regions folder can be moved to /projects/b1059/data/{species}/WI/divergent_regions/{date} Everything in the tree folder can be moved to /projects/b1059/data/{species}/WI/tree/{date} Everything in the variation folder can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf Everything in the NemaScan folder can replace the old verions in NemaScan when ready NemaScan/input_data/{species}/isotypes Everything in the EIGENSTRAT folder can be moved to /projects/b1059/data/{species}/WI/pca/{date}","title":"Cleanup"},{"location":"pipeline-postGATK/#updating_cendr","text":"Check out the CeNDR page for more information about updating a new data release for CeNDR.","title":"Updating CeNDR"},{"location":"pipeline-postGATK/#updating_nemascan","text":"Once a new CeNDR release is ready, it is important to also update the genome-wide association mapping packages to ensure users can appropriately analyze data from new strains as well as old strains. Here is a list of things that need to be updated: The default vcf should be changed to the newest release date (i.e. from 20200815 to 20210121). Users will still have the option to use an earlier vcf. Everything in the NemaScan folder can replace the old verions in NemaScan when ready NemaScan/input_data/{species}/isotypes Be sure that the small-vcf is stored in the proper file location to be accessed, both on QUEST and on GCP for CeNDR/local users. Note Although users will have the option to use an older vcf, the divergent region data will always be pulled from the most recent release. There could be minor changes from release to release. If this is a concern, we could switch to pulling the divergent data directly from b1059 instead of including it in the bin/ of the mapping pipeline.","title":"Updating NemaScan"},{"location":"pipeline-trimming/","text":"trim-fq-nf \u00b6 trim-fq-nf Pipeline overview Software requirements Usage Testing the pipeline on QUEST Running the pipeline on QUEST Profiles -profile standard (Default) -profile trim_only -profile sp_check_only Parameters --debug --fastq_folder --raw_path (optional) --processed_path (optional) --genome_sheet (optional) --out (optional) --subsample_read_count (optional) Output Data storage Backup Poor quality data Cleanup The trim-fq-nf workflow performs FASTQ trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. You should not use the trim-fq-nf workflow on low-coverage NIL or RIL data. Recent updates in 2021 also include running a species check on the FASTQ and generating a sample sheet of high-quality, species-confirmed samples for alignment . Pipeline overview \u00b6 ____ .__ _____ _____ _/ |_ _______ |__| _____ _/ ____\\\\ ______ ____ _/ ____\\\\ \\\\ __\\\\\\\\_ __ \\\\| | / \\\\ ______ \\\\ __\\\\ / ____/ ______ / \\\\ \\\\ __\\\\ | | | | \\\\/| || Y Y \\\\ /_____/ | | < <_| | /_____/ | | \\\\ | | |__| |__| |__||__|_| / |__| \\\\__ | |___| / |__| \\\\/ |__| \\\\/ parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode ${params.debug} --fastq_folder Name of the raw fastq folder ${params.fastq_folder} --raw_path Path to raw fastq folder ${params.raw_path} --processed_path Path to processed fastq folder (output) ${params.processed_path} --genome_sheet File with fasta locations for species check ${params.genome_sheet} --out Folder name to write results ${params.out} --subsample_read_count How many reads to use for species check ${params.subsample_read_count} You have downloaded FASTQ Data to a subdirectory within a raw directory. For wild isolates this will be /projects/b1059/data/transfer/raw/<folder_name> FASTQs must end in a .fq.gz extension for the pipeline to work. You have modified FASTQ names if necessary to add strain names or other identifying information. You have installed software-requirements (see below for more info) Software requirements \u00b6 Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Currently only runs on Quest with conda environments installed at /projects/b1059/software/conda_envs/ Note All FASTQs should end with a _R1_001.fastq.gz or a _R2_001.fastq.gz . You can rename FASTQs using the rename command: rename --dry-run --subst .fq.gz .fastq.gz --subst _1 _R1_001 --subst _2 _R2_001 *.fq.gz The --dry-run flag will output how files will be renamed. Review the output and remove the flag when you are ready. Usage \u00b6 Testing the pipeline on QUEST \u00b6 This command uses a test dataset nextflow run andersenlab/trim-fq-nf --debug Running the pipeline on QUEST \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. nextflow run andersenlab/trim-fq-nf --fastq_folder <name_of_folder> Important The pipeline expects the folder containing raw fastq files to be located at /projects/b1059/data/transfer/raw/ . And all processed fastq files will be output to /projects/b1059/data/transfer/processed/ Profiles \u00b6 -profile standard (Default) \u00b6 If no profile is designated, the default profile will run both fastq trimming AND species check -profile trim_only \u00b6 Use this profile to only trim fastq files and not perform species check. -profile sp_check_only \u00b6 Use this profile to only run species check and not fastq trimming. This is useful for running species checks on previously trimmed fastqs. Parameters \u00b6 --debug \u00b6 You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data/raw folder). For example: nextflow run andersenlab/trim-fq-nf --debug -resume Using --debug will automatically set the fastq_folder to test_data/raw/20210406_test1 --fastq_folder \u00b6 This should be the name of the folder containing all fastq files located at /projects/b1059/data/transfer/raw/ . As long as there are no overlapping file names (be sure to check this first), you can combine multiple pools sequenced at the same time into one larger folder at this step. --raw_path (optional) \u00b6 The path to the fastq_folder if not default ( /projects/b1059/data/transfer/raw/ ) --processed_path (optional) \u00b6 The path to output folder if not default ( /projects/b1059/data/transfer/processed/ ) --genome_sheet (optional) \u00b6 Path to a tsv file listing project IDs for species. Default is located in bin/genome_sheet.tsv --out (optional) \u00b6 Name of output folder with results. Default is \"processFQ-{fastq_folder}\" --subsample_read_count (optional) \u00b6 How many reads to use for species check. Default = 10,000 Output \u00b6 \u251c\u2500\u2500 b1059/data/transfer/processed/ \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}.fq.gz - - - - - - - - - - - - - - - - - - - - - - - - - - - - \u251c\u2500\u2500 multi_QC \u2502 \u251c\u2500\u2500 multiqc_data \u2502 \u2502 \u251c\u2500\u2500 *.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_data.json \u2502 \u251c\u2500\u2500 multiqc_report.html \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}_fastp.html \u251c\u2500\u2500 multiqc_data \u2502 \u2514\u2500\u2500 multiqc_samtools_stats.txt \u251c\u2500\u2500 sample_sheet \u2502 \u251c\u2500\u2500 sample_sheet_{species}_{date}_ALL.tsv \u2502 \u2514\u2500\u2500 sample_sheet_{species}_{date}_NEW.tsv \u251c\u2500\u2500 species_check \u2502 \u251c\u2500\u2500 species_check_{fastq_folder}.html \u2502 \u251c\u2500\u2500 {library}_multiple_librarires.tsv \u2502 \u251c\u2500\u2500 {library}_strains_most_likely_species.tsv \u2502 \u251c\u2500\u2500 {library}_strains_not_in_master_sheet.tsv \u2502 \u251c\u2500\u2500 {library}_strains_possibly_diff_species.tsv \u2502 \u2514\u2500\u2500 WI_all_{date}.tsv \u251c\u2500\u2500 sample_sheet_{fastq_folder}_all_temp.tsv \u2514\u2500\u2500 log.txt The resulting trimmed FASTQs will be output in the b1059/data/transfer/processed directory. The rest of the output files and reports will be generated in a new folder in the directory in which you ran the nextflow pipeline, labeled by processFQ-{fastq_folder} . MultiQC multi_QC/{strain}_{library}_fastp.html - fastp html report detailing trimming and quality multi_QC/multiqc_report.html - aggregate multi QC report for all strains pre and post trimming multi_QC/multiqc_data/*.txt or .json - files used to make previous reports Species check If a species check is run, the multiqc_data/multiqc_samtools_stats.txt will contain the results of reads mapped to each species. Furthermore, several reports and sample sheets will be generated: species_check/species_check_{date}_{library}.html is an HTML report showing how many strains have issues (not in master sheet, possibly different species, etc.) species_check/{library}_multiple_libraries.tsv - strains sequenced in multiple libraries species_check/{library}_strains_most_likely_species.tsv - list of all strains in library labeled by (1) the species in record and (2) most likely species by sequencing species_check/{library}_strains_not_in_master.tsv - list of strains not found in Robyn's wild isolate master sheets for CE, CB, or CT species_check/{library}_strains_possibly_diff_species.tsv - list of strains whose species in record does not match the likely species by sequencing species_check/WI_all_{date}.tsv - copy of all strains (CE, CB, and CT) and species designation in record sample_sheet_{date}_{library}_all_temp.tsv - temporary sample sheet with all strains for all species combined. DO NOT USE THIS FOR ALIGNMENT. Sample sheets If a species check is run, the species_check/sample_sheet folder will also contain 6 sample sheets to be used for alignment: sample_sheet/sample_sheet_{species}_{date}_ALL.tsv - sample sheet for alignment-nf using ALL strains of a particular species (i.e. c_elegans). This is useful for species we have not performed any alignments for or when we update the reference genome and need to re-align all strains. sample_sheet/sample_sheet_{species}_{date}_NEW.tsv - sample sheet for alignment-nf using all fastq from any library for ONLY strains sequenced in this particular library of a particular species (i.e. c_elegans, RET63). This is useful when the reference genome does not change and there is no need to re-align thousands of strains to save on computational power. Note The \"new\" sample sheet will still contain old fastq sequenced in a previous pool (i.e. RET55) if that strain was re-sequenced in the current pool (i.e. RET63). After running alignment-nf , this will create a new BAM file incorporating all fastq for that strain. Data storage \u00b6 Backup \u00b6 Once you have completed the trim-fq-nf pipeline you should backup the raw FASTQs. More information on this is available in the backup Poor quality data \u00b6 If you observe poor quality sequence data you should notify Robyn through the appropriate channels and then remove the data from further analysis. Cleanup \u00b6 If you have triple-checked everything and are satisfied with the results, the original raw sequence data can be deleted. The processed sequence data (FASTQ files) should be moved to their appropriate location, split by species ( /projects/b1059/data/{species}/WI/fastq/dna/ ). The following line can be used to move processed fastq prior to running alignment-nf : # change directories into the folder containing the processed fastq files cd /projects/b1059/data/transfer/processed/20210510_RET63/ # move files one species at a time (might be a more efficient line of code for this, but it works...) # !!!! make sure to change the file name !!!!! # file name ~ - ~ CHANGE THIS ~ - ~ file='/projects/b1059/Katie/trim-fq-nf/20210510_RET63/species_check/sample_sheet/sample_sheet_c_tropicalis_20201222a_NEW.tsv' # species sp=\"c_`echo $file | xargs -n1 basename | awk -F[__] '{print $4}'`\" # get list of files to move from file awk NR\\>1 $file > temp.tsv cat temp.tsv | awk '{print $4}' > files_to_move.txt cat temp.tsv | awk '{print $5}' >> files_to_move.txt # move files cat files_to_move.txt | while read line do mv $line /projects/b1059/data/$sp/WI/fastq/dna/ done # remove temp file rm files_to_move.txt rm temp.tsv Note The sample sheets ONLY contain strains that species in record matches most likely species by sequencing. If, after moving all the FASTQ for each species to their proper folder, you have FASTQ remaining, these are likely to be found in strains_possibly_diff_species.tsv . You should notify Robyn and Erik about these strains through the appropriate channels and delete the FASTQ or move to another temporary location until it can be re-sequenced.","title":"trim-fq-nf"},{"location":"pipeline-trimming/#trim-fq-nf","text":"trim-fq-nf Pipeline overview Software requirements Usage Testing the pipeline on QUEST Running the pipeline on QUEST Profiles -profile standard (Default) -profile trim_only -profile sp_check_only Parameters --debug --fastq_folder --raw_path (optional) --processed_path (optional) --genome_sheet (optional) --out (optional) --subsample_read_count (optional) Output Data storage Backup Poor quality data Cleanup The trim-fq-nf workflow performs FASTQ trimming to remove poor quality sequences and technical sequences such as adapters. It should be used with high-coverage genomic DNA. You should not use the trim-fq-nf workflow on low-coverage NIL or RIL data. Recent updates in 2021 also include running a species check on the FASTQ and generating a sample sheet of high-quality, species-confirmed samples for alignment .","title":"trim-fq-nf"},{"location":"pipeline-trimming/#pipeline_overview","text":"____ .__ _____ _____ _/ |_ _______ |__| _____ _/ ____\\\\ ______ ____ _/ ____\\\\ \\\\ __\\\\\\\\_ __ \\\\| | / \\\\ ______ \\\\ __\\\\ / ____/ ______ / \\\\ \\\\ __\\\\ | | | | \\\\/| || Y Y \\\\ /_____/ | | < <_| | /_____/ | | \\\\ | | |__| |__| |__||__|_| / |__| \\\\__ | |___| / |__| \\\\/ |__| \\\\/ parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode ${params.debug} --fastq_folder Name of the raw fastq folder ${params.fastq_folder} --raw_path Path to raw fastq folder ${params.raw_path} --processed_path Path to processed fastq folder (output) ${params.processed_path} --genome_sheet File with fasta locations for species check ${params.genome_sheet} --out Folder name to write results ${params.out} --subsample_read_count How many reads to use for species check ${params.subsample_read_count} You have downloaded FASTQ Data to a subdirectory within a raw directory. For wild isolates this will be /projects/b1059/data/transfer/raw/<folder_name> FASTQs must end in a .fq.gz extension for the pipeline to work. You have modified FASTQ names if necessary to add strain names or other identifying information. You have installed software-requirements (see below for more info)","title":"Pipeline overview"},{"location":"pipeline-trimming/#software_requirements","text":"Nextflow v20.01+ (see the dry guide on Nextflow here or the Nextflow documentation here ). On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Currently only runs on Quest with conda environments installed at /projects/b1059/software/conda_envs/ Note All FASTQs should end with a _R1_001.fastq.gz or a _R2_001.fastq.gz . You can rename FASTQs using the rename command: rename --dry-run --subst .fq.gz .fastq.gz --subst _1 _R1_001 --subst _2 _R2_001 *.fq.gz The --dry-run flag will output how files will be renamed. Review the output and remove the flag when you are ready.","title":"Software requirements"},{"location":"pipeline-trimming/#usage","text":"","title":"Usage"},{"location":"pipeline-trimming/#testing_the_pipeline_on_quest","text":"This command uses a test dataset nextflow run andersenlab/trim-fq-nf --debug","title":"Testing the pipeline on QUEST"},{"location":"pipeline-trimming/#running_the_pipeline_on_quest","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. nextflow run andersenlab/trim-fq-nf --fastq_folder <name_of_folder> Important The pipeline expects the folder containing raw fastq files to be located at /projects/b1059/data/transfer/raw/ . And all processed fastq files will be output to /projects/b1059/data/transfer/processed/","title":"Running the pipeline on QUEST"},{"location":"pipeline-trimming/#profiles","text":"","title":"Profiles"},{"location":"pipeline-trimming/#-profile_standard_default","text":"If no profile is designated, the default profile will run both fastq trimming AND species check","title":"-profile standard (Default)"},{"location":"pipeline-trimming/#-profile_trim_only","text":"Use this profile to only trim fastq files and not perform species check.","title":"-profile trim_only"},{"location":"pipeline-trimming/#-profile_sp_check_only","text":"Use this profile to only run species check and not fastq trimming. This is useful for running species checks on previously trimmed fastqs.","title":"-profile sp_check_only"},{"location":"pipeline-trimming/#parameters","text":"","title":"Parameters"},{"location":"pipeline-trimming/#--debug","text":"You should use --debug true for testing/debugging purposes. This will run the debug test set (located in the test_data/raw folder). For example: nextflow run andersenlab/trim-fq-nf --debug -resume Using --debug will automatically set the fastq_folder to test_data/raw/20210406_test1","title":"--debug"},{"location":"pipeline-trimming/#--fastq_folder","text":"This should be the name of the folder containing all fastq files located at /projects/b1059/data/transfer/raw/ . As long as there are no overlapping file names (be sure to check this first), you can combine multiple pools sequenced at the same time into one larger folder at this step.","title":"--fastq_folder"},{"location":"pipeline-trimming/#--raw_path_optional","text":"The path to the fastq_folder if not default ( /projects/b1059/data/transfer/raw/ )","title":"--raw_path (optional)"},{"location":"pipeline-trimming/#--processed_path_optional","text":"The path to output folder if not default ( /projects/b1059/data/transfer/processed/ )","title":"--processed_path (optional)"},{"location":"pipeline-trimming/#--genome_sheet_optional","text":"Path to a tsv file listing project IDs for species. Default is located in bin/genome_sheet.tsv","title":"--genome_sheet (optional)"},{"location":"pipeline-trimming/#--out_optional","text":"Name of output folder with results. Default is \"processFQ-{fastq_folder}\"","title":"--out (optional)"},{"location":"pipeline-trimming/#--subsample_read_count_optional","text":"How many reads to use for species check. Default = 10,000","title":"--subsample_read_count (optional)"},{"location":"pipeline-trimming/#output","text":"\u251c\u2500\u2500 b1059/data/transfer/processed/ \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}.fq.gz - - - - - - - - - - - - - - - - - - - - - - - - - - - - \u251c\u2500\u2500 multi_QC \u2502 \u251c\u2500\u2500 multiqc_data \u2502 \u2502 \u251c\u2500\u2500 *.txt \u2502 \u2502 \u2514\u2500\u2500 multiqc_data.json \u2502 \u251c\u2500\u2500 multiqc_report.html \u2502 \u2514\u2500\u2500 {strain}_{library}_{read}_fastp.html \u251c\u2500\u2500 multiqc_data \u2502 \u2514\u2500\u2500 multiqc_samtools_stats.txt \u251c\u2500\u2500 sample_sheet \u2502 \u251c\u2500\u2500 sample_sheet_{species}_{date}_ALL.tsv \u2502 \u2514\u2500\u2500 sample_sheet_{species}_{date}_NEW.tsv \u251c\u2500\u2500 species_check \u2502 \u251c\u2500\u2500 species_check_{fastq_folder}.html \u2502 \u251c\u2500\u2500 {library}_multiple_librarires.tsv \u2502 \u251c\u2500\u2500 {library}_strains_most_likely_species.tsv \u2502 \u251c\u2500\u2500 {library}_strains_not_in_master_sheet.tsv \u2502 \u251c\u2500\u2500 {library}_strains_possibly_diff_species.tsv \u2502 \u2514\u2500\u2500 WI_all_{date}.tsv \u251c\u2500\u2500 sample_sheet_{fastq_folder}_all_temp.tsv \u2514\u2500\u2500 log.txt The resulting trimmed FASTQs will be output in the b1059/data/transfer/processed directory. The rest of the output files and reports will be generated in a new folder in the directory in which you ran the nextflow pipeline, labeled by processFQ-{fastq_folder} . MultiQC multi_QC/{strain}_{library}_fastp.html - fastp html report detailing trimming and quality multi_QC/multiqc_report.html - aggregate multi QC report for all strains pre and post trimming multi_QC/multiqc_data/*.txt or .json - files used to make previous reports Species check If a species check is run, the multiqc_data/multiqc_samtools_stats.txt will contain the results of reads mapped to each species. Furthermore, several reports and sample sheets will be generated: species_check/species_check_{date}_{library}.html is an HTML report showing how many strains have issues (not in master sheet, possibly different species, etc.) species_check/{library}_multiple_libraries.tsv - strains sequenced in multiple libraries species_check/{library}_strains_most_likely_species.tsv - list of all strains in library labeled by (1) the species in record and (2) most likely species by sequencing species_check/{library}_strains_not_in_master.tsv - list of strains not found in Robyn's wild isolate master sheets for CE, CB, or CT species_check/{library}_strains_possibly_diff_species.tsv - list of strains whose species in record does not match the likely species by sequencing species_check/WI_all_{date}.tsv - copy of all strains (CE, CB, and CT) and species designation in record sample_sheet_{date}_{library}_all_temp.tsv - temporary sample sheet with all strains for all species combined. DO NOT USE THIS FOR ALIGNMENT. Sample sheets If a species check is run, the species_check/sample_sheet folder will also contain 6 sample sheets to be used for alignment: sample_sheet/sample_sheet_{species}_{date}_ALL.tsv - sample sheet for alignment-nf using ALL strains of a particular species (i.e. c_elegans). This is useful for species we have not performed any alignments for or when we update the reference genome and need to re-align all strains. sample_sheet/sample_sheet_{species}_{date}_NEW.tsv - sample sheet for alignment-nf using all fastq from any library for ONLY strains sequenced in this particular library of a particular species (i.e. c_elegans, RET63). This is useful when the reference genome does not change and there is no need to re-align thousands of strains to save on computational power. Note The \"new\" sample sheet will still contain old fastq sequenced in a previous pool (i.e. RET55) if that strain was re-sequenced in the current pool (i.e. RET63). After running alignment-nf , this will create a new BAM file incorporating all fastq for that strain.","title":"Output"},{"location":"pipeline-trimming/#data_storage","text":"","title":"Data storage"},{"location":"pipeline-trimming/#backup","text":"Once you have completed the trim-fq-nf pipeline you should backup the raw FASTQs. More information on this is available in the backup","title":"Backup"},{"location":"pipeline-trimming/#poor_quality_data","text":"If you observe poor quality sequence data you should notify Robyn through the appropriate channels and then remove the data from further analysis.","title":"Poor quality data"},{"location":"pipeline-trimming/#cleanup","text":"If you have triple-checked everything and are satisfied with the results, the original raw sequence data can be deleted. The processed sequence data (FASTQ files) should be moved to their appropriate location, split by species ( /projects/b1059/data/{species}/WI/fastq/dna/ ). The following line can be used to move processed fastq prior to running alignment-nf : # change directories into the folder containing the processed fastq files cd /projects/b1059/data/transfer/processed/20210510_RET63/ # move files one species at a time (might be a more efficient line of code for this, but it works...) # !!!! make sure to change the file name !!!!! # file name ~ - ~ CHANGE THIS ~ - ~ file='/projects/b1059/Katie/trim-fq-nf/20210510_RET63/species_check/sample_sheet/sample_sheet_c_tropicalis_20201222a_NEW.tsv' # species sp=\"c_`echo $file | xargs -n1 basename | awk -F[__] '{print $4}'`\" # get list of files to move from file awk NR\\>1 $file > temp.tsv cat temp.tsv | awk '{print $4}' > files_to_move.txt cat temp.tsv | awk '{print $5}' >> files_to_move.txt # move files cat files_to_move.txt | while read line do mv $line /projects/b1059/data/$sp/WI/fastq/dna/ done # remove temp file rm files_to_move.txt rm temp.tsv Note The sample sheets ONLY contain strains that species in record matches most likely species by sequencing. If, after moving all the FASTQ for each species to their proper folder, you have FASTQ remaining, these are likely to be found in strains_possibly_diff_species.tsv . You should notify Robyn and Erik about these strains through the appropriate channels and delete the FASTQ or move to another temporary location until it can be re-sequenced.","title":"Cleanup"},{"location":"pipeline-wiGATK/","text":"wi-gatk \u00b6 wi-gatk Pipeline Overview Software Requirements Usage Profiles Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Parameters --sample_sheet --bam_location (optional) --species (optional) --project (optional) --ws_build (optional) --reference (optional) --mito_name (optional) --output (optional) Output Data Storage Cleanup The wi-gatk pipeline filters and calls variants from wild isolate sequence data. Pipeline Overview \u00b6 _______ _______ _______ __ __ _______ _______ | __| _ |_ _| |/ | | | | ___| | | | | | | | < | | ___| |_______|___|___| |___| |__|\\\\__| |__|____|___| parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --output Release Directory WI-{date} --sample_sheet Sample sheet null --bam_location Directory of bam files /projects/b1059/data/{species}/WI/alignments/ --mito_name Contig not to polarize hetero sites MtDNA Reference Genome --------------- --reference_base Location of ref genomes /projects/b1059/data/{species}/genomes/ --species/project/build These 4 params form --reference {species} / {project} / {ws_build} Variant Filters --------------- --min_depth Minimum variant depth 5 --qual Variant QUAL score 30 --strand_odds_ratio SOR_strand_odds_ratio 5 --quality_by_depth QD_quality_by_depth 20 --fisherstrand FS_fisher_strand 100 --high_missing Max % missing genotypes 0.95 --high_heterozygosity Max % max heterozygosity 0.10 Software Requirements \u00b6 The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update This pipeline also uses a docker image andersenlab/gatk4 to manage software and reproducibility. To access this docker image, first load the singularity module on QUEST. module load singularity Usage \u00b6 Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page. Profiles \u00b6 The nextflow.config file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. Note If you forget to add a -profile , the quest profile will be chosen as default Running the pipeline locally \u00b6 When running locally, the pipeline will run using the andersenlab/gatk4 docker image. You must have docker installed. nextflow run andersenlab/wi-gatk -profile local -resume Debugging the pipeline on Quest \u00b6 When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run andersenlab/wi-gatk -profile quest_debug -resume Running the pipeline on Quest \u00b6 The pipeline can be run on Quest using the following command: nextflow run andersenlab/wi-gatk -profile quest --sample_sheet <path_to_sheet> -resume Parameters \u00b6 Most configuration is handled using the -profile flag and nextflow.config ; If you want to fine tune things you can use the options below. --sample_sheet \u00b6 The sample sheet is the output of 5.low_map_cov_for_seq_sheet.Rmd after running alignment-nf (soon to be integrated into alignment-nf). The sample sheet contains 5 columns as detailed below: strain bam bai coverage percent_mapped AB1 AB1.bam AB1.bam.bai 64 99.4 AB4 AB4.bam AB4.bam.bai 52 99.2 BRC20067 BRC20067.bam BRC20067.bam.bai 30 92.5 Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced. --bam_location (optional) \u00b6 Path to directory holding all the alignment files for strains in the analysis. Defaults to /projects/b1059/data/{species}/WI/alignments/ Important Remember to move your bam files output from alignment-nf to this location prior to running wi-gatk . In most cases, you will want to run wi-gatk on all samples, new and old combined. --species (optional) \u00b6 default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis --project (optional) \u00b6 default = PRJNA13758 WormBase project ID for selected species. Choose from some examples here --ws_build (optional) \u00b6 default = WS276 WormBase version to use for reference genome. --reference (optional) \u00b6 A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz Note If running on QUEST, instead of changing the reference parameter, opt to change the species , project , and ws_build for other species like c_briggsae (and then the reference will change automatically) --mito_name (optional) \u00b6 Name of contig to skip het polarization. Might need to change for other species besides c_elegans if the mitochondria contig is named differently. Defaults to MtDNA . --output (optional) \u00b6 A directory in which to output results. By default it will be WI-YYYYMMDD where YYYYMMDD is todays date. Output \u00b6 The final output directory looks like this: \u251c\u2500\u2500 variation \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.hard-filter.stats.txt \u2502 \u251c\u2500\u2500 *.hard-filter.filter_stats.txt \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.soft-filter.stats.txt \u2502 \u2514\u2500\u2500 *.soft-filter.filter_stats.txt \u2514\u2500\u2500 report \u251c\u2500\u2500 multiqc.html \u2514\u2500\u2500 multiqc_data \u2514\u2500\u2500 multiqc_*.json Data Storage \u00b6 Cleanup \u00b6 The hard-filter.vcf is the input for both the concordance-nf pipeline and the post-gatk-nf pipeline. Once both pipelines have been completed successfully, the hard and soft filter vcf and index files (everything output in the variation folder) can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf .","title":"wi-gatk"},{"location":"pipeline-wiGATK/#wi-gatk","text":"wi-gatk Pipeline Overview Software Requirements Usage Profiles Running the pipeline locally Debugging the pipeline on Quest Running the pipeline on Quest Parameters --sample_sheet --bam_location (optional) --species (optional) --project (optional) --ws_build (optional) --reference (optional) --mito_name (optional) --output (optional) Output Data Storage Cleanup The wi-gatk pipeline filters and calls variants from wild isolate sequence data.","title":"wi-gatk"},{"location":"pipeline-wiGATK/#pipeline_overview","text":"_______ _______ _______ __ __ _______ _______ | __| _ |_ _| |/ | | | | ___| | | | | | | | < | | ___| |_______|___|___| |___| |__|\\\\__| |__|____|___| parameters description Set/Default ========== =========== ======================== --debug Use --debug to indicate debug mode null --output Release Directory WI-{date} --sample_sheet Sample sheet null --bam_location Directory of bam files /projects/b1059/data/{species}/WI/alignments/ --mito_name Contig not to polarize hetero sites MtDNA Reference Genome --------------- --reference_base Location of ref genomes /projects/b1059/data/{species}/genomes/ --species/project/build These 4 params form --reference {species} / {project} / {ws_build} Variant Filters --------------- --min_depth Minimum variant depth 5 --qual Variant QUAL score 30 --strand_odds_ratio SOR_strand_odds_ratio 5 --quality_by_depth QD_quality_by_depth 20 --fisherstrand FS_fisher_strand 100 --high_missing Max % missing genotypes 0.95 --high_heterozygosity Max % max heterozygosity 0.10","title":"Pipeline Overview"},{"location":"pipeline-wiGATK/#software_requirements","text":"The latest update requires Nextflow version 20.0+. On QUEST, you can access this version by loading the nf20 conda environment prior to running the pipeline command: module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env Alternatively you can update Nextflow by running: nextflow self-update This pipeline also uses a docker image andersenlab/gatk4 to manage software and reproducibility. To access this docker image, first load the singularity module on QUEST. module load singularity","title":"Software Requirements"},{"location":"pipeline-wiGATK/#usage","text":"Note: if you are having issues running Nextflow or need reminders, check out the Nextflow page.","title":"Usage"},{"location":"pipeline-wiGATK/#profiles","text":"The nextflow.config file included with this pipeline contains three profiles. These set up the environment for testing local development, testing on Quest, and running the pipeline on Quest. local - Used for local development. Uses the docker container. quest_debug - Runs a small subset of available test data. Should complete within a couple of hours. For testing/diagnosing issues on Quest. quest - Runs the entire dataset. Note If you forget to add a -profile , the quest profile will be chosen as default","title":"Profiles"},{"location":"pipeline-wiGATK/#running_the_pipeline_locally","text":"When running locally, the pipeline will run using the andersenlab/gatk4 docker image. You must have docker installed. nextflow run andersenlab/wi-gatk -profile local -resume","title":"Running the pipeline locally"},{"location":"pipeline-wiGATK/#debugging_the_pipeline_on_quest","text":"When running on Quest, you should first run the quest debug profile. The Quest debug profile will use a test dataset and sample sheet which runs much faster and will encounter errors much sooner should they need to be fixed. If the debug dataset runs to completion it is likely that the full dataset will as well. nextflow run andersenlab/wi-gatk -profile quest_debug -resume","title":"Debugging the pipeline on Quest"},{"location":"pipeline-wiGATK/#running_the_pipeline_on_quest","text":"The pipeline can be run on Quest using the following command: nextflow run andersenlab/wi-gatk -profile quest --sample_sheet <path_to_sheet> -resume","title":"Running the pipeline on Quest"},{"location":"pipeline-wiGATK/#parameters","text":"Most configuration is handled using the -profile flag and nextflow.config ; If you want to fine tune things you can use the options below.","title":"Parameters"},{"location":"pipeline-wiGATK/#--sample_sheet","text":"The sample sheet is the output of 5.low_map_cov_for_seq_sheet.Rmd after running alignment-nf (soon to be integrated into alignment-nf). The sample sheet contains 5 columns as detailed below: strain bam bai coverage percent_mapped AB1 AB1.bam AB1.bam.bai 64 99.4 AB4 AB4.bam AB4.bam.bai 52 99.2 BRC20067 BRC20067.bam BRC20067.bam.bai 30 92.5 Important It is essential that you always use the pipelines and scripts to generate this sample sheet and NEVER manually. There are lots of strains and we want to make sure the entire process can be reproduced.","title":"--sample_sheet"},{"location":"pipeline-wiGATK/#--bam_location_optional","text":"Path to directory holding all the alignment files for strains in the analysis. Defaults to /projects/b1059/data/{species}/WI/alignments/ Important Remember to move your bam files output from alignment-nf to this location prior to running wi-gatk . In most cases, you will want to run wi-gatk on all samples, new and old combined.","title":"--bam_location (optional)"},{"location":"pipeline-wiGATK/#--species_optional","text":"default = c_elegans Options: c_elegans, c_briggsae, or c_tropicalis","title":"--species (optional)"},{"location":"pipeline-wiGATK/#--project_optional","text":"default = PRJNA13758 WormBase project ID for selected species. Choose from some examples here","title":"--project (optional)"},{"location":"pipeline-wiGATK/#--ws_build_optional","text":"default = WS276 WormBase version to use for reference genome.","title":"--ws_build (optional)"},{"location":"pipeline-wiGATK/#--reference_optional","text":"A fasta reference indexed with BWA. On Quest, the reference is available here: /projects/b1059/data/c_elegans/genomes/PRJNA13758/WS276/c_elegans.PRJNA13758.WS276.genome.fa.gz Note If running on QUEST, instead of changing the reference parameter, opt to change the species , project , and ws_build for other species like c_briggsae (and then the reference will change automatically)","title":"--reference (optional)"},{"location":"pipeline-wiGATK/#--mito_name_optional","text":"Name of contig to skip het polarization. Might need to change for other species besides c_elegans if the mitochondria contig is named differently. Defaults to MtDNA .","title":"--mito_name (optional)"},{"location":"pipeline-wiGATK/#--output_optional","text":"A directory in which to output results. By default it will be WI-YYYYMMDD where YYYYMMDD is todays date.","title":"--output (optional)"},{"location":"pipeline-wiGATK/#output","text":"The final output directory looks like this: \u251c\u2500\u2500 variation \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.hard-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.hard-filter.stats.txt \u2502 \u251c\u2500\u2500 *.hard-filter.filter_stats.txt \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.gz \u2502 \u251c\u2500\u2500 *.soft-filter.vcf.tbi \u2502 \u251c\u2500\u2500 *.soft-filter.stats.txt \u2502 \u2514\u2500\u2500 *.soft-filter.filter_stats.txt \u2514\u2500\u2500 report \u251c\u2500\u2500 multiqc.html \u2514\u2500\u2500 multiqc_data \u2514\u2500\u2500 multiqc_*.json","title":"Output"},{"location":"pipeline-wiGATK/#data_storage","text":"","title":"Data Storage"},{"location":"pipeline-wiGATK/#cleanup","text":"The hard-filter.vcf is the input for both the concordance-nf pipeline and the post-gatk-nf pipeline. Once both pipelines have been completed successfully, the hard and soft filter vcf and index files (everything output in the variation folder) can be moved to /projects/b1059/data/{species}/WI/variation/{date}/vcf .","title":"Cleanup"},{"location":"quest-conda/","text":"Using conda on Quest \u00b6 Using conda on Quest Why Conda Setting up Conda on Quest Using Conda Running Nextflow with conda Notes on conda versions on Quest Why Conda \u00b6 Computational Reproducibility is the ability to reproduce an analysis exactly. In order for computational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub (for help, see the Github page ). Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software, package and environment managers such as Conda and Docker are very useful. Conda works similarly to brew or pyenv pyenv that were used in the legacy Andersen-Lab-Env. Note The software environments on Mac and Linux are not exactly identical...but they are very close. Setting up Conda on Quest \u00b6 Anaconda is a Python distribution that contains many packages including conda. Miniconda is a more compact version of Anaconda that also includes conda. So in order to install conda, we usually either install Miniconda or Anaconda. On Quest, Anaconda is already installed. However there are many versions of Anaconda, each can have a different version of Python and Conda. The current lab environments mainly used module load python/anaconda3.6 (See Notes below for more info). In your home directory ~/ , create a file called .condarc and put the following lines into it. It sets the channel priority when conda searches for packages. If possible, in one environment it is good to use packages from the same channel. channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /projects/b1059/software/conda_envs/ Using Conda \u00b6 Conda Documentation Conda Cheatsheet After loading the Anaconda module, one can create an environment and install packages into that environment: # Create conda environment named \"name_of_env\" conda create --name name_of_env # Create conda environment in a specific folder conda create -p /projects/b1059/software/conda_envs/test # activate conda environment # for some `conda activate` works and for others `source activate` conda activate test source activate test # install package into conda environment conda install bcftools When looking to install a package, one resource to check out is anaconda.org , search for your package and run the install command listed on the page. Note Remember to keep in mind the different versions of software/packages. You could have bcftools-v1.10 or bcftools-v.1.12, so make sure you install the correct one! Important You can also install R packages with conda, however conda and R don't always work well together. Check out our quick fix here Running Nextflow with conda \u00b6 When running Nextflow, conda environments can be specified as part of a process or in the nextflow.config file to apply to the entire pipeline (check out the documentation ): Conda within a process: process foo { conda '/projects/b1059/software/conda_envs/cegwas2-nf_env' ''' your_command --here ''' } Conda for the entire pipeline: // in the nextflow.config file: conda { conda.enabled = true conda.cacheDir = \".env\" } process { conda = \"/projects/b1059/software/conda_envs/cegwas2-nf_env\" } Notes on conda versions on Quest \u00b6 tl;dr; If having trouble with conda, or Nextflow gives conda-related errors, try to load a different version of anaconda on Quest. At some point it may be worth re-creating all conda environments in the lab with a consistent version of conda. As of the end of 2020, existing conda environments for the lab were mostly created by module load python/anaconda (which got automatically loaded with module git by accident). It loads Python version 2.7.18 and conda 4.5.2. The other environments were created with module load python/anaconda3.6 which loads Python 3.6.0 and conda 4.3.30. To see versions, use conda info or conda -V . Once you activate an environment with conda activate env_name or source activate env_name , the default conda usually get re-directed to the conda that were originally used to create the environment. This is good because it helps ensure that all packages in the same environment uses the same version of conda. One can go to cd ~/.conda/env_name/bin and readlink -f conda or readlink -f activate to see which version of conda is used by this environment. This is exactly how Nextflow determines which conda to use when using an existing conda environment. Quest people recommended module load python-anaconda3 , but that version does not mix well with the versions mentioned above. But if one were to re-install all environments we have, this is probably the version to stick to.","title":"Conda"},{"location":"quest-conda/#using_conda_on_quest","text":"Using conda on Quest Why Conda Setting up Conda on Quest Using Conda Running Nextflow with conda Notes on conda versions on Quest","title":"Using conda on Quest"},{"location":"quest-conda/#why_conda","text":"Computational Reproducibility is the ability to reproduce an analysis exactly. In order for computational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub (for help, see the Github page ). Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software, package and environment managers such as Conda and Docker are very useful. Conda works similarly to brew or pyenv pyenv that were used in the legacy Andersen-Lab-Env. Note The software environments on Mac and Linux are not exactly identical...but they are very close.","title":"Why Conda"},{"location":"quest-conda/#setting_up_conda_on_quest","text":"Anaconda is a Python distribution that contains many packages including conda. Miniconda is a more compact version of Anaconda that also includes conda. So in order to install conda, we usually either install Miniconda or Anaconda. On Quest, Anaconda is already installed. However there are many versions of Anaconda, each can have a different version of Python and Conda. The current lab environments mainly used module load python/anaconda3.6 (See Notes below for more info). In your home directory ~/ , create a file called .condarc and put the following lines into it. It sets the channel priority when conda searches for packages. If possible, in one environment it is good to use packages from the same channel. channels: - conda-forge - bioconda - defaults auto_activate_base: false envs_dirs: - ~/.conda/envs/ - /projects/b1059/software/conda_envs/","title":"Setting up Conda on Quest"},{"location":"quest-conda/#using_conda","text":"Conda Documentation Conda Cheatsheet After loading the Anaconda module, one can create an environment and install packages into that environment: # Create conda environment named \"name_of_env\" conda create --name name_of_env # Create conda environment in a specific folder conda create -p /projects/b1059/software/conda_envs/test # activate conda environment # for some `conda activate` works and for others `source activate` conda activate test source activate test # install package into conda environment conda install bcftools When looking to install a package, one resource to check out is anaconda.org , search for your package and run the install command listed on the page. Note Remember to keep in mind the different versions of software/packages. You could have bcftools-v1.10 or bcftools-v.1.12, so make sure you install the correct one! Important You can also install R packages with conda, however conda and R don't always work well together. Check out our quick fix here","title":"Using Conda"},{"location":"quest-conda/#running_nextflow_with_conda","text":"When running Nextflow, conda environments can be specified as part of a process or in the nextflow.config file to apply to the entire pipeline (check out the documentation ): Conda within a process: process foo { conda '/projects/b1059/software/conda_envs/cegwas2-nf_env' ''' your_command --here ''' } Conda for the entire pipeline: // in the nextflow.config file: conda { conda.enabled = true conda.cacheDir = \".env\" } process { conda = \"/projects/b1059/software/conda_envs/cegwas2-nf_env\" }","title":"Running Nextflow with conda"},{"location":"quest-conda/#notes_on_conda_versions_on_quest","text":"tl;dr; If having trouble with conda, or Nextflow gives conda-related errors, try to load a different version of anaconda on Quest. At some point it may be worth re-creating all conda environments in the lab with a consistent version of conda. As of the end of 2020, existing conda environments for the lab were mostly created by module load python/anaconda (which got automatically loaded with module git by accident). It loads Python version 2.7.18 and conda 4.5.2. The other environments were created with module load python/anaconda3.6 which loads Python 3.6.0 and conda 4.3.30. To see versions, use conda info or conda -V . Once you activate an environment with conda activate env_name or source activate env_name , the default conda usually get re-directed to the conda that were originally used to create the environment. This is good because it helps ensure that all packages in the same environment uses the same version of conda. One can go to cd ~/.conda/env_name/bin and readlink -f conda or readlink -f activate to see which version of conda is used by this environment. This is exactly how Nextflow determines which conda to use when using an existing conda environment. Quest people recommended module load python-anaconda3 , but that version does not mix well with the versions mentioned above. But if one were to re-install all environments we have, this is probably the version to stick to.","title":"Notes on conda versions on Quest"},{"location":"quest-intro/","text":"Introduction \u00b6 Introduction New Users Signing into Quest Login Nodes Home Directory Projects and Queues Running interactive jobs on Quest Using screen or nohup to keep jobs from timing out Using packages already installed on Quest Submitting jobs to Quest Monitoring SLURM Jobs on Quest The Andersen Lab makes use of Quest, the supercomputer at Northwestern. Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up: Quest Documentation Note New to the command line? Check out the quick and dirty bash intro or this introduction first! New Users \u00b6 To gain access to Quest: Register new user with your NetID here . Apply to be added to partition b1059 here . Allocation manager: Erik Andersen, erik.andersen@northwestern.edu Apply to be added to partition b1042 here . Allocation manager: Janna Nugent, janna.nugent@northwestern.edu Quest has its Slack channel: genomics-rcs.slack.com ( here ) and help email: quest-help@northwestern.edu for users to get help. Signing into Quest \u00b6 After you gain access to the cluster you can login using: ssh <netid>@quest.it.northwestern.edu To avoid typing in the password everytime, one can set up a ssh key . I recommend setting an alias in your ~/.bash_profile to make logging in quicker: alias quest=\"ssh <netid>@quest.it.northwestern.edu\" The above line makes it so you simply type quest and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below). Login Nodes \u00b6 There are four login nodes we use: quser21-24. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired ( e.g. ssh quser21 ). Warning When using screen to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those screen sessions. Home Directory \u00b6 Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 80 Gb. More information on quotas, storage, etc . You can check your storgae space using the following command: du -hs * More information is provided below to help install and use software. Projects and Queues \u00b6 Quest is broadly organized into projects (or partitions). Projects have associated with them storage, nodes, and users. The Andersen lab has access to two projects. b1042 - The 'Genomics' Project has 200 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at /projects/b1042/AndersenLab/ . By default, files are deleted after 30 days. One can submit jobs to --partition=genomicsguestA to use this partition, with a max job duration of 48hr. b1059 - The Andersen Lab Project. b1059 has 5 computing nodes qnode9031 - qnode9033 (192Gb of RAM and 40 cores eacy) and qnode0277 - qnode0278 (192Gb of RAM and 52 cores each), and has 77 Tb of storage. b1059 storage is located at: /projects/b1059/ (Check out this page to learn more about how data is stored on b1059 ). One can submit jobs to --partition=b1059 to use this partition, with no limit on job duration. Note Anyone who uses quest should build your own project folder under /projects/b1059/projects with your name. You should only write and revise files under your project folder. You can read/copy data from b1059 but don't write any data out of your project folder. Important It is important that we keep the 77 Tb of storage space on b1059 from filling up with extraneous or intermediate files. It is good practice to clean up old data/files and backup important data at least every few months. You can check the percent of space remaining with checkproject b1059 Running interactive jobs on Quest \u00b6 If you are running a few simple commands or want to experiment with files directly you can start an interactive session on Quest. The command below will give you access to a node where you can run your commands srun -A b1042 --partition=genomicsguestA -N 1 -n 24 --mem=64G --time=12:00:00 --pty bash -i Important Do not run commands for big data on quser21-24 . These are login nodes and are not meant for running heavy-load workflows. Using screen or nohup to keep jobs from timing out \u00b6 If you have ever tried to run a pipeline or script that takes a long time (think NemaScan ), you know that if you close down your terminal or if your QUEST session logs out, it will cause your jobs to end prematurely. There are several ways to avoid this: screen Perhaps the most common way to deal with scripts that run for a long time is screen . For the most simple case use, type screen to open a new screen session and then run your script like normal. Below are some more intermediate commands for taking full advantage of screen : screen -S <some_descriptive_name> : Use this command to name your screen session. Especially useful if you have several scren sessions running and/or want to get back to this particular one later. Ctrl+a follwed by Ctrl+d to detach from the current screen session (NOT Ctrl+a+d !) exit to end the current screen session screen -ls lists the IDs of all screen sessions currently running screen -r <screen_id> : Use this command to resume a particular screen session. If you only have one session running you can simply use screen -r Important When using screen on QUEST, take note that screen sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh quser22 ). nohup Another way to avoid SIGHUP errors is to use nohup . nohup is very simple to use, simply type nohup before your normal command and that's it! nohup nextflow run main.nf Running a command with nohup will look a little different than usual because it will not print anything to the console. Instead, it prints all console outputs into a file in the current directory called nohup.out . You can redirect the output to another filename using: nohup nextflow run main.nf > cegwas_{date}_output.txt When you exit this window and open a new session, you can always look at the contents of the output file using cat nohup.out to see the progress. Note You can also run nohup in the background to continue using the same window for other processes by running nohup nextflow run main.nf & . Using packages already installed on Quest \u00b6 Quest has a collection of packages installed. You can run module avail to see what packages are currently available on Quest. You can use module load bcftools or module load bcftools/1.10.1 to load packages with the default or a specific version ( module add does the same thing). If you do echo $PATH before and after loading modules, you can see what module does is simply appending paths to the packages into your $PATH so the packages can be found in your environment. module purge will remove all loaded packages and can be helpful to try when troubleshooting. Note You can also load/install software packages into conda environments for use with a specific project/pipeline. Check out the conda tutorial here . Submitting jobs to Quest \u00b6 Jobs on Quest are managed by SLURM . While most of our pipelines are managed by Nextflow, it is useful to know how to submit jobs directly to SLURM. Below is a template to submit an array job to SLURM that will run 10 parallel jobs. One can run it with sbatch script.sh . If you dig into the Nextflow working directory, you can see Nextflow actually generates such scripts and submits them to SLURM on your behalf. Thanks, Nextflow! #!/bin/bash #SBATCH -J name # Name of job #SBATCH -A b1042 # Allocation #SBATCH -p genomicsguestA # Queue #SBATCH -t 24:00:00 # Walltime/duration of the job (hh:mm:ss) #SBATCH --cpus-per-task=1 # Number of cores (= processors = cpus) for each task #SBATCH --mem-per-cpu=3G # Memory per core needed for a job #SBATCH --array=0-9 # number of parallel jobs to run counting from 0. Make sure to change this to match total number of files. # make a list of all the files you want to process # this script will run a parallel process for each file in this list name_list=(*.bam) # take the nth ($SGE_TASK_ID-th) file in name_list # don't change this line Input=${name_list[$SLURM_ARRAY_TASK_ID]} # then do your operation on this file Output=`echo $Input | sed 's/.bam/.sam/'` Monitoring SLURM Jobs on Quest \u00b6 Here are some commmon commands used to interact with SLURM and check on job status. For more, check out this page . squeue -u <netid> to check all jobs of a user. scancel <jobid> to cancel a job. scancel {30000..32000} to cancel a range of jobs (job id between 30000 and 32000). sinfo | grep b1059 to check status of nodes. alloc means all cores on a node are completed engaged; mix means some cores on a node are engaged; idle means all cores on a node are available to accept jobs.","title":"Introduction"},{"location":"quest-intro/#introduction","text":"Introduction New Users Signing into Quest Login Nodes Home Directory Projects and Queues Running interactive jobs on Quest Using screen or nohup to keep jobs from timing out Using packages already installed on Quest Submitting jobs to Quest Monitoring SLURM Jobs on Quest The Andersen Lab makes use of Quest, the supercomputer at Northwestern. Take some time to read over the overview of what Quest is, what it does, how to use it, and how to sign up: Quest Documentation Note New to the command line? Check out the quick and dirty bash intro or this introduction first!","title":"Introduction"},{"location":"quest-intro/#new_users","text":"To gain access to Quest: Register new user with your NetID here . Apply to be added to partition b1059 here . Allocation manager: Erik Andersen, erik.andersen@northwestern.edu Apply to be added to partition b1042 here . Allocation manager: Janna Nugent, janna.nugent@northwestern.edu Quest has its Slack channel: genomics-rcs.slack.com ( here ) and help email: quest-help@northwestern.edu for users to get help.","title":"New Users"},{"location":"quest-intro/#signing_into_quest","text":"After you gain access to the cluster you can login using: ssh <netid>@quest.it.northwestern.edu To avoid typing in the password everytime, one can set up a ssh key . I recommend setting an alias in your ~/.bash_profile to make logging in quicker: alias quest=\"ssh <netid>@quest.it.northwestern.edu\" The above line makes it so you simply type quest and the login process is initiated. If you are not familiar with what a bash profile is, take a look at this . Important When you login it is important to be conscientious of the fact that you are on a login node. You should not be running any sort of heavy duty operation on these nodes. Instead, any analysis you perform should be submitted as a job or run using an interactive job (see below).","title":"Signing into Quest"},{"location":"quest-intro/#login_nodes","text":"There are four login nodes we use: quser21-24. When you login you will be assigned to a random login node. You can switch login nodes by typing ssh and the node desired ( e.g. ssh quser21 ). Warning When using screen to submit and run jobs they will only persist on the login node you are currently on. If you log out and later log back in you may be logged in to a different login node. You will need to switch to that login node to access those screen sessions.","title":"Login Nodes"},{"location":"quest-intro/#home_directory","text":"Logging in places you in your home directory. You can install software in your home directory for use when you run jobs, or for small files/analysis. Your home directory has a quota of 80 Gb. More information on quotas, storage, etc . You can check your storgae space using the following command: du -hs * More information is provided below to help install and use software.","title":"Home Directory"},{"location":"quest-intro/#projects_and_queues","text":"Quest is broadly organized into projects (or partitions). Projects have associated with them storage, nodes, and users. The Andersen lab has access to two projects. b1042 - The 'Genomics' Project has 200 Tb of space and 100 nodes associated with it. This space is shared with other labs and is designed for temporary use only (covered in greater detail in the Nextflow Section). The space is available at /projects/b1042/AndersenLab/ . By default, files are deleted after 30 days. One can submit jobs to --partition=genomicsguestA to use this partition, with a max job duration of 48hr. b1059 - The Andersen Lab Project. b1059 has 5 computing nodes qnode9031 - qnode9033 (192Gb of RAM and 40 cores eacy) and qnode0277 - qnode0278 (192Gb of RAM and 52 cores each), and has 77 Tb of storage. b1059 storage is located at: /projects/b1059/ (Check out this page to learn more about how data is stored on b1059 ). One can submit jobs to --partition=b1059 to use this partition, with no limit on job duration. Note Anyone who uses quest should build your own project folder under /projects/b1059/projects with your name. You should only write and revise files under your project folder. You can read/copy data from b1059 but don't write any data out of your project folder. Important It is important that we keep the 77 Tb of storage space on b1059 from filling up with extraneous or intermediate files. It is good practice to clean up old data/files and backup important data at least every few months. You can check the percent of space remaining with checkproject b1059","title":"Projects and Queues"},{"location":"quest-intro/#running_interactive_jobs_on_quest","text":"If you are running a few simple commands or want to experiment with files directly you can start an interactive session on Quest. The command below will give you access to a node where you can run your commands srun -A b1042 --partition=genomicsguestA -N 1 -n 24 --mem=64G --time=12:00:00 --pty bash -i Important Do not run commands for big data on quser21-24 . These are login nodes and are not meant for running heavy-load workflows.","title":"Running interactive jobs on Quest"},{"location":"quest-intro/#using_screen_or_nohup_to_keep_jobs_from_timing_out","text":"If you have ever tried to run a pipeline or script that takes a long time (think NemaScan ), you know that if you close down your terminal or if your QUEST session logs out, it will cause your jobs to end prematurely. There are several ways to avoid this: screen Perhaps the most common way to deal with scripts that run for a long time is screen . For the most simple case use, type screen to open a new screen session and then run your script like normal. Below are some more intermediate commands for taking full advantage of screen : screen -S <some_descriptive_name> : Use this command to name your screen session. Especially useful if you have several scren sessions running and/or want to get back to this particular one later. Ctrl+a follwed by Ctrl+d to detach from the current screen session (NOT Ctrl+a+d !) exit to end the current screen session screen -ls lists the IDs of all screen sessions currently running screen -r <screen_id> : Use this command to resume a particular screen session. If you only have one session running you can simply use screen -r Important When using screen on QUEST, take note that screen sessions are only visible/available when you are logged on to the particular node it was created on. You can jump between nodes by simply typing ssh and the login node you want ( e.g. ssh quser22 ). nohup Another way to avoid SIGHUP errors is to use nohup . nohup is very simple to use, simply type nohup before your normal command and that's it! nohup nextflow run main.nf Running a command with nohup will look a little different than usual because it will not print anything to the console. Instead, it prints all console outputs into a file in the current directory called nohup.out . You can redirect the output to another filename using: nohup nextflow run main.nf > cegwas_{date}_output.txt When you exit this window and open a new session, you can always look at the contents of the output file using cat nohup.out to see the progress. Note You can also run nohup in the background to continue using the same window for other processes by running nohup nextflow run main.nf & .","title":"Using screen or nohup to keep jobs from timing out"},{"location":"quest-intro/#using_packages_already_installed_on_quest","text":"Quest has a collection of packages installed. You can run module avail to see what packages are currently available on Quest. You can use module load bcftools or module load bcftools/1.10.1 to load packages with the default or a specific version ( module add does the same thing). If you do echo $PATH before and after loading modules, you can see what module does is simply appending paths to the packages into your $PATH so the packages can be found in your environment. module purge will remove all loaded packages and can be helpful to try when troubleshooting. Note You can also load/install software packages into conda environments for use with a specific project/pipeline. Check out the conda tutorial here .","title":"Using packages already installed on Quest"},{"location":"quest-intro/#submitting_jobs_to_quest","text":"Jobs on Quest are managed by SLURM . While most of our pipelines are managed by Nextflow, it is useful to know how to submit jobs directly to SLURM. Below is a template to submit an array job to SLURM that will run 10 parallel jobs. One can run it with sbatch script.sh . If you dig into the Nextflow working directory, you can see Nextflow actually generates such scripts and submits them to SLURM on your behalf. Thanks, Nextflow! #!/bin/bash #SBATCH -J name # Name of job #SBATCH -A b1042 # Allocation #SBATCH -p genomicsguestA # Queue #SBATCH -t 24:00:00 # Walltime/duration of the job (hh:mm:ss) #SBATCH --cpus-per-task=1 # Number of cores (= processors = cpus) for each task #SBATCH --mem-per-cpu=3G # Memory per core needed for a job #SBATCH --array=0-9 # number of parallel jobs to run counting from 0. Make sure to change this to match total number of files. # make a list of all the files you want to process # this script will run a parallel process for each file in this list name_list=(*.bam) # take the nth ($SGE_TASK_ID-th) file in name_list # don't change this line Input=${name_list[$SLURM_ARRAY_TASK_ID]} # then do your operation on this file Output=`echo $Input | sed 's/.bam/.sam/'`","title":"Submitting jobs to Quest"},{"location":"quest-intro/#monitoring_slurm_jobs_on_quest","text":"Here are some commmon commands used to interact with SLURM and check on job status. For more, check out this page . squeue -u <netid> to check all jobs of a user. scancel <jobid> to cancel a job. scancel {30000..32000} to cancel a range of jobs (job id between 30000 and 32000). sinfo | grep b1059 to check status of nodes. alloc means all cores on a node are completed engaged; mix means some cores on a node are engaged; idle means all cores on a node are available to accept jobs.","title":"Monitoring SLURM Jobs on Quest"},{"location":"quest-mount/","text":"1. Download and Install Fuse for Mac OS https://osxfuse.github.io/ 2. Install sshfs You can use the link on https://osxfuse.github.io/ or use: brew install sshfs 3. Create a folder in your documents called b1059 mkdir ~/b1059 4. Mount our labs quest project folder ( b1059 ) to the b1059 folder you created locally sshfs <NETID>@quest.it.northwestern.edu:/projects/b1059/ ~/Documents/b1059 -ovolname=b1059 To mount alignments of isotypes at this location: sshfs <NETID>@quest.it.northwestern.edu:/projects/b1059/data/alignments/WI/isotype ~/Documents/b1059 -ovolname=b1059","title":"Quest mount"},{"location":"quest-nextflow/","text":"Nextflow \u00b6 Nextflow Installation Nextflow versions Quest cluster configuration Global Configuration: ~/.nextflow/config Running Nextflow Running Nextflow from a remote directory Running a different branch of a pipeline Running a specific commit Troubleshooting remote pipelines Running Nextflow from a local directory Resume Getting an email or text when complete Writing Nextflow Pipelines Nextflow is an awesome program that allows you to write a computational pipeline by making it simpler to put together many different tasks, maybe even using different programming languages. Nextflow makes parallelism easy and works with SLURM to schedule jobs so you don't have to! Nextflow also supports docker containers and conda enviornments to streamline reproducible pipelines and can be easily adapted to run on different systems - including Google Cloud! Not convinced? Check out this intro . Installation \u00b6 Nextflow can be installed with two easy steps: # download with wget or curl wget -qO- https://get.nextflow.io | bash # or # curl -s https://get.nextflow.io | bash # make binary executable on your system chmod +x nextflow # Move nextflow to directory accessible by your $PATH to avoid having to type full path to nextflow each time # you can check your $PATH with: echo $PATH # it likely contains `/usr/local/bin` so you could move nextflow there mv nextflow /usr/local/bin/ Nextflow versions \u00b6 You might also want to update nextflow or be able to run different versions. This can be done in several different ways Update Nextflow nextflow self-update Use a specific version of Nextflow NXF_VER=20.04.0 nextflow run main.nf Use the nf20 conda environment built for AndersenLab pipelines module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env (You can check your Nextflow version with nextflow -v or nextflow --version ) Note If you load this conda environment, it is not necessary to have Nextflow version 20 installed on your system -- you don't even need to have Nextflow installed at all! Because different versions might have updates that affect the running of Nextflow, it is important to keep track of the version of Nextflow you are using, as well as all software packages. Important Many of the Andersen Lab pipelines (if not all) are written with the new DSL2 (see below) which requires Nextflow-v20.0+. Loading the nf20 conda environment is a great way to run these pipelines Quest cluster configuration \u00b6 Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config . Global Configuration: ~/.nextflow/config \u00b6 In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { executor = 'slurm' queue = 'genomicsguestA' clusterOptions = '-A b1042 -t 24:00:00 -e errlog.txt' } workDir = \"/projects/b1042/AndersenLab/work/<your folder>\" tmpDir = \"/projects/b1042/AndersenLab/tmp\" This configuration file does the following: Sets the executor to slurm (which is what Quest uses) Sets the queue to genomicsguestA which submits jobs to genomics nodes. The genomicsguestA will submit jobs to our dedicated nodes first, which we have high priority. If our dedicated nodes are full, it will submit to other nodes we don't have priority. So far, our lab have 2 dedicated nodes, with 28 cores and related memory (close to 1:5) for each dedicated node. We will have more in the future. clusterOptions - Sets the account to b1042 ; granting access to genomics-dedicated scratch space. workDir - Sets the working directory to scratch space on b1042. To better organization, Please build your own folder under /projects/b1042/AndersenLab/work/ , and define it here. Note: replace '< your folder >' with your name tmpDir - Creates a temporary working directory. This can be used within workflows when necessary. Running Nextflow \u00b6 Theoretically, running a Nextflow pipeline should be very straightforward (although with any developing pipelines there are always bugs to work out). Running Nextflow from a remote directory \u00b6 The prefered (and sometimes easiest) way to run a nextflow pipeline can be done in just one single step. When this works (see troubleshooting section below), pipelines can be run without first cloning the git repo. You can just tell Nextflow which git repo to use and it will do the rest! This can be helpful to reduce clutter and avoid making changes to the actual pipeline. Additionally, this method allows you to control which branch and/or commit of the pipeline to run, and Nextflow will automatically track that information for you allowing for great reproducibility. Note There is no need to clone a copy of the repo to run pipelines this way. Behind the scenes, nextflow is actually cloning the repo to your home directory and keeping track of branches/commits. # example command to run the latest version of NemaScan nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 # Note: can also write in one line, the \\ were used to make the code more readable: nextflow run andersenlab/nemascan --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv --vcf 20210121 Note Parameters or arguments to Nextflow scripts are designated with the -- . In the case above, there is a parameter called \"in\" which we are setting to be the test.tsv file. When Nextflow is running, it will print to the console the name of each process in the pipeline as well as update on the progress of the script: For example, in the above screenshot from a NemaScan run, there are 14 different processes in the pipeline. Notice that the fix_strain_names_bulk process has already completed! Meanwhile, the vcf_t_geno_matrix process is still running. You can also see that the prepare_gcta_files is actually run 4 times (in this case, because it is run once per 4 traits in my dataset). Another important piece of information from this Nextflow run is the hash that designates the working directory of each process. the [ad/eea615] next to the fix_strain_names_bulk process indicates that the working directory is located at path_to_nextflow_working_directory/ad/eea615... . This can be helpful if you want to go into that directory to see what is actually happening or trouble shoot errors. Note I highly recommend adding this function to your ~/.bash_profile to easily access the Nexflow working directory: gw() {cd /projects/b1042/AndersenLab/work/<your_name>/$1*} so that when you type gw 3f/6a21a5 (the hash Nextflow shows that indicates the specific working directory for a process) you will go to that folder automatically. Running a different branch of a pipeline \u00b6 Instead of cloning the repo and then remembering to switch branches, you can specify the branch in your nextflow call. This is most beneficial because the pipeline will then keep a record of which branch you used so future you doesn't have to remember. # the following command runs the \"develop\" branch of nemascan # (not recommended unless you know what you are doing, might break) nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r develop Running a specific commit \u00b6 Sometimes we want to keep the version of a pipeline the same across several different runs (maybe preparing for a manuscript etc.). To do this, you can again use the -r argument and just provide the commit ID which is a long alphanumeric code that can be found on github or in the log of your previous nextflow run. # the following command runs will always run the exact same code, no matter how many changes to nemascan there are in the future nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r 769a2b75545d7f870a880447dd31482cfc628792 Troubleshooting remote pipelines \u00b6 For some reason, running nextflow remotely sometimes throws errors and I am not 100% sure why. Here are some of the most common errors I see and how to fix them. 1. It doesn't pull the latest commit Go to github.com and find the latest commit ID and use that with -r XXX (see above on running specific commits) If that doesn't work, you might have to clone the repo and run it locally (see below) 2. It says the \"repository may be corrupt\" I don't understand this error, but I think it happens when you try to run different branches/commits. Usually, I just go to the path where the error says is corrupt and delete the folder with rm -rf . For example: rm -rf /home/kek973/.nextflow/assets/andersenlab/nemascan Warning You should always be careful when using rm , especially rm -rf . There is no going back. Make certain you want to delete the folder and everything inside it. In this case, it will be fine because nextflow will just clone it again fresh. Running Nextflow from a local directory \u00b6 Another way to run Nextflow is by first cloning the git repo to your directory and then running the pipeline. This has advantages and disadvantages over running the pipeline remotely (see below), however if you need to make changes to the pipeline specific to your analysis, you will need to follow these steps. git clone https://github.com/AndersenLab/NemaScan.git cd NemaScan nextflow run main.nf --debug Resume \u00b6 Another great thing about Nextflow is that it caches all the processes that completed successfully, so if you have an error at the middle or end of your pipeline, you can re-run the same Nextflow command with -resume and it will start where it finished last time! nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -resume There is usually no downside to adding -resume , so you can get into the habit of always adding it if you want. Important There is some confusion with how the -resume works and sometimes it doesn't work as expected. Check out this and this other guide for more help. One thing I've learned is there is a difference between process.cache = 'deep' vs. 'lenient' . Getting an email or text when complete \u00b6 If you would like to receive an email or text from Nextflow when your pipeline finishes (either successfully or with error), all you need to do is add -N <email> to your code. Most phone companies have a way to \"email\" your phone as an SMS, so you can use this email address to get a text alert. For example: # send email nextflow run andersenlab/nemascan --debug -N kathryn.evans@northwestern.edu # send text to 801-867-5309 with verizon nextflow run andersenlab/nemascan --debut -N 8018675309@vtext.com Writing Nextflow Pipelines \u00b6 See this page for tips on how to get started with your own nextflow pipeline. Also check out the Nextflow documentation for help getting started!","title":"Nextflow"},{"location":"quest-nextflow/#nextflow","text":"Nextflow Installation Nextflow versions Quest cluster configuration Global Configuration: ~/.nextflow/config Running Nextflow Running Nextflow from a remote directory Running a different branch of a pipeline Running a specific commit Troubleshooting remote pipelines Running Nextflow from a local directory Resume Getting an email or text when complete Writing Nextflow Pipelines Nextflow is an awesome program that allows you to write a computational pipeline by making it simpler to put together many different tasks, maybe even using different programming languages. Nextflow makes parallelism easy and works with SLURM to schedule jobs so you don't have to! Nextflow also supports docker containers and conda enviornments to streamline reproducible pipelines and can be easily adapted to run on different systems - including Google Cloud! Not convinced? Check out this intro .","title":"Nextflow"},{"location":"quest-nextflow/#installation","text":"Nextflow can be installed with two easy steps: # download with wget or curl wget -qO- https://get.nextflow.io | bash # or # curl -s https://get.nextflow.io | bash # make binary executable on your system chmod +x nextflow # Move nextflow to directory accessible by your $PATH to avoid having to type full path to nextflow each time # you can check your $PATH with: echo $PATH # it likely contains `/usr/local/bin` so you could move nextflow there mv nextflow /usr/local/bin/","title":"Installation"},{"location":"quest-nextflow/#nextflow_versions","text":"You might also want to update nextflow or be able to run different versions. This can be done in several different ways Update Nextflow nextflow self-update Use a specific version of Nextflow NXF_VER=20.04.0 nextflow run main.nf Use the nf20 conda environment built for AndersenLab pipelines module load python/anaconda3.6 source activate /projects/b1059/software/conda_envs/nf20_env (You can check your Nextflow version with nextflow -v or nextflow --version ) Note If you load this conda environment, it is not necessary to have Nextflow version 20 installed on your system -- you don't even need to have Nextflow installed at all! Because different versions might have updates that affect the running of Nextflow, it is important to keep track of the version of Nextflow you are using, as well as all software packages. Important Many of the Andersen Lab pipelines (if not all) are written with the new DSL2 (see below) which requires Nextflow-v20.0+. Loading the nf20 conda environment is a great way to run these pipelines","title":"Nextflow versions"},{"location":"quest-nextflow/#quest_cluster_configuration","text":"Configuration files allow you to define the way a pipeline is executed on Quest. Read the quest documentation on configuration files Configuration files are defined at a global level in ~/.nextflow/config and on a per-pipeline basis within <pipeline_directory>/nextflow.config . Settings written in <pipeline_directory>/nextflow.config override settings written in ~/.nextflow/config .","title":"Quest cluster configuration"},{"location":"quest-nextflow/#global_configuration_nextflowconfig","text":"In order to use nextflow on quest you will need to define some global variables regarding the process. Our lab utilizies nodes and space dedicated to genomics projects. In order to access these resources your account will need to be granted access. Contact Quest and request access to the genomics nodes and project b1042 . Once you have access you will need to modify your global configuration. Set your ~/.nextflow/config file to be the following: process { executor = 'slurm' queue = 'genomicsguestA' clusterOptions = '-A b1042 -t 24:00:00 -e errlog.txt' } workDir = \"/projects/b1042/AndersenLab/work/<your folder>\" tmpDir = \"/projects/b1042/AndersenLab/tmp\" This configuration file does the following: Sets the executor to slurm (which is what Quest uses) Sets the queue to genomicsguestA which submits jobs to genomics nodes. The genomicsguestA will submit jobs to our dedicated nodes first, which we have high priority. If our dedicated nodes are full, it will submit to other nodes we don't have priority. So far, our lab have 2 dedicated nodes, with 28 cores and related memory (close to 1:5) for each dedicated node. We will have more in the future. clusterOptions - Sets the account to b1042 ; granting access to genomics-dedicated scratch space. workDir - Sets the working directory to scratch space on b1042. To better organization, Please build your own folder under /projects/b1042/AndersenLab/work/ , and define it here. Note: replace '< your folder >' with your name tmpDir - Creates a temporary working directory. This can be used within workflows when necessary.","title":"Global Configuration: ~/.nextflow/config"},{"location":"quest-nextflow/#running_nextflow","text":"Theoretically, running a Nextflow pipeline should be very straightforward (although with any developing pipelines there are always bugs to work out).","title":"Running Nextflow"},{"location":"quest-nextflow/#running_nextflow_from_a_remote_directory","text":"The prefered (and sometimes easiest) way to run a nextflow pipeline can be done in just one single step. When this works (see troubleshooting section below), pipelines can be run without first cloning the git repo. You can just tell Nextflow which git repo to use and it will do the rest! This can be helpful to reduce clutter and avoid making changes to the actual pipeline. Additionally, this method allows you to control which branch and/or commit of the pipeline to run, and Nextflow will automatically track that information for you allowing for great reproducibility. Note There is no need to clone a copy of the repo to run pipelines this way. Behind the scenes, nextflow is actually cloning the repo to your home directory and keeping track of branches/commits. # example command to run the latest version of NemaScan nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 # Note: can also write in one line, the \\ were used to make the code more readable: nextflow run andersenlab/nemascan --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv --vcf 20210121 Note Parameters or arguments to Nextflow scripts are designated with the -- . In the case above, there is a parameter called \"in\" which we are setting to be the test.tsv file. When Nextflow is running, it will print to the console the name of each process in the pipeline as well as update on the progress of the script: For example, in the above screenshot from a NemaScan run, there are 14 different processes in the pipeline. Notice that the fix_strain_names_bulk process has already completed! Meanwhile, the vcf_t_geno_matrix process is still running. You can also see that the prepare_gcta_files is actually run 4 times (in this case, because it is run once per 4 traits in my dataset). Another important piece of information from this Nextflow run is the hash that designates the working directory of each process. the [ad/eea615] next to the fix_strain_names_bulk process indicates that the working directory is located at path_to_nextflow_working_directory/ad/eea615... . This can be helpful if you want to go into that directory to see what is actually happening or trouble shoot errors. Note I highly recommend adding this function to your ~/.bash_profile to easily access the Nexflow working directory: gw() {cd /projects/b1042/AndersenLab/work/<your_name>/$1*} so that when you type gw 3f/6a21a5 (the hash Nextflow shows that indicates the specific working directory for a process) you will go to that folder automatically.","title":"Running Nextflow from a remote directory"},{"location":"quest-nextflow/#running_a_different_branch_of_a_pipeline","text":"Instead of cloning the repo and then remembering to switch branches, you can specify the branch in your nextflow call. This is most beneficial because the pipeline will then keep a record of which branch you used so future you doesn't have to remember. # the following command runs the \"develop\" branch of nemascan # (not recommended unless you know what you are doing, might break) nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r develop","title":"Running a different branch of a pipeline"},{"location":"quest-nextflow/#running_a_specific_commit","text":"Sometimes we want to keep the version of a pipeline the same across several different runs (maybe preparing for a manuscript etc.). To do this, you can again use the -r argument and just provide the commit ID which is a long alphanumeric code that can be found on github or in the log of your previous nextflow run. # the following command runs will always run the exact same code, no matter how many changes to nemascan there are in the future nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -r 769a2b75545d7f870a880447dd31482cfc628792","title":"Running a specific commit"},{"location":"quest-nextflow/#troubleshooting_remote_pipelines","text":"For some reason, running nextflow remotely sometimes throws errors and I am not 100% sure why. Here are some of the most common errors I see and how to fix them. 1. It doesn't pull the latest commit Go to github.com and find the latest commit ID and use that with -r XXX (see above on running specific commits) If that doesn't work, you might have to clone the repo and run it locally (see below) 2. It says the \"repository may be corrupt\" I don't understand this error, but I think it happens when you try to run different branches/commits. Usually, I just go to the path where the error says is corrupt and delete the folder with rm -rf . For example: rm -rf /home/kek973/.nextflow/assets/andersenlab/nemascan Warning You should always be careful when using rm , especially rm -rf . There is no going back. Make certain you want to delete the folder and everything inside it. In this case, it will be fine because nextflow will just clone it again fresh.","title":"Troubleshooting remote pipelines"},{"location":"quest-nextflow/#running_nextflow_from_a_local_directory","text":"Another way to run Nextflow is by first cloning the git repo to your directory and then running the pipeline. This has advantages and disadvantages over running the pipeline remotely (see below), however if you need to make changes to the pipeline specific to your analysis, you will need to follow these steps. git clone https://github.com/AndersenLab/NemaScan.git cd NemaScan nextflow run main.nf --debug","title":"Running Nextflow from a local directory"},{"location":"quest-nextflow/#resume","text":"Another great thing about Nextflow is that it caches all the processes that completed successfully, so if you have an error at the middle or end of your pipeline, you can re-run the same Nextflow command with -resume and it will start where it finished last time! nextflow run andersenlab/nemascan \\ --traitfile input_data/c_elegans/phenotypes/test_pheno.tsv \\ --vcf 20210121 \\ -resume There is usually no downside to adding -resume , so you can get into the habit of always adding it if you want. Important There is some confusion with how the -resume works and sometimes it doesn't work as expected. Check out this and this other guide for more help. One thing I've learned is there is a difference between process.cache = 'deep' vs. 'lenient' .","title":"Resume"},{"location":"quest-nextflow/#getting_an_email_or_text_when_complete","text":"If you would like to receive an email or text from Nextflow when your pipeline finishes (either successfully or with error), all you need to do is add -N <email> to your code. Most phone companies have a way to \"email\" your phone as an SMS, so you can use this email address to get a text alert. For example: # send email nextflow run andersenlab/nemascan --debug -N kathryn.evans@northwestern.edu # send text to 801-867-5309 with verizon nextflow run andersenlab/nemascan --debut -N 8018675309@vtext.com","title":"Getting an email or text when complete"},{"location":"quest-nextflow/#writing_nextflow_pipelines","text":"See this page for tips on how to get started with your own nextflow pipeline. Also check out the Nextflow documentation for help getting started!","title":"Writing Nextflow Pipelines"},{"location":"r/","text":"R \u00b6 R General R resources Andersen Lab R Packages cegwas2 linkagemapping COPASutils easysorter easyXpress easyFulcrum Using R on Quest General R resources \u00b6 If you are looking for some help getting started with R or taking your R to the next level, check out these useful resources: Swirl - interactive R learning Tidyverse workshop and resources Andersen Lab R Knowledge base & Cheatsheet R-bloggers - tips and tricks Using Rprojects to organize scripts Using workflowR for reproducible work Using Rmarkdown to generate reports Also check out the lab_code slack channel for help/questions! Andersen Lab R Packages \u00b6 The Andersen lab maintains several R packages useful for high-throughput data analysis. cegwas2 \u00b6 This package contains a set of functions to process phenotype data, perform GWAS, and perform post-mapping data processing for C. elegans . In 2019, the cegwas2-nf Nextflow pipeline was developed to perform GWA mapping on QUEST using this cegwas2 R package. However, mapping is rarely if never done with cegwas2 in R manually. To learn more about the cegwas2 R package, see the andersenlab/cegwas2 repo. For help running a GWA mapping using cegwas2, see cegwas2-nf or the dry guide Note cegwas2 was preceeded by cegwas and is now (as of 2021) superceeded by NemaScan linkagemapping \u00b6 This package includes all data and functions necessary to complete a mapping for the phenotype of your choice using the recombinant inbred lines from Andersen, et al. 2015 (G3) . Included with this package are the cross and map objects for this strain set as well a markers.rds file containing a lookup table for the physical positions of all markers used for mapping. To learn more about linkagemapping including how to install and use the package, check out the andersenlab/linkagemapping repo. Note Also check out the linkagemapping-nf repo for a reproducible Nextflow pipeline for linkage mapping and two-dimensional genome scans (scan2) for one or several traits. COPASutils \u00b6 The R package COPASutils provides a logical workflow for the reading, processing, and visualization of data obtained from the Union Biometrica Complex Object Parametric Analyzer and Sorter (COPAS) or the BioSorter large-particle flow cytometers. Data obtained from these powerful experimental platforms can be unwieldy, leading to difficulties in the ability to process and visualize the data using existing tools. Researchers studying small organisms, such as Caenorhabditis elegans, Anopheles gambiae, and Danio rerio, and using these devices will benefit from this streamlined and extensible R package. COPASutils offers a powerful suite of functions for the rapid processing and analysis of large high-throughput screening data sets. To learn more about COPASutils including how to install and use the package, check out the andersenlab/COPASutils repo and the COPASutils manuscript easysorter \u00b6 This package is effectively version 2 of the COPASutils package. This package is specialized for use with worms and includes additional functionality on top of that provided by COPASutils, including division of recorded objects by larval stage and the ability to regress out control phenotypes from those recorded in experimental conditions To learn more about easysorter including how to install and use the package, check out the andersenlab/easysorter repo. Here are some of the papers using easysorter : The first easysorter paper A Powerful New Quantitative Genetics Platform, Combining Caenorhabditis elegans High-Throughput Fitness Assays with a Large Collection of Recombinant Strains ( Andersen et al. 2015 ) The first \"V3\" easysorter paper The Gene scb-1 Underlies Variation in Caenorhabditis elegans Chemotherapeutic Responses ( Evans and Andersen 2020 ) The first dominance/hemizygosity easysorter paper A Novel Gene Underlies Bleomycin-Response Variation in Caenorhabditis elegans ( Brady et al. 2019 ) Almost every paper published from the lab has used easysorter, for more, check out our lab papers Note The easysorter package requires COPASutils installation as well. easyXpress \u00b6 This package is designed for the reading, processing, and visualization of images obtained from the Molecular Devices ImageExpress Nano Imager, and processed with CellProfiler's WormToolbox. To learn more about easyXpress including how to install and use the package, check out the andersenlab/easyXpress repo and the easyXpress manuscript . easyFulcrum \u00b6 This package is designed for processing and analyzing ecological sampling data generated using the Fulcrum mobile application. To learn more about how to use easyFulcrum, check out the andersenlab/easyFulcrum repo and the easyFulcrum manuscript . Using R on Quest \u00b6 Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. Fortunately, there are several ways to get around this issue: Docker container - instead of using conda environments (or maybe in addition to), a docker image can be generated with R packages installed. The benefit is that docker images can be used on Quest or locally and minimal set up is required. The downside is that on some occasions there can be errors generating the docker image with incompatible package versions. Library Paths - For several recent pipelines on Quest, we have gotten around using a docker container for R packages by installing the proper versions of R packages to a shared location ( /projects/b1059/software/R_lib_3.6.0 ). With the following code, any lab member can load the R package version from that location when running the pipeline, causing no errors, even if they don't have the package installed in their local R. # to manually install a package to this specific folder # make sure to first load the correct R version, our pipelines are currently using 3.6.0 module load R/3.6.0 # open R R # install package install.packages(\"tidyverse\", lib = \"/projects/b1059/software/R_lib_3.6.0\") # if you load the package normally, it won't work (unless you also have it installed in your path) library(tidyverse) # won't work # load the package from the specific folder library(tidyverse, lib.loc = \"/projects/b1059/software/R_lib_3.6.0\") # or add the path to your local R library to make for easier loading .libPaths(c(\"/projects/b1059/software/R_lib_3.6.0\", .libPaths() )) library(tidyverse) # works # you can add the following lines to a nextflow script to use these R packages # set a parameter for R library path in case it updates in the future params.R_libpath = \"/projects/b1059/software/R_lib_3.6.0\" # add the .libPaths to the top of the script dynamically (we don't want it statically in case someone wants to use the pipeline outside of quest) echo \".libPaths(c(\\\\\"${params.R_libpath}\\\\\", .libPaths() ))\" | cat - ${workflow.projectDir}/bin/Script.R > Script.R Rscript --vanilla Script.R Running Rstudio on QUEST The Quest Analytics Nodes allow users with active Quest allocations to use RStudio from a web browser. See Research Computing: Quest Analytics Nodes for an overview of the system. Go to the Rstudio browser ( make sure you are connected with VPN if you are off campus ). Log in with your netid and password just like you would on QUEST. Note The version of R on the Rstudio browser is currently 4.1.1, which is likely different from the version of R you run on QUEST. Therefore, you will need to re-install any packages you want to use in the browser. You can set the working directory with setwd(\"path_to_directory\") and then open and save files and data in Rstudio just like you were using it locally on your computer -- but with data and files on Quest!!","title":"R"},{"location":"r/#r","text":"R General R resources Andersen Lab R Packages cegwas2 linkagemapping COPASutils easysorter easyXpress easyFulcrum Using R on Quest","title":"R"},{"location":"r/#general_r_resources","text":"If you are looking for some help getting started with R or taking your R to the next level, check out these useful resources: Swirl - interactive R learning Tidyverse workshop and resources Andersen Lab R Knowledge base & Cheatsheet R-bloggers - tips and tricks Using Rprojects to organize scripts Using workflowR for reproducible work Using Rmarkdown to generate reports Also check out the lab_code slack channel for help/questions!","title":"General R resources"},{"location":"r/#andersen_lab_r_packages","text":"The Andersen lab maintains several R packages useful for high-throughput data analysis.","title":"Andersen Lab R Packages"},{"location":"r/#cegwas2","text":"This package contains a set of functions to process phenotype data, perform GWAS, and perform post-mapping data processing for C. elegans . In 2019, the cegwas2-nf Nextflow pipeline was developed to perform GWA mapping on QUEST using this cegwas2 R package. However, mapping is rarely if never done with cegwas2 in R manually. To learn more about the cegwas2 R package, see the andersenlab/cegwas2 repo. For help running a GWA mapping using cegwas2, see cegwas2-nf or the dry guide Note cegwas2 was preceeded by cegwas and is now (as of 2021) superceeded by NemaScan","title":"cegwas2"},{"location":"r/#linkagemapping","text":"This package includes all data and functions necessary to complete a mapping for the phenotype of your choice using the recombinant inbred lines from Andersen, et al. 2015 (G3) . Included with this package are the cross and map objects for this strain set as well a markers.rds file containing a lookup table for the physical positions of all markers used for mapping. To learn more about linkagemapping including how to install and use the package, check out the andersenlab/linkagemapping repo. Note Also check out the linkagemapping-nf repo for a reproducible Nextflow pipeline for linkage mapping and two-dimensional genome scans (scan2) for one or several traits.","title":"linkagemapping"},{"location":"r/#copasutils","text":"The R package COPASutils provides a logical workflow for the reading, processing, and visualization of data obtained from the Union Biometrica Complex Object Parametric Analyzer and Sorter (COPAS) or the BioSorter large-particle flow cytometers. Data obtained from these powerful experimental platforms can be unwieldy, leading to difficulties in the ability to process and visualize the data using existing tools. Researchers studying small organisms, such as Caenorhabditis elegans, Anopheles gambiae, and Danio rerio, and using these devices will benefit from this streamlined and extensible R package. COPASutils offers a powerful suite of functions for the rapid processing and analysis of large high-throughput screening data sets. To learn more about COPASutils including how to install and use the package, check out the andersenlab/COPASutils repo and the COPASutils manuscript","title":"COPASutils"},{"location":"r/#easysorter","text":"This package is effectively version 2 of the COPASutils package. This package is specialized for use with worms and includes additional functionality on top of that provided by COPASutils, including division of recorded objects by larval stage and the ability to regress out control phenotypes from those recorded in experimental conditions To learn more about easysorter including how to install and use the package, check out the andersenlab/easysorter repo. Here are some of the papers using easysorter : The first easysorter paper A Powerful New Quantitative Genetics Platform, Combining Caenorhabditis elegans High-Throughput Fitness Assays with a Large Collection of Recombinant Strains ( Andersen et al. 2015 ) The first \"V3\" easysorter paper The Gene scb-1 Underlies Variation in Caenorhabditis elegans Chemotherapeutic Responses ( Evans and Andersen 2020 ) The first dominance/hemizygosity easysorter paper A Novel Gene Underlies Bleomycin-Response Variation in Caenorhabditis elegans ( Brady et al. 2019 ) Almost every paper published from the lab has used easysorter, for more, check out our lab papers Note The easysorter package requires COPASutils installation as well.","title":"easysorter"},{"location":"r/#easyxpress","text":"This package is designed for the reading, processing, and visualization of images obtained from the Molecular Devices ImageExpress Nano Imager, and processed with CellProfiler's WormToolbox. To learn more about easyXpress including how to install and use the package, check out the andersenlab/easyXpress repo and the easyXpress manuscript .","title":"easyXpress"},{"location":"r/#easyfulcrum","text":"This package is designed for processing and analyzing ecological sampling data generated using the Fulcrum mobile application. To learn more about how to use easyFulcrum, check out the andersenlab/easyFulcrum repo and the easyFulcrum manuscript .","title":"easyFulcrum"},{"location":"r/#using_r_on_quest","text":"Unfortunately, R doesn't seem to work very well with conda environments, and making sure everyone has the same version of several different R packages (specifically Tidyverse) can be a nightmare for software development. Fortunately, there are several ways to get around this issue: Docker container - instead of using conda environments (or maybe in addition to), a docker image can be generated with R packages installed. The benefit is that docker images can be used on Quest or locally and minimal set up is required. The downside is that on some occasions there can be errors generating the docker image with incompatible package versions. Library Paths - For several recent pipelines on Quest, we have gotten around using a docker container for R packages by installing the proper versions of R packages to a shared location ( /projects/b1059/software/R_lib_3.6.0 ). With the following code, any lab member can load the R package version from that location when running the pipeline, causing no errors, even if they don't have the package installed in their local R. # to manually install a package to this specific folder # make sure to first load the correct R version, our pipelines are currently using 3.6.0 module load R/3.6.0 # open R R # install package install.packages(\"tidyverse\", lib = \"/projects/b1059/software/R_lib_3.6.0\") # if you load the package normally, it won't work (unless you also have it installed in your path) library(tidyverse) # won't work # load the package from the specific folder library(tidyverse, lib.loc = \"/projects/b1059/software/R_lib_3.6.0\") # or add the path to your local R library to make for easier loading .libPaths(c(\"/projects/b1059/software/R_lib_3.6.0\", .libPaths() )) library(tidyverse) # works # you can add the following lines to a nextflow script to use these R packages # set a parameter for R library path in case it updates in the future params.R_libpath = \"/projects/b1059/software/R_lib_3.6.0\" # add the .libPaths to the top of the script dynamically (we don't want it statically in case someone wants to use the pipeline outside of quest) echo \".libPaths(c(\\\\\"${params.R_libpath}\\\\\", .libPaths() ))\" | cat - ${workflow.projectDir}/bin/Script.R > Script.R Rscript --vanilla Script.R Running Rstudio on QUEST The Quest Analytics Nodes allow users with active Quest allocations to use RStudio from a web browser. See Research Computing: Quest Analytics Nodes for an overview of the system. Go to the Rstudio browser ( make sure you are connected with VPN if you are off campus ). Log in with your netid and password just like you would on QUEST. Note The version of R on the Rstudio browser is currently 4.1.1, which is likely different from the version of R you run on QUEST. Therefore, you will need to re-install any packages you want to use in the browser. You can set the working directory with setwd(\"path_to_directory\") and then open and save files and data in Rstudio just like you were using it locally on your computer -- but with data and files on Quest!!","title":"Using R on Quest"},{"location":"sharing-globus/","text":"Sharing files with Globus \u00b6 Sharing files with Globus Setting up a share Receiving a Globus share Setting up a share \u00b6 Sign in to Globus . The account doesn't have to be Northwestern-affiliated, but you will have to sign in with your Northwestern credentials to access Quest anyways, so might as well. In \"Collection\", search for \"Northwestern Quest\", then authenticate with your Northwestern account (if you didn't already in step 1). If you just authenticated, might need to search \"Northwestern Quest\" in the Collection again. Navigate to the folder you want to share, click \"Share\", and follow the prompts. Click \"Add Permissions - Share With\" and set permissions so that all users can read. Click \"Show link for sharing\", copy the link, and send to collaborators along with the link to this page walking them through downloading the files. Receiving a Globus share \u00b6 Click on the globus link you were sent, it should take you here: After you log in, the screen should look like this: If you have personal Globus endpoint installed already, skip to step 6 Click the search bar, then \"Install Globus Connect Personal\" Follow this page and prompts to install and set up personal endpoint. After installation, start Globus personal endpoint in Applications. Nothing will seem to happen, except a new icon will appear in the upper right corner of the computer (for Macs). Click on the icon and take note of the name of your personal end point. Click on the icon, then \"Preferences\". Only folders on this list can recieve files from Globus, so add new folders if necessary. Go back to the internet browser and click on \"File Manager\" at the top left. Click on the search bar on the right panel and type in the name of your personal end point (step), then select it. Select files you want to download from the left panel (they should look blue after selection), then navigate to the receiving folder on the right panel and select \"Start\" to begin transfer.","title":"Globus file sharing"},{"location":"sharing-globus/#sharing_files_with_globus","text":"Sharing files with Globus Setting up a share Receiving a Globus share","title":"Sharing files with Globus"},{"location":"sharing-globus/#setting_up_a_share","text":"Sign in to Globus . The account doesn't have to be Northwestern-affiliated, but you will have to sign in with your Northwestern credentials to access Quest anyways, so might as well. In \"Collection\", search for \"Northwestern Quest\", then authenticate with your Northwestern account (if you didn't already in step 1). If you just authenticated, might need to search \"Northwestern Quest\" in the Collection again. Navigate to the folder you want to share, click \"Share\", and follow the prompts. Click \"Add Permissions - Share With\" and set permissions so that all users can read. Click \"Show link for sharing\", copy the link, and send to collaborators along with the link to this page walking them through downloading the files.","title":"Setting up a share"},{"location":"sharing-globus/#receiving_a_globus_share","text":"Click on the globus link you were sent, it should take you here: After you log in, the screen should look like this: If you have personal Globus endpoint installed already, skip to step 6 Click the search bar, then \"Install Globus Connect Personal\" Follow this page and prompts to install and set up personal endpoint. After installation, start Globus personal endpoint in Applications. Nothing will seem to happen, except a new icon will appear in the upper right corner of the computer (for Macs). Click on the icon and take note of the name of your personal end point. Click on the icon, then \"Preferences\". Only folders on this list can recieve files from Globus, so add new folders if necessary. Go back to the internet browser and click on \"File Manager\" at the top left. Click on the search bar on the right panel and type in the name of your personal end point (step), then select it. Select files you want to download from the left panel (they should look blue after selection), then navigate to the receiving folder on the right panel and select \"Start\" to begin transfer.","title":"Receiving a Globus share"},{"location":"shiny/","text":"R Shiny applications \u00b6 R Shiny applications Andersen Lab Shiny Applications PCR calculator HTA Dilutions Fine-map QTL NILs NIL browser Linkagemapping analysis How to start a new shiny app? Simple example Reactivity Publishing your shiny app to shinyapps.io Shiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions. Andersen Lab Shiny Applications \u00b6 PCR calculator \u00b6 An R shiny web app developed for calculating PCR reagents. Link to application: here Link to github page with code and explanation of functionality: NA HTA Dilutions \u00b6 An R shiny web app developed to calculate drug dilutions for the high-throughput drug-response assays (sorter or imager). Link to application: here Link to github page with code and explanation of functionality: here Fine-map QTL NILs \u00b6 An R shiny web app developed to visualize the results from the high-throughput assays (specifically NIL results for fine-mapping a QTL). Link to application: here Link to github page with code and explanation of functionality: here NIL browser \u00b6 An R shiny web app developed to 1) visualize NIL genotypes and 2) find existing NILs for a project. Link to application: here Link to github page with code and explanation of functionality: NA Linkagemapping analysis \u00b6 An R shiny web app developed to visualize the results from the Andersen Lab linkagemapping experiments in 2014. Link to application: here Link to github page with code and explanation of functionality: here How to start a new shiny app? \u00b6 If you already know R, getting started in shiny just requires learning a few new concepts and some new functions/syntax. Check out this great tutorial to learn more (great video!)! Getting started You can create a shiny application in Rstudio by clicking File > New File > Shiny Web App . Rstudio will install any packages necessary (like shiny ) and then ask if you want your application to be in one file called app.R or two files: ui.R and server.R . Either way is okay. If you have a large, complex application, it might be easier to split up the UI (user interface) and the server (the meat of the application). Just like with Rmarkdown, a new Rshiny application comes preloaded with some basic code for you to get a look at. To run a shiny application from Rstudio, simply press the green \"run application\" button in the upper right of the script panel. A new window should pop up with the application. You can also choose at this point to \"open in browser\" instead by selecting that button in the upper right of the shiny app. You now have the beginning of a shiny app! You can add new visual elements to your application in the ui.R or in the shiny::ui() function in app.R and you can create the objects, plots, and manipulate data in the server . Here are some of the most commonly used objects (for a full list, check out this page ): ** Inputs You can create an input using one of the following functions in the ui , then use that input in the sever with input$input_id * shiny::radioButtons() * shiny::textInput() * shiny::selectInput() * shiny::sliderInput() * shiny::fileInput() * shiny::actionButton() Outputs You must create an output in the server and then display that output in the ui : | Create ouput in server | Show output in UI | | --- | --- | | shiny::renderPlot() | shiny::plotOutput() | | shiny::renderTable() | shiny::tableOutput() | | shiny::renderUI() | shiny::uiOutput() | | shiny::renderText() | shiny::textOutput() | Simple example \u00b6 library(shiny) # Define UI for application that draws a histogram ui <- shiny::fluidPage( # Application title shiny::titlePanel(\"Old Faithful Geyser Data\"), # Sidebar with a slider input for number of bins shiny::sidebarLayout( shiny::sidebarPanel( shiny::sliderInput(\"bins\", \"Number of bins:\", min = 1, max = 50, value = 30) ), # Show a plot of the generated distribution shiny::mainPanel( shiny::plotOutput(\"distPlot\") ) ) ) # Define server logic required to draw a histogram server <- function(input, output) { output$distPlot <- shiny::renderPlot({ # generate bins based on input$bins from ui.R x <- faithful[, 2] bins <- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = 'darkgray', border = 'white') }) } # Run the application shiny::shinyApp(ui = ui, server = server) Reactivity \u00b6 Check out this great overview on reactivity , the cornerstone of shiny applications. It talks about how the inputs are related to outputs and when outputs update in response to inputs. Publishing your shiny app to shinyapps.io \u00b6 After testing your new shiny app in Rstudio, you might be ready to deploy to the web for other people to access! The Andersen Lab has their own shinyapps.io account, so if you are making a lab-related app it is best to use this account (for login details, ask Robyn!) Publishing is simple - press the \"publish\" button in the upper right-hand corner of your running application and follow the prompts to select the right account. Once published, your application will be available at https://andersen-lab.shinyapps.io/{your_app_name}","title":"R Shiny"},{"location":"shiny/#r_shiny_applications","text":"R Shiny applications Andersen Lab Shiny Applications PCR calculator HTA Dilutions Fine-map QTL NILs NIL browser Linkagemapping analysis How to start a new shiny app? Simple example Reactivity Publishing your shiny app to shinyapps.io Shiny is an R package that makes it easy to build interactive web apps straight from R. You can host standalone apps on a webpage or embed them in R Markdown documents or build dashboards. You can also extend your Shiny apps with CSS themes, htmlwidgets, and JavaScript actions.","title":"R Shiny applications"},{"location":"shiny/#andersen_lab_shiny_applications","text":"","title":"Andersen Lab Shiny Applications"},{"location":"shiny/#pcr_calculator","text":"An R shiny web app developed for calculating PCR reagents. Link to application: here Link to github page with code and explanation of functionality: NA","title":"PCR calculator"},{"location":"shiny/#hta_dilutions","text":"An R shiny web app developed to calculate drug dilutions for the high-throughput drug-response assays (sorter or imager). Link to application: here Link to github page with code and explanation of functionality: here","title":"HTA Dilutions"},{"location":"shiny/#fine-map_qtl_nils","text":"An R shiny web app developed to visualize the results from the high-throughput assays (specifically NIL results for fine-mapping a QTL). Link to application: here Link to github page with code and explanation of functionality: here","title":"Fine-map QTL NILs"},{"location":"shiny/#nil_browser","text":"An R shiny web app developed to 1) visualize NIL genotypes and 2) find existing NILs for a project. Link to application: here Link to github page with code and explanation of functionality: NA","title":"NIL browser"},{"location":"shiny/#linkagemapping_analysis","text":"An R shiny web app developed to visualize the results from the Andersen Lab linkagemapping experiments in 2014. Link to application: here Link to github page with code and explanation of functionality: here","title":"Linkagemapping analysis"},{"location":"shiny/#how_to_start_a_new_shiny_app","text":"If you already know R, getting started in shiny just requires learning a few new concepts and some new functions/syntax. Check out this great tutorial to learn more (great video!)! Getting started You can create a shiny application in Rstudio by clicking File > New File > Shiny Web App . Rstudio will install any packages necessary (like shiny ) and then ask if you want your application to be in one file called app.R or two files: ui.R and server.R . Either way is okay. If you have a large, complex application, it might be easier to split up the UI (user interface) and the server (the meat of the application). Just like with Rmarkdown, a new Rshiny application comes preloaded with some basic code for you to get a look at. To run a shiny application from Rstudio, simply press the green \"run application\" button in the upper right of the script panel. A new window should pop up with the application. You can also choose at this point to \"open in browser\" instead by selecting that button in the upper right of the shiny app. You now have the beginning of a shiny app! You can add new visual elements to your application in the ui.R or in the shiny::ui() function in app.R and you can create the objects, plots, and manipulate data in the server . Here are some of the most commonly used objects (for a full list, check out this page ): ** Inputs You can create an input using one of the following functions in the ui , then use that input in the sever with input$input_id * shiny::radioButtons() * shiny::textInput() * shiny::selectInput() * shiny::sliderInput() * shiny::fileInput() * shiny::actionButton() Outputs You must create an output in the server and then display that output in the ui : | Create ouput in server | Show output in UI | | --- | --- | | shiny::renderPlot() | shiny::plotOutput() | | shiny::renderTable() | shiny::tableOutput() | | shiny::renderUI() | shiny::uiOutput() | | shiny::renderText() | shiny::textOutput() |","title":"How to start a new shiny app?"},{"location":"shiny/#simple_example","text":"library(shiny) # Define UI for application that draws a histogram ui <- shiny::fluidPage( # Application title shiny::titlePanel(\"Old Faithful Geyser Data\"), # Sidebar with a slider input for number of bins shiny::sidebarLayout( shiny::sidebarPanel( shiny::sliderInput(\"bins\", \"Number of bins:\", min = 1, max = 50, value = 30) ), # Show a plot of the generated distribution shiny::mainPanel( shiny::plotOutput(\"distPlot\") ) ) ) # Define server logic required to draw a histogram server <- function(input, output) { output$distPlot <- shiny::renderPlot({ # generate bins based on input$bins from ui.R x <- faithful[, 2] bins <- seq(min(x), max(x), length.out = input$bins + 1) # draw the histogram with the specified number of bins hist(x, breaks = bins, col = 'darkgray', border = 'white') }) } # Run the application shiny::shinyApp(ui = ui, server = server)","title":"Simple example"},{"location":"shiny/#reactivity","text":"Check out this great overview on reactivity , the cornerstone of shiny applications. It talks about how the inputs are related to outputs and when outputs update in response to inputs.","title":"Reactivity"},{"location":"shiny/#publishing_your_shiny_app_to_shinyappsio","text":"After testing your new shiny app in Rstudio, you might be ready to deploy to the web for other people to access! The Andersen Lab has their own shinyapps.io account, so if you are making a lab-related app it is best to use this account (for login details, ask Robyn!) Publishing is simple - press the \"publish\" button in the upper right-hand corner of your running application and follow the prompts to select the right account. Once published, your application will be available at https://andersen-lab.shinyapps.io/{your_app_name}","title":"Publishing your shiny app to shinyapps.io"},{"location":"sra/","text":"Uploading WI FASTQ sequence data to SRA \u00b6 For each CENDR release, it is important to also upload the FASTQ files to NCBI's Sequence Read Archive (SRA). If a bioproject already exists, you can create a new submission and link to the previous bioproject . If there is no previous bioproject, you can create a new bioproject and add all relevant data. See below for more instructions. SRA submission \u00b6 Begin submission In the SRA submission portal , click the button for \"New submission\" and follow the prompts. Remember to add the previous bioproject ID if applicable to link this submission to previous submissions. Select \"Model organism or animal\" for biosample type, select \"upload file using excel\" and download the template Create biosample sheet (an example can be found below) An easy starting point here is the sample sheet used for alignment-nf . You will keep the id as the sample name (unique identifier) and strain (note strain also needs to be unique - if there are multiple library preps for the same strain, please append \"-2\" etc. to the strain) To these two columns, you can add bioproject, organism, developmental stage, sex, and tissue (same across all strains) Finally, join this data with the WI species master sheet to get collected by, collection date, and latitude/longitude. - Note: latitude/longitude need to be converted into one shared column in the format \"34.89 S 56.15 W\" (+ refers to North and East) Copy the data into the relevent columns in the template , save, and upload to the submission portal Create SRA metadata sheet Again, select \"upload a file using excel\" and download the template An easy starting point here is, again, the sample sheet used for alignment-nf . You will keep the id as the sample name and the lb as library id. You will also keep fq1 and fq2 for filename and filename2. You will then add the rest of the columns as shown below. Note: the formatting is very specific for this sheet. The title can be found on the bioproject page. The instrument_model can be found by using the \"sequencing_folder\" (not shown, but part of the original sample sheet) and looking up the instrument that folder was run on in the Sequencing Runs google sheet ( here ) Copy and paste the rows from this file into the template to check for correct formatting. Then save the tab as a tsv and upload to the submission portal. Pre-upload FASTQ files using FTP Create a list of files to upload to the FTP server by combining the filename1 and filename2 from the SRA metadata sheet (above). Begin submission by creating an NCBI account (or signing in -- personal account). Then follow the link to the SRA submission portal Follow the instructions under the \"FTP upload\": # establish FTP connection from terminal (on QUEST!) # ftp <address> ftp ftp-private.ncbi.nlm.nih.gov # navigate to your account folder (i.e.) cd uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4 # create new folder for submission (i.e.) mkdir 20210121_submission # exit FTP connection exit # back on quest, run the following line to transfer every file with path listed in \"files_to_upload.tsv\" to that folder # make sure to change your upload folder and files to upload module load parallel parallel --verbose lftp -e \\\"put -O /uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4/20210121_submission {}\\; bye\\\" -u subftp,w4pYB9VQ ftp-private.ncbi.nlm.nih.gov < files_to_upload.tsv Note: it is important that this step is completely finished before you complete your SRA submission Complete submission When finished, in the SRA portal you will be asked to select which folder you want to pull files from Review and submit! If there are any issues they will let you know. List of current bioprojects associated with the Andersen Lab \u00b6 C. elegans WI genome FASTQ - PRJNA549503 (link here )","title":"Uploading WI FASTQ sequence data to SRA"},{"location":"sra/#uploading_wi_fastq_sequence_data_to_sra","text":"For each CENDR release, it is important to also upload the FASTQ files to NCBI's Sequence Read Archive (SRA). If a bioproject already exists, you can create a new submission and link to the previous bioproject . If there is no previous bioproject, you can create a new bioproject and add all relevant data. See below for more instructions.","title":"Uploading WI FASTQ sequence data to SRA"},{"location":"sra/#sra_submission","text":"Begin submission In the SRA submission portal , click the button for \"New submission\" and follow the prompts. Remember to add the previous bioproject ID if applicable to link this submission to previous submissions. Select \"Model organism or animal\" for biosample type, select \"upload file using excel\" and download the template Create biosample sheet (an example can be found below) An easy starting point here is the sample sheet used for alignment-nf . You will keep the id as the sample name (unique identifier) and strain (note strain also needs to be unique - if there are multiple library preps for the same strain, please append \"-2\" etc. to the strain) To these two columns, you can add bioproject, organism, developmental stage, sex, and tissue (same across all strains) Finally, join this data with the WI species master sheet to get collected by, collection date, and latitude/longitude. - Note: latitude/longitude need to be converted into one shared column in the format \"34.89 S 56.15 W\" (+ refers to North and East) Copy the data into the relevent columns in the template , save, and upload to the submission portal Create SRA metadata sheet Again, select \"upload a file using excel\" and download the template An easy starting point here is, again, the sample sheet used for alignment-nf . You will keep the id as the sample name and the lb as library id. You will also keep fq1 and fq2 for filename and filename2. You will then add the rest of the columns as shown below. Note: the formatting is very specific for this sheet. The title can be found on the bioproject page. The instrument_model can be found by using the \"sequencing_folder\" (not shown, but part of the original sample sheet) and looking up the instrument that folder was run on in the Sequencing Runs google sheet ( here ) Copy and paste the rows from this file into the template to check for correct formatting. Then save the tab as a tsv and upload to the submission portal. Pre-upload FASTQ files using FTP Create a list of files to upload to the FTP server by combining the filename1 and filename2 from the SRA metadata sheet (above). Begin submission by creating an NCBI account (or signing in -- personal account). Then follow the link to the SRA submission portal Follow the instructions under the \"FTP upload\": # establish FTP connection from terminal (on QUEST!) # ftp <address> ftp ftp-private.ncbi.nlm.nih.gov # navigate to your account folder (i.e.) cd uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4 # create new folder for submission (i.e.) mkdir 20210121_submission # exit FTP connection exit # back on quest, run the following line to transfer every file with path listed in \"files_to_upload.tsv\" to that folder # make sure to change your upload folder and files to upload module load parallel parallel --verbose lftp -e \\\"put -O /uploads/kathrynevans2015_u.northwestern.edu_YSlKSXQ4/20210121_submission {}\\; bye\\\" -u subftp,w4pYB9VQ ftp-private.ncbi.nlm.nih.gov < files_to_upload.tsv Note: it is important that this step is completely finished before you complete your SRA submission Complete submission When finished, in the SRA portal you will be asked to select which folder you want to pull files from Review and submit! If there are any issues they will let you know.","title":"SRA submission"},{"location":"sra/#list_of_current_bioprojects_associated_with_the_andersen_lab","text":"C. elegans WI genome FASTQ - PRJNA549503 (link here )","title":"List of current bioprojects associated with the Andersen Lab"},{"location":"travis-ci/","text":"Setting up Quest \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Travis ci"},{"location":"travis-ci/#setting_up_quest","text":"For full documentation visit mkdocs.org .","title":"Setting up Quest"},{"location":"travis-ci/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"travis-ci/#project_layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"writing-nextflow/","text":"Writing Nextflow pipelines \u00b6 Check out the Nextflow documentation for help getting started! Note Learning to script with Nextflow definitely has a high learning curve. Don't get discouraged! Start with something small and simple. Maybe convert a current script you have that uses a large for loop into a nextflow pipeline to start getting the hang of things! When should my pipeline be a Nextflow script? \u00b6 Not every analysis needs to be a Nextflow pipeline. For smaller analyses (especially those with few inputs) it might be easier to write a bash/shell script. However, there are many advantages to Nextflow: When you are running many parallel tasks Nextflow takes care of all the job submissions and is really good at running the same basic script across 6 chromosomes, 500 strains, 1000 permutations, whatever you need! When your analysis consists of several different steps that are either sequential and/or can be run simultaneously. Because Nextflow is great at parallelization, it knows which steps rely on other steps and can speed up your script by running independent steps in parallel. Also, you can take advantage of the -resume function for scripts that take a long time to run because Nextflow caches results (which means if there is an error you can fix it but you don't have to start over from the begining!) You want to be able to easily run your script on different computing platforms (i.e. QUEST, local machine, GCP...) This can be done by creating different profiles for each platform. Check out the nextflow documentation on profiles. The basics \u00b6 Channels All input and output files in Nextflow are piped through \"channels\". You can create a channel with an input file or parameter and then feed these channels to a \"process\" (or step). Channels can be merged, split, or rearranged. Check out the Nextflow documentation for channels to learn more. Also check out this cheatsheet for useful operators. Miscellaneous channels tips: Channel.from(\"A.txt\") will put A.txt as is into the channel Channel.fromPath(\"A.txt\") will add a full path (usually current directory) and put /path/A.txt into the channel. Channel.fromPath(\"folder/A.txt\") will add a full path (usually current directory) and put /path/folder/A.txt into the channel. Channel.fromPath(\"/path/A.txt\") will put /path/A.txt into the channel. In other words, Channel.fromPath will only add a full path if there isn't already one and ensure there is always a full path in the resulting channel. This goes hand in hand with input: path(\"A.txt\") inside the process, where Nextflow actually creates a symlink named A.txt (note the path from first / to last / is stripped) linking to /path/A.txt in the working directory , so it can be accessed within the working directory by the script cat A.txt without specifying a path. Processes Each chunk of code in a nextflow script is broken up into distinct \"processes\" that you can think of as \"steps\" or even small/large \"functions\". A process can have one line of code or hundreds. It can be done in bash or by running an R or python script. An example process is shown below: # first process in linkagemapping-nf process split_pheno { input: file('infile') output: file('*.tsv') \"\"\" Rscript --vanilla ${workflow.projectDir}/bin/split_pheno.R ${infile} ${params.thresh} \"\"\" } Each process is defined by process <name> {} and has three basic parts: 1) input, 2) output, 3) script. You can dictate the pipeline by generating a workflow that acts like a protocol or recipe for Nextflow: which inputs go to which process and what order to do things in. Note, this is different than in DSL1, which is not actively used anymore . To learn more about processes, check out the nextflow docs . # example of a simple workflow (at the top of a nextflow script) workflow { # create a channel from the input file and give it to the split_pheno process Channel.fromPath(params.in) | split_pheno # take the output from split_pheno and \"flatten it\" and send to the mapping process split_pheno.out.flatten() | mapping } Miscellaneous notes on input paths for process: With input: path(\"A.txt\") one can refer to the file in the script as A.txt . Side note A.txt doesn't have to be the same name as in channel creation, it can be anything, input: path(\"B.txt\") , input: path(\"n\") etc. With input: path(A) one can refer to the file in the script as $A , and the value of $A will be the original file name (without path, see section above). input: path(\"A.txt\") and input: path \"A.txt\" generally both work. Occasionally had errors that required the following (tip from @danielecook ): If not in a tuple, use input: path \"A.txt\" If in a tuple, use input: tuple path(\"A.txt\"), path(\"B.txt\") This goes the same for output . From @pditommaso : path(A) is almost the same as file(A) , however the first interprets a value of type string as the input file path (ie the location in the file system where it's stored), the latter interprets a value of type string and materialise it to a temporary files. It's recommended the use of path since it's less ambiguous and fits better in most use-cases. The working directory One of the main distinctions of Nextflow is that each execution of a process happens in its own temporary working directory. This is important for several reasons: You do not need to name temporary files dynamically (i.e. with strain or trait name) to avoid overwriting files, because you can repeat the same process with a different trait in a different directory. This means you can call a temporary file strain.bam instead of DL238.bam and CB4856.bam . This can make for simpler coding However, if you provide strain name as a value in the input channel, it is easy to name files dynamically with ${strain}.bam which will output DL238.bam or CB4856.bam If there is an error, you can go into the working directory to see all input and output files (sometimes in the form of symlinks) for that specific process. You can also find the script ( .command.sh ) that was run and try to reproduce the error manually. If there was an error, the message is recorded in errlog.txt If there is an error, the Nextflow error output will point you to the working directory for that specific process and might look something like /projects/b1042/AndersenLab/work/katie/4c/4d9c3b333734a5b63d66f0bc0cfcdc You can also find the working directory from the hash shown next to a running/completed process. For example [4c/4d9c3b] corresponds to the working directory above. See the running nextflow page for creating a function to automatically cd into the working directory given that hash. You can also find the working directory in the .nextflow.log file or in the report.html if one is generated. Miscellaneous tips on the working directory: Note that with publishDir \"path\", mode: 'move' , the output file will be moved outside of the working directory and Nextflow will not be able to use it as input for another process, so only use it when there is not a following process that uses the output file. Be mindful that if the \"\"\" (script section) \"\"\" involves changing directory, such as cd or rmarkdown::render( knit_root_dir = \"folder/\" ) , Nextflow will still only search the working directory for output files. Run nextflow clean -f in the excecution folder to clean up the working directories. In Nextflow scripts (.nf files), one can use: ${workflow.projectDir} to refer where the project locates (usually the folder of main.nf). For example: publishDir \"${workflow.projectDir}/output\", mode: 'copy' or Rscript ${workflow.projectDir}/bin/task.R . ${workflow.launchDir} to refer to where the script is called from. $baseDir usually refers to the same folder as ${workflow.projectDir} but it can also be used in the config file, where ${workflow.projectDir} and ${workflow.launchDir} are not accessible. They are much more reliable than $PWD or $pwd . Note The standard name of a nextflow script is main.nf but it doesn't have to be! If you just call nextflow run andersenlab/nemascan it will automatically choose the main.nf script. It is best practice to always write out the script name though Debugging with print - To print a channel, use .view() . It's especially useful to resolve WARN: Input tuple does not match input set cardinality declared by process . (Don't forget to remove .view() after debugging) channel_vcf .combine(channel_index) .combine(channel_chr) .view() To print from the script section inside the processes, add echo true . process test { echo true // this will print the stdout from the script section on Terminal input: path(vcf) \"\"\" head $vcf \"\"\" } Notes on transition to DSL2 If you are new to nextflow or don't know anything about DSL1 or DSL2, you can disregard this section and use DSL2 syntax! - Moving to DSL2 is a one-way street. It's so intuitive with clean and readable code. - In DSL1, each queue channel can only be used once. - In DSL2, a channel can be fed into multiple processes - In DSL2, each process can only be called once. The solution is either .concat() the input channels so they run as parallel processes, or put the process in a module and import multiple times from the module. (One may be able to call a process in different workflows, haven't tested yet). - DSL2 also enforces that all inputs needs to be combined into 1 channel before it goes into a process. See the cheatsheet for useful operators. - Simple steps to convert from original syntax to DSL2 - Deprecated operators . Run reports nextflow main.nf -with-report -with-timeline -with-dag -with-report Nextflow html report contains resource usage for each process, and details (most useful being the status and working directory) for each process -with-timeline How much wait time and run time each process took for the run. Very useful reference for optimizing resource allocation and improving run time. -with-dag Make a flowchart to show the relationship of channels and processes. Software dependencies to use these features. Note the differences on Mac and Linux. Or, set this up in the nextflow.config file for a pipeline to ensure they are generated each time the script is run: import java.time.* Date now = new Date() params { tracedir = \"pipeline_info\" timestamp = now.format(\"yyyyMMdd-HH-mm-ss\") } timeline { enabled = true file = \"${params.tracedir}/${params.timestamp}_timeline.html\" } report { enabled = true file = \"${params.tracedir}/${params.timestamp}_report.html\" } How to require users to sepcify a parameter value There are 2 types of paramters: (a) one with no actual value (b) one with actual values. (a) If a parameter is specified but no value is given, it is implicitly considered true . So one can use this to run debug mode nextflow main.nf --debug if (params.debug) { ... (set parameters for debug mode) } else { ... (set parameters for normal use) } or to print help message nextflow main.nf --help if (params.help) { println \"\"\" ... (help msg here) \"\"\" exit 0 } (b) For parameters that need to contain a value, Nextflow recommends to set a default and let users to overwrite it as needed. However, if you want to require it to be specified by the user: params.reference = null // no quotes. this line is optional, since without initialising the parameter it will default to null. if (params.reference == null) error \"Please specify a reference genome with --reference\" Below works as long as the user always append a value: --reference=something . It will not print the error message with: nextflow main.nf --reference (without specifying a value) because this will set params.reference to true (see point (a) ) and !params.reference will be false . if (!params.reference) error \"Please specify a reference genome with --reference\" Resources \u00b6 Nextflow documentation Nextflow cheatsheet Nextflow gitter Awesome Nextflow pipeline examples - Repository of great nextflow pipelines. Official Nextflow patterns Google group","title":"Writing Nextflow pipelines"},{"location":"writing-nextflow/#writing_nextflow_pipelines","text":"Check out the Nextflow documentation for help getting started! Note Learning to script with Nextflow definitely has a high learning curve. Don't get discouraged! Start with something small and simple. Maybe convert a current script you have that uses a large for loop into a nextflow pipeline to start getting the hang of things!","title":"Writing Nextflow pipelines"},{"location":"writing-nextflow/#when_should_my_pipeline_be_a_nextflow_script","text":"Not every analysis needs to be a Nextflow pipeline. For smaller analyses (especially those with few inputs) it might be easier to write a bash/shell script. However, there are many advantages to Nextflow: When you are running many parallel tasks Nextflow takes care of all the job submissions and is really good at running the same basic script across 6 chromosomes, 500 strains, 1000 permutations, whatever you need! When your analysis consists of several different steps that are either sequential and/or can be run simultaneously. Because Nextflow is great at parallelization, it knows which steps rely on other steps and can speed up your script by running independent steps in parallel. Also, you can take advantage of the -resume function for scripts that take a long time to run because Nextflow caches results (which means if there is an error you can fix it but you don't have to start over from the begining!) You want to be able to easily run your script on different computing platforms (i.e. QUEST, local machine, GCP...) This can be done by creating different profiles for each platform. Check out the nextflow documentation on profiles.","title":"When should my pipeline be a Nextflow script?"},{"location":"writing-nextflow/#the_basics","text":"Channels All input and output files in Nextflow are piped through \"channels\". You can create a channel with an input file or parameter and then feed these channels to a \"process\" (or step). Channels can be merged, split, or rearranged. Check out the Nextflow documentation for channels to learn more. Also check out this cheatsheet for useful operators. Miscellaneous channels tips: Channel.from(\"A.txt\") will put A.txt as is into the channel Channel.fromPath(\"A.txt\") will add a full path (usually current directory) and put /path/A.txt into the channel. Channel.fromPath(\"folder/A.txt\") will add a full path (usually current directory) and put /path/folder/A.txt into the channel. Channel.fromPath(\"/path/A.txt\") will put /path/A.txt into the channel. In other words, Channel.fromPath will only add a full path if there isn't already one and ensure there is always a full path in the resulting channel. This goes hand in hand with input: path(\"A.txt\") inside the process, where Nextflow actually creates a symlink named A.txt (note the path from first / to last / is stripped) linking to /path/A.txt in the working directory , so it can be accessed within the working directory by the script cat A.txt without specifying a path. Processes Each chunk of code in a nextflow script is broken up into distinct \"processes\" that you can think of as \"steps\" or even small/large \"functions\". A process can have one line of code or hundreds. It can be done in bash or by running an R or python script. An example process is shown below: # first process in linkagemapping-nf process split_pheno { input: file('infile') output: file('*.tsv') \"\"\" Rscript --vanilla ${workflow.projectDir}/bin/split_pheno.R ${infile} ${params.thresh} \"\"\" } Each process is defined by process <name> {} and has three basic parts: 1) input, 2) output, 3) script. You can dictate the pipeline by generating a workflow that acts like a protocol or recipe for Nextflow: which inputs go to which process and what order to do things in. Note, this is different than in DSL1, which is not actively used anymore . To learn more about processes, check out the nextflow docs . # example of a simple workflow (at the top of a nextflow script) workflow { # create a channel from the input file and give it to the split_pheno process Channel.fromPath(params.in) | split_pheno # take the output from split_pheno and \"flatten it\" and send to the mapping process split_pheno.out.flatten() | mapping } Miscellaneous notes on input paths for process: With input: path(\"A.txt\") one can refer to the file in the script as A.txt . Side note A.txt doesn't have to be the same name as in channel creation, it can be anything, input: path(\"B.txt\") , input: path(\"n\") etc. With input: path(A) one can refer to the file in the script as $A , and the value of $A will be the original file name (without path, see section above). input: path(\"A.txt\") and input: path \"A.txt\" generally both work. Occasionally had errors that required the following (tip from @danielecook ): If not in a tuple, use input: path \"A.txt\" If in a tuple, use input: tuple path(\"A.txt\"), path(\"B.txt\") This goes the same for output . From @pditommaso : path(A) is almost the same as file(A) , however the first interprets a value of type string as the input file path (ie the location in the file system where it's stored), the latter interprets a value of type string and materialise it to a temporary files. It's recommended the use of path since it's less ambiguous and fits better in most use-cases. The working directory One of the main distinctions of Nextflow is that each execution of a process happens in its own temporary working directory. This is important for several reasons: You do not need to name temporary files dynamically (i.e. with strain or trait name) to avoid overwriting files, because you can repeat the same process with a different trait in a different directory. This means you can call a temporary file strain.bam instead of DL238.bam and CB4856.bam . This can make for simpler coding However, if you provide strain name as a value in the input channel, it is easy to name files dynamically with ${strain}.bam which will output DL238.bam or CB4856.bam If there is an error, you can go into the working directory to see all input and output files (sometimes in the form of symlinks) for that specific process. You can also find the script ( .command.sh ) that was run and try to reproduce the error manually. If there was an error, the message is recorded in errlog.txt If there is an error, the Nextflow error output will point you to the working directory for that specific process and might look something like /projects/b1042/AndersenLab/work/katie/4c/4d9c3b333734a5b63d66f0bc0cfcdc You can also find the working directory from the hash shown next to a running/completed process. For example [4c/4d9c3b] corresponds to the working directory above. See the running nextflow page for creating a function to automatically cd into the working directory given that hash. You can also find the working directory in the .nextflow.log file or in the report.html if one is generated. Miscellaneous tips on the working directory: Note that with publishDir \"path\", mode: 'move' , the output file will be moved outside of the working directory and Nextflow will not be able to use it as input for another process, so only use it when there is not a following process that uses the output file. Be mindful that if the \"\"\" (script section) \"\"\" involves changing directory, such as cd or rmarkdown::render( knit_root_dir = \"folder/\" ) , Nextflow will still only search the working directory for output files. Run nextflow clean -f in the excecution folder to clean up the working directories. In Nextflow scripts (.nf files), one can use: ${workflow.projectDir} to refer where the project locates (usually the folder of main.nf). For example: publishDir \"${workflow.projectDir}/output\", mode: 'copy' or Rscript ${workflow.projectDir}/bin/task.R . ${workflow.launchDir} to refer to where the script is called from. $baseDir usually refers to the same folder as ${workflow.projectDir} but it can also be used in the config file, where ${workflow.projectDir} and ${workflow.launchDir} are not accessible. They are much more reliable than $PWD or $pwd . Note The standard name of a nextflow script is main.nf but it doesn't have to be! If you just call nextflow run andersenlab/nemascan it will automatically choose the main.nf script. It is best practice to always write out the script name though Debugging with print - To print a channel, use .view() . It's especially useful to resolve WARN: Input tuple does not match input set cardinality declared by process . (Don't forget to remove .view() after debugging) channel_vcf .combine(channel_index) .combine(channel_chr) .view() To print from the script section inside the processes, add echo true . process test { echo true // this will print the stdout from the script section on Terminal input: path(vcf) \"\"\" head $vcf \"\"\" } Notes on transition to DSL2 If you are new to nextflow or don't know anything about DSL1 or DSL2, you can disregard this section and use DSL2 syntax! - Moving to DSL2 is a one-way street. It's so intuitive with clean and readable code. - In DSL1, each queue channel can only be used once. - In DSL2, a channel can be fed into multiple processes - In DSL2, each process can only be called once. The solution is either .concat() the input channels so they run as parallel processes, or put the process in a module and import multiple times from the module. (One may be able to call a process in different workflows, haven't tested yet). - DSL2 also enforces that all inputs needs to be combined into 1 channel before it goes into a process. See the cheatsheet for useful operators. - Simple steps to convert from original syntax to DSL2 - Deprecated operators . Run reports nextflow main.nf -with-report -with-timeline -with-dag -with-report Nextflow html report contains resource usage for each process, and details (most useful being the status and working directory) for each process -with-timeline How much wait time and run time each process took for the run. Very useful reference for optimizing resource allocation and improving run time. -with-dag Make a flowchart to show the relationship of channels and processes. Software dependencies to use these features. Note the differences on Mac and Linux. Or, set this up in the nextflow.config file for a pipeline to ensure they are generated each time the script is run: import java.time.* Date now = new Date() params { tracedir = \"pipeline_info\" timestamp = now.format(\"yyyyMMdd-HH-mm-ss\") } timeline { enabled = true file = \"${params.tracedir}/${params.timestamp}_timeline.html\" } report { enabled = true file = \"${params.tracedir}/${params.timestamp}_report.html\" } How to require users to sepcify a parameter value There are 2 types of paramters: (a) one with no actual value (b) one with actual values. (a) If a parameter is specified but no value is given, it is implicitly considered true . So one can use this to run debug mode nextflow main.nf --debug if (params.debug) { ... (set parameters for debug mode) } else { ... (set parameters for normal use) } or to print help message nextflow main.nf --help if (params.help) { println \"\"\" ... (help msg here) \"\"\" exit 0 } (b) For parameters that need to contain a value, Nextflow recommends to set a default and let users to overwrite it as needed. However, if you want to require it to be specified by the user: params.reference = null // no quotes. this line is optional, since without initialising the parameter it will default to null. if (params.reference == null) error \"Please specify a reference genome with --reference\" Below works as long as the user always append a value: --reference=something . It will not print the error message with: nextflow main.nf --reference (without specifying a value) because this will set params.reference to true (see point (a) ) and !params.reference will be false . if (!params.reference) error \"Please specify a reference genome with --reference\"","title":"The basics"},{"location":"writing-nextflow/#resources","text":"Nextflow documentation Nextflow cheatsheet Nextflow gitter Awesome Nextflow pipeline examples - Repository of great nextflow pipelines. Official Nextflow patterns Google group","title":"Resources"},{"location":"archive/quest-andersen-lab-env/","text":"The Andersen Lab Software Environment (no longer supported) \u00b6 The Andersen Lab Software Environment (no longer supported) andersen-lab-env pyenv Setting the global version Setting the local version pyenv-virtualenv conda conda integrates with pyenv and pyenv-virtualenv pyenv environments are inherited andersen-lab-env structure Installing the andersen-lab-env andersen-lab-env git structure adding new software andersen-lab-env \u00b6 Computational Reproducibility is the ability to reproduce an analysis exactly. In order for comutational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub. Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software we've developed the andersen-lab-env . The andersen-lab-env is a set of software environments that can be used in conjunction with the bioinformatic pipelines we have developed for Quest. These environments can be installed locally on a Mac or on Quest. The andersen-lab-env is designed to change over time, but we explicitly define the software versions, and we track changes to the environments over time. The system is in some ways complex. This page is designed to try to explain how it works. We rely on three different tools to manage software environments. In concert they provide a lot of flexibility when it comes to setting up the software environment. Note The software environments on Mac and Linux are not exactly identical...but they are very close. There is an installation script you can use to install the andersen-lab-env , but it is recommended that you read this page before doing so. pyenv \u00b6 pyenv documentation pyenv is used to install and manage different versions of python. For example, you might have a python 3 script for one project and a python 2 script for another. You want to be able to run both scripts on your system. One option is to modify the python 2 script to work with python 3, but this is not always an option. The solution is to be able to install multiple versions of python simultaneously. pyenv allows you to do this. More than this, pyenv allows you to set the python version that you want to use at the local or global level. local - Sets the python version to a specific directory. global - Sets the python version to use everywhere unless a local version is set. Lets look at an example of this. First, lets install two different versions of python. pyenv install 2.7.11 pyenv install 3.6.0 Now you can see the installed versions by typing pyenv versions : $ pyenv versions * system 2.7.11 3.6.0 The * indicates that that is the current version of python you are using. In the case above it is set to use the system python which is preinstalled and is often python 2. Setting the global version \u00b6 Example setting the global version of python to 3.6.0 pyenv global 3.6.0 Now when we run python it will use python version 3.6.0. Setting the local version \u00b6 Example setting the local version of python to 2.7.11 mkdir my_python2_project cd my_python2_project pyenv local 2.7.11 Now if we go into a particular directoy and type pyenv local 2.7.11 , a .python-version file is created that says 2.7.11 and makes it so that directory always uses python 2.7.11. Now lets see what this looks like: As is illustrated above, versions are inherited from parent directories. When you cd to a directory, pyenv searches up through each directory looking for a .python-version file to identify which version of python to use. If it reaches the top before finding one it uses the global version. tl;dr; - pyenv allows us to install separate versions of python and set them at the directory level. pyenv-virtualenv \u00b6 Documentation pyenv lets us install multiple versions of python, and lets us use those versions locally within certain directories or globally. But what if we have two projects that use Python 2.7.11 and one requires a python module with a specific version: networkx==1.0 . Another project the same module greater than version 2.0 networkx>2.0 . How can we simultaneously work on both projects on the same system? virtualenv is a python tool for creating isolated python environments (also known as virtualenvs; The usage tends to be specific for python virtual environments and is short for 'virtual environment'). You can create a virtualenv for every project that you do - and these can be used to ensure that when you update or install modules for a given project that they do not interfere with each other. We won't be using virtualenv directly, but instead will the pyenv flavor of virtualenvs. pyenv-virtualenv is a tool that can create virtual environments that operate similar to the way pyenv python environments do. You can create virtualenvs that act globally or you can create virtualenvs that are local to a specific directory. To create a pyenv-virtualenv you must provide a base python environment that you have installed and a name for the environment. For example, below the python environment is 2.7.11 and the name of the environment is c_elegans_project : pyenv virtualenv 2.7.11 c_elegans_project Then you can set that virtualenv to a local directory using: mkdir c_elegans_project cd c_elegans_project pyenv local c_elegans_project Notice that the folder name is the same as the virtualenv. This can be a good idea for clarity. You can see a list of python versions and virtual environments by typing: pyenv versions Output: 2.7.11 * 2.7.11/envs/c_elegans_project 3.6.0 Virtual environments are designated as <version>/envs/<name> . Now we can also install the module we need for that specific project. pyenv installs a python-specific package manager called pip : pip install networkx==1.0 Notice that at this point we have isolated independent environments that do not interfere with one another. If we leave them alone for a year we should be able to come back and the software environment should be the same... and if they work with data they should reproduce the identical result. tl;dr - pyenv-virtualenv can define custom isolated python environments and set them the same way pyenv sets python installations. conda \u00b6 Conda Documentation Thus far we've managed to install multiple versions of python and figured out how to use them in independent, isolated environments. But we obviously use a lot more than just Python. We need to be able to install things like bcftools to work with variant data. We need to be able to install Java packages, and R packages, and all kinds of software. Conda can help us with this. Conda is a language-agnostic package manager. That means it can be used to install packages from python, R, Java, C/C++, etc. For example, the command below will install R and the R Tidyverse . conda install r-tidyverse conda integrates with pyenv and pyenv-virtualenv \u00b6 Important for our purposes, conda can be installed by pyenv . When I stated earlier that pyenv is used to install and manage versions of python I ommitted the fact that it can also install conda to avoid confusion. conda is not a version of python , but it is written in python, and it can be used to install python modules in addition to lots of other stuff. Similar to python virtualenvs, isolated conda environments can be created as was demonstrated above. You would run something like: pyenv install miniconda3-4.3.27 pyenv virtualenv miniconda3-4.3.27 my_new_conda_env pyenv local my_new_conda_env conda install bcftools pip install requests # This version of pip is specific gto What is great about these environments is that we can create custom software environments to suit any project. We can install R packages, python modules, C/C++ executables, and more. pyenv environments are inherited \u00b6 We can now install custom environments for each project. Even better, pyenv allows you to specify multiple environments together. Consider the example in this diagram: There are two environments defined: env_1 bcftools v1.6 bedtools v1.2 R-tidyverse 1.0 env_2 bcftools v1.6 vcf-kit v1.6 Those environments on their own appear in blue above. If we were to use the following command to specify these environments: pyenv local env_2 env_1 base_version We would produce the green environment in the diagram. What you are seeing are two environments being combined. However, the order you specify them in matters. Notice that bcftools v1.7 is used and not bcftools v1.6 . This is because env_2 is searched first when commands libraries are retrieved. After pulling all the libraries in env_2 , the combined library will inherit anything remaining in env_1 . This allows to easily combine environments for analysis. Remember that each of these virtual environments is based on a version of python or conda. But you can also put a plain version of python or conda as your last environment. This is useful when using conda because the conda command does not inherit from conda-based virtualenvs. andersen-lab-env structure \u00b6 The anderse-lab-env uses two conda environments: primary - The primary environment contains the majority of the tools required for performing sequence analysis. py2 - For programs that require python 2. You can create and use your own conda environments for projects, but these are designed to be comprehensive. Installing the andersen-lab-env \u00b6 If you are on Quest Edit your .bashrc file to contain the following: # .bashrc export PKG_CONFIG_PATH=/usr/share/pkgconfig:$PKG_CONFIG_PATH # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi Installation The andersen-lab-env can be installed by running the following command: cd && if cd ~/andersen-lab-env; then git pull; else git clone http://www.github.com/andersenlab/andersen-lab-env; fi bash setup.sh This command will clone the repo, cd into it, and run the setup.sh script. When you run the setup.sh script it will install the latest version of the primary and py2 environments, and it will assign these environments globally as: pyenv primary-(date) py2-(date) minicondax-x.x.x You should not need to change your global environment Note If you have existing versions of the primary and py2 environments installed they will remain. You can set them locally at the project level if necessary. andersen-lab-env git structure \u00b6 The andersen-lab-env is used to manage and version the software environments. The repo has the following structure: \u251c\u2500\u2500 Brewfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 primary.environment.yaml \u251c\u2500\u2500 py2.environment.yaml \u251c\u2500\u2500 rebuild_envs.sh \u251c\u2500\u2500 setup.sh \u251c\u2500\u2500 user_bash_profile.sh \u2514\u2500\u2500 versions \u251c\u2500\u2500 Linux.2018-03-08.primary.yaml \u251c\u2500\u2500 Linux.2018-03-08.py2.yaml \u251c\u2500\u2500 Mac.2018-03-08.primary.yaml \u2514\u2500\u2500 Mac.2018-03-08.py2.yaml primary.environment.yaml - base primary environment. This lists the software to be installed, but not specific versions of it. py2.environment.yaml - The base py2 environment. This lists the software to be installed, but not specific versions of it. Brewfile - Defines the software software-dependencies to be installed when running setup.sh rebuild_envs - Used to construct new versions of the environments. Note that you need to do this on a Linux and Mac computer. user_bash_profile.sh - The optional bash profile that is created with setup.sh . versions/ - Software required for each environment with all dependencies. Versioned in git and by platform and date. adding new software \u00b6 When you want to add new software a new version of the primary and py2 environments should be created. You must modify the primary.environment.yaml or py2.environment.yaml files and build the files you see in the versions folder which define the required software by specific version and includes all the dependencies. bash rebuild_envs.sh This will output two new versions specific to your platform in the versions/ folder. You must run this script and generate the appropriate version files on both Mac and Linux. Commit the updated versions to git. Other users can then install them by running the command in installing the andersen-lab-env","title":"The Andersen Lab Software Environment (no longer supported)"},{"location":"archive/quest-andersen-lab-env/#the_andersen_lab_software_environment_no_longer_supported","text":"The Andersen Lab Software Environment (no longer supported) andersen-lab-env pyenv Setting the global version Setting the local version pyenv-virtualenv conda conda integrates with pyenv and pyenv-virtualenv pyenv environments are inherited andersen-lab-env structure Installing the andersen-lab-env andersen-lab-env git structure adding new software","title":"The Andersen Lab Software Environment (no longer supported)"},{"location":"archive/quest-andersen-lab-env/#andersen-lab-env","text":"Computational Reproducibility is the ability to reproduce an analysis exactly. In order for comutational research to be reproducible you need to keep track of the code, software, and data. We keep track of code using git and GitHub. Our starting data (usually FASTQs) is almost always static, so we don't need to track changes to the data. We perform a new analysis when data is added. To track software we've developed the andersen-lab-env . The andersen-lab-env is a set of software environments that can be used in conjunction with the bioinformatic pipelines we have developed for Quest. These environments can be installed locally on a Mac or on Quest. The andersen-lab-env is designed to change over time, but we explicitly define the software versions, and we track changes to the environments over time. The system is in some ways complex. This page is designed to try to explain how it works. We rely on three different tools to manage software environments. In concert they provide a lot of flexibility when it comes to setting up the software environment. Note The software environments on Mac and Linux are not exactly identical...but they are very close. There is an installation script you can use to install the andersen-lab-env , but it is recommended that you read this page before doing so.","title":"andersen-lab-env"},{"location":"archive/quest-andersen-lab-env/#pyenv","text":"pyenv documentation pyenv is used to install and manage different versions of python. For example, you might have a python 3 script for one project and a python 2 script for another. You want to be able to run both scripts on your system. One option is to modify the python 2 script to work with python 3, but this is not always an option. The solution is to be able to install multiple versions of python simultaneously. pyenv allows you to do this. More than this, pyenv allows you to set the python version that you want to use at the local or global level. local - Sets the python version to a specific directory. global - Sets the python version to use everywhere unless a local version is set. Lets look at an example of this. First, lets install two different versions of python. pyenv install 2.7.11 pyenv install 3.6.0 Now you can see the installed versions by typing pyenv versions : $ pyenv versions * system 2.7.11 3.6.0 The * indicates that that is the current version of python you are using. In the case above it is set to use the system python which is preinstalled and is often python 2.","title":"pyenv"},{"location":"archive/quest-andersen-lab-env/#setting_the_global_version","text":"Example setting the global version of python to 3.6.0 pyenv global 3.6.0 Now when we run python it will use python version 3.6.0.","title":"Setting the global version"},{"location":"archive/quest-andersen-lab-env/#setting_the_local_version","text":"Example setting the local version of python to 2.7.11 mkdir my_python2_project cd my_python2_project pyenv local 2.7.11 Now if we go into a particular directoy and type pyenv local 2.7.11 , a .python-version file is created that says 2.7.11 and makes it so that directory always uses python 2.7.11. Now lets see what this looks like: As is illustrated above, versions are inherited from parent directories. When you cd to a directory, pyenv searches up through each directory looking for a .python-version file to identify which version of python to use. If it reaches the top before finding one it uses the global version. tl;dr; - pyenv allows us to install separate versions of python and set them at the directory level.","title":"Setting the local version"},{"location":"archive/quest-andersen-lab-env/#pyenv-virtualenv","text":"Documentation pyenv lets us install multiple versions of python, and lets us use those versions locally within certain directories or globally. But what if we have two projects that use Python 2.7.11 and one requires a python module with a specific version: networkx==1.0 . Another project the same module greater than version 2.0 networkx>2.0 . How can we simultaneously work on both projects on the same system? virtualenv is a python tool for creating isolated python environments (also known as virtualenvs; The usage tends to be specific for python virtual environments and is short for 'virtual environment'). You can create a virtualenv for every project that you do - and these can be used to ensure that when you update or install modules for a given project that they do not interfere with each other. We won't be using virtualenv directly, but instead will the pyenv flavor of virtualenvs. pyenv-virtualenv is a tool that can create virtual environments that operate similar to the way pyenv python environments do. You can create virtualenvs that act globally or you can create virtualenvs that are local to a specific directory. To create a pyenv-virtualenv you must provide a base python environment that you have installed and a name for the environment. For example, below the python environment is 2.7.11 and the name of the environment is c_elegans_project : pyenv virtualenv 2.7.11 c_elegans_project Then you can set that virtualenv to a local directory using: mkdir c_elegans_project cd c_elegans_project pyenv local c_elegans_project Notice that the folder name is the same as the virtualenv. This can be a good idea for clarity. You can see a list of python versions and virtual environments by typing: pyenv versions Output: 2.7.11 * 2.7.11/envs/c_elegans_project 3.6.0 Virtual environments are designated as <version>/envs/<name> . Now we can also install the module we need for that specific project. pyenv installs a python-specific package manager called pip : pip install networkx==1.0 Notice that at this point we have isolated independent environments that do not interfere with one another. If we leave them alone for a year we should be able to come back and the software environment should be the same... and if they work with data they should reproduce the identical result. tl;dr - pyenv-virtualenv can define custom isolated python environments and set them the same way pyenv sets python installations.","title":"pyenv-virtualenv"},{"location":"archive/quest-andersen-lab-env/#conda","text":"Conda Documentation Thus far we've managed to install multiple versions of python and figured out how to use them in independent, isolated environments. But we obviously use a lot more than just Python. We need to be able to install things like bcftools to work with variant data. We need to be able to install Java packages, and R packages, and all kinds of software. Conda can help us with this. Conda is a language-agnostic package manager. That means it can be used to install packages from python, R, Java, C/C++, etc. For example, the command below will install R and the R Tidyverse . conda install r-tidyverse","title":"conda"},{"location":"archive/quest-andersen-lab-env/#conda_integrates_with_pyenv_and_pyenv-virtualenv","text":"Important for our purposes, conda can be installed by pyenv . When I stated earlier that pyenv is used to install and manage versions of python I ommitted the fact that it can also install conda to avoid confusion. conda is not a version of python , but it is written in python, and it can be used to install python modules in addition to lots of other stuff. Similar to python virtualenvs, isolated conda environments can be created as was demonstrated above. You would run something like: pyenv install miniconda3-4.3.27 pyenv virtualenv miniconda3-4.3.27 my_new_conda_env pyenv local my_new_conda_env conda install bcftools pip install requests # This version of pip is specific gto What is great about these environments is that we can create custom software environments to suit any project. We can install R packages, python modules, C/C++ executables, and more.","title":"conda integrates with pyenv and pyenv-virtualenv"},{"location":"archive/quest-andersen-lab-env/#pyenv_environments_are_inherited","text":"We can now install custom environments for each project. Even better, pyenv allows you to specify multiple environments together. Consider the example in this diagram: There are two environments defined: env_1 bcftools v1.6 bedtools v1.2 R-tidyverse 1.0 env_2 bcftools v1.6 vcf-kit v1.6 Those environments on their own appear in blue above. If we were to use the following command to specify these environments: pyenv local env_2 env_1 base_version We would produce the green environment in the diagram. What you are seeing are two environments being combined. However, the order you specify them in matters. Notice that bcftools v1.7 is used and not bcftools v1.6 . This is because env_2 is searched first when commands libraries are retrieved. After pulling all the libraries in env_2 , the combined library will inherit anything remaining in env_1 . This allows to easily combine environments for analysis. Remember that each of these virtual environments is based on a version of python or conda. But you can also put a plain version of python or conda as your last environment. This is useful when using conda because the conda command does not inherit from conda-based virtualenvs.","title":"pyenv environments are inherited"},{"location":"archive/quest-andersen-lab-env/#andersen-lab-env_structure","text":"The anderse-lab-env uses two conda environments: primary - The primary environment contains the majority of the tools required for performing sequence analysis. py2 - For programs that require python 2. You can create and use your own conda environments for projects, but these are designed to be comprehensive.","title":"andersen-lab-env structure"},{"location":"archive/quest-andersen-lab-env/#installing_the_andersen-lab-env","text":"If you are on Quest Edit your .bashrc file to contain the following: # .bashrc export PKG_CONFIG_PATH=/usr/share/pkgconfig:$PKG_CONFIG_PATH # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi Installation The andersen-lab-env can be installed by running the following command: cd && if cd ~/andersen-lab-env; then git pull; else git clone http://www.github.com/andersenlab/andersen-lab-env; fi bash setup.sh This command will clone the repo, cd into it, and run the setup.sh script. When you run the setup.sh script it will install the latest version of the primary and py2 environments, and it will assign these environments globally as: pyenv primary-(date) py2-(date) minicondax-x.x.x You should not need to change your global environment Note If you have existing versions of the primary and py2 environments installed they will remain. You can set them locally at the project level if necessary.","title":"Installing the andersen-lab-env"},{"location":"archive/quest-andersen-lab-env/#andersen-lab-env_git_structure","text":"The andersen-lab-env is used to manage and version the software environments. The repo has the following structure: \u251c\u2500\u2500 Brewfile \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 primary.environment.yaml \u251c\u2500\u2500 py2.environment.yaml \u251c\u2500\u2500 rebuild_envs.sh \u251c\u2500\u2500 setup.sh \u251c\u2500\u2500 user_bash_profile.sh \u2514\u2500\u2500 versions \u251c\u2500\u2500 Linux.2018-03-08.primary.yaml \u251c\u2500\u2500 Linux.2018-03-08.py2.yaml \u251c\u2500\u2500 Mac.2018-03-08.primary.yaml \u2514\u2500\u2500 Mac.2018-03-08.py2.yaml primary.environment.yaml - base primary environment. This lists the software to be installed, but not specific versions of it. py2.environment.yaml - The base py2 environment. This lists the software to be installed, but not specific versions of it. Brewfile - Defines the software software-dependencies to be installed when running setup.sh rebuild_envs - Used to construct new versions of the environments. Note that you need to do this on a Linux and Mac computer. user_bash_profile.sh - The optional bash profile that is created with setup.sh . versions/ - Software required for each environment with all dependencies. Versioned in git and by platform and date.","title":"andersen-lab-env git structure"},{"location":"archive/quest-andersen-lab-env/#adding_new_software","text":"When you want to add new software a new version of the primary and py2 environments should be created. You must modify the primary.environment.yaml or py2.environment.yaml files and build the files you see in the versions folder which define the required software by specific version and includes all the dependencies. bash rebuild_envs.sh This will output two new versions specific to your platform in the versions/ folder. You must run this script and generate the appropriate version files on both Mac and Linux. Commit the updated versions to git. Other users can then install them by running the command in installing the andersen-lab-env","title":"adding new software"},{"location":"archive/sample-sheets/","text":"Sample Sheets \u00b6 Sample Sheets Creating sample sheets wi-nf and concordance-nf pipelines nil-ril-nf Sample-Sheet Format Absolute vs. relative paths The wi-nf , concordance-nf , and nil-ril-nf pipelines all make use of sample sheets. Sample sheets specify which fastqs belong to a given strain or isotype. Creating sample sheets \u00b6 wi-nf and concordance-nf pipelines \u00b6 For the wi-nf and concordance-nf pipelines, sample-sheets are generated using the file located (in each of these repos) in the scripts/construct_sample_sheet.sh . Importantly, these scripts are almost identical except that the concordance-nf pipeline constructs a sample sheet for strains whereas the wi-nf sample sheet is for isotypes . When adding new sequence data you need to update these scripts. Note The nomenclature regarding sample sheets and scripts was changed in March of 2018 to make it clearer. You may encounter older files with the following names that correspond to the newer names SM_sample_sheet --> sample_sheet.tsv construct_SM_sheet.sh --> construct_sample_sheet.tsv nil-ril-nf \u00b6 For the nil-ril-nf pipelines you must manually create the sample sheets according to the format below. Sample-Sheet Format \u00b6 The sample sheet defines which FASTQs belong to which strain/isotype and specifies additional information regarding a sample. Additional information specfieid are the FASTQ ID (a unique identifier for a FASTQ-pair), Sequencing POOL (which defines the group of samples that were sequenced together), the locations of the FASTQs, and the sequencing folder. Note Internally, the 'sequencing pool' information as treated as the DNA-library identifier by BWA ( LB ). Our lab processes sequence data such that the pool name uniquely identifies DNA-libraries for each sample. Sample sheet structure All columns are required. Sample Identifier - How FASTQs should be grouped in the pipeline. Usually this is by strain or isotype. FASTQ ID - A unique ID for the FASTQ pair. It must be unique for all sequencing runs defined in the sample sheet. Sequencing pool - The sequencing pool is often defined arbitrarily. It refers to the set of strains that were sequenced together. It acts as an identifer of the DNA library within the pipelines. FASTQ1 - A relative or absolute path to the first FASTQ. FASTQ2 - A relative or absolute path to the second FASTQ. Sequencing Folder - This column is provided for informational purposes. It generally refers to the name of the folder containing the FASTQs. Example AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set Notice that the file does not include a header . The table with corresponding header included below look like this: Sample Identifeir FASTQ ID Sequencing Pool fastq-1-path fastq-2-path sequencing_folder AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set Absolute vs. relative paths \u00b6 When constructing the sample sheet for the wi-nf and concordance-nf pipelines you are required to use the absolute paths to each FASTQ. The nil-ril-nf pipeline can use relative paths to FASTQs by specifying the --fq_file_prefix option to the parent directory containing FASTQs.","title":"Sample Sheets"},{"location":"archive/sample-sheets/#sample_sheets","text":"Sample Sheets Creating sample sheets wi-nf and concordance-nf pipelines nil-ril-nf Sample-Sheet Format Absolute vs. relative paths The wi-nf , concordance-nf , and nil-ril-nf pipelines all make use of sample sheets. Sample sheets specify which fastqs belong to a given strain or isotype.","title":"Sample Sheets"},{"location":"archive/sample-sheets/#creating_sample_sheets","text":"","title":"Creating sample sheets"},{"location":"archive/sample-sheets/#wi-nf_and_concordance-nf_pipelines","text":"For the wi-nf and concordance-nf pipelines, sample-sheets are generated using the file located (in each of these repos) in the scripts/construct_sample_sheet.sh . Importantly, these scripts are almost identical except that the concordance-nf pipeline constructs a sample sheet for strains whereas the wi-nf sample sheet is for isotypes . When adding new sequence data you need to update these scripts. Note The nomenclature regarding sample sheets and scripts was changed in March of 2018 to make it clearer. You may encounter older files with the following names that correspond to the newer names SM_sample_sheet --> sample_sheet.tsv construct_SM_sheet.sh --> construct_sample_sheet.tsv","title":"wi-nf and concordance-nf pipelines"},{"location":"archive/sample-sheets/#nil-ril-nf","text":"For the nil-ril-nf pipelines you must manually create the sample sheets according to the format below.","title":"nil-ril-nf"},{"location":"archive/sample-sheets/#sample-sheet_format","text":"The sample sheet defines which FASTQs belong to which strain/isotype and specifies additional information regarding a sample. Additional information specfieid are the FASTQ ID (a unique identifier for a FASTQ-pair), Sequencing POOL (which defines the group of samples that were sequenced together), the locations of the FASTQs, and the sequencing folder. Note Internally, the 'sequencing pool' information as treated as the DNA-library identifier by BWA ( LB ). Our lab processes sequence data such that the pool name uniquely identifies DNA-libraries for each sample. Sample sheet structure All columns are required. Sample Identifier - How FASTQs should be grouped in the pipeline. Usually this is by strain or isotype. FASTQ ID - A unique ID for the FASTQ pair. It must be unique for all sequencing runs defined in the sample sheet. Sequencing pool - The sequencing pool is often defined arbitrarily. It refers to the set of strains that were sequenced together. It acts as an identifer of the DNA library within the pipelines. FASTQ1 - A relative or absolute path to the first FASTQ. FASTQ2 - A relative or absolute path to the second FASTQ. Sequencing Folder - This column is provided for informational purposes. It generally refers to the name of the folder containing the FASTQs. Example AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set Notice that the file does not include a header . The table with corresponding header included below look like this: Sample Identifeir FASTQ ID Sequencing Pool fastq-1-path fastq-2-path sequencing_folder AB1 BGI1-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI1-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI2-RET2-AB1 RET2 /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI2-RET2-AB1-trim-2P.fq.gz original_wi_set AB1 BGI3-RET2b-AB1 RET2b /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-1P.fq.gz /projects/b1059/data/fastq/WI/dna/processed/original_wi_set/BGI3-RET2b-AB1-trim-2P.fq.gz original_wi_set","title":"Sample-Sheet Format"},{"location":"archive/sample-sheets/#absolute_vs_relative_paths","text":"When constructing the sample sheet for the wi-nf and concordance-nf pipelines you are required to use the absolute paths to each FASTQ. The nil-ril-nf pipeline can use relative paths to FASTQs by specifying the --fq_file_prefix option to the parent directory containing FASTQs.","title":"Absolute vs. relative paths"}]}